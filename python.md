
## datetime å¯¹è±¡

```python
import datetime

datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
# datetime.datetime.now() è¿”å›äº†ä¸€ä¸ªæ—¶é—´å¯¹è±¡ï¼Œä¾æ¬¡ä¸ºï¼šå¹´ã€æœˆã€æ—¥ã€æ—¶ã€åˆ†ã€ç§’ã€æ¯«ç§’ã€‚å…¶ä¸­ï¼Œæ¯«ç§’çš„å–å€¼èŒƒå›´åœ¨ 0 <= microsecond < 1000000ã€‚

# ç­‰æ•ˆæ–¹æ³• datetime.datetime.now()
datetime.datetime.today()  # è·å–å½“å‰æ—¶é—´

# åªå–ä¸€éƒ¨åˆ†æ•°å€¼
datetime.datetime.now().year  # è¿”å›å½“å‰å¹´ä»½

# æ‰‹åŠ¨æŒ‡å®šæ—¶é—´
datetime.datetime(2017, 10, 1, 10, 59, 30)  # æŒ‡å®šä»»æ„æ—¶é—´
```

### è®¡ç®—

```python
# è¿”å›äº† datetime.timedelta å¯¹è±¡ã€‚timedelta å¯ä»¥ç”¨äºè¡¨ç¤ºæ—¶é—´ä¸Šçš„ä¸åŒï¼Œä½†æœ€å¤šåªä¿ç•™ 3 ä½ï¼Œåˆ†åˆ«æ˜¯ï¼šå¤©ã€ç§’ã€æ¯«ç§’
datetime.datetime(2018, 10, 1) - datetime.datetime(2017, 10, 1)  # è®¡ç®—æ—¶é—´é—´éš”

datetime.datetime.now() - datetime.datetime(2017, 10, 1)  # timedelta è¡¨ç¤ºé—´éš”æ—¶é—´
# datetime.timedelta(days=2232, seconds=48988, microseconds=37705)

# é€šè¿‡ timedelta å¢åŠ ä¸€å¹´æ—¶é—´
datetime.datetime.now() + datetime.timedelta(365)  # éœ€å°†å¹´ä»½è½¬æ¢ä¸ºå¤©
```

### æ ¼å¼åŒ–

```python
datetime.datetime.now().strftime("%Y-%m-%d")  # è½¬æ¢ä¸ºè‡ªå®šä¹‰æ ·å¼
datetime.datetime.now().strftime("%Y å¹´ %m æœˆ %d æ—¥")  # è½¬æ¢ä¸ºè‡ªå®šä¹‰æ ·å¼

# å­—ç¬¦ä¸²è½¬æ—¶é—´å¯¹è±¡
datetime.datetime.strptime("2018-10-1", "%Y-%m-%d")
```

å‚æ•°ï¼š
%y ä¸¤ä½æ•°çš„å¹´ä»½è¡¨ç¤ºï¼ˆ00-99ï¼‰

%Y å››ä½æ•°çš„å¹´ä»½è¡¨ç¤ºï¼ˆ000-9999ï¼‰

%m æœˆä»½ï¼ˆ01 - 12ï¼‰

%d æœˆå†…ä¸­çš„ä¸€å¤©ï¼ˆ0 - 31ï¼‰

%H 24 å°æ—¶åˆ¶å°æ—¶æ•°ï¼ˆ0 - 23ï¼‰

%I 12 å°æ—¶åˆ¶å°æ—¶æ•°ï¼ˆ01 - 12ï¼‰

%M åˆ†é’Ÿæ•°ï¼ˆ00 - 59ï¼‰

%S ç§’ï¼ˆ00 - 59ï¼‰

%a æœ¬åœ°ç®€åŒ–æ˜ŸæœŸåç§°

%A æœ¬åœ°å®Œæ•´æ˜ŸæœŸåç§°

%b æœ¬åœ°ç®€åŒ–çš„æœˆä»½åç§°

%B æœ¬åœ°å®Œæ•´çš„æœˆä»½åç§°

%c æœ¬åœ°ç›¸åº”çš„æ—¥æœŸè¡¨ç¤ºå’Œæ—¶é—´è¡¨ç¤º

%j å¹´å†…çš„ä¸€å¤©ï¼ˆ001 - 366ï¼‰

%p æœ¬åœ° A.M. æˆ– P.M. çš„ç­‰ä»·ç¬¦

%U ä¸€å¹´ä¸­çš„æ˜ŸæœŸæ•°ï¼ˆ00 - 53ï¼‰æ˜ŸæœŸå¤©ä¸ºæ˜ŸæœŸçš„å¼€å§‹

%w æ˜ŸæœŸï¼ˆ0 - 6ï¼‰ï¼Œæ˜ŸæœŸå¤©ä¸ºæ˜ŸæœŸçš„å¼€å§‹

%W ä¸€å¹´ä¸­çš„æ˜ŸæœŸæ•°ï¼ˆ00 - 53ï¼‰æ˜ŸæœŸä¸€ä¸ºæ˜ŸæœŸçš„å¼€å§‹

%x æœ¬åœ°ç›¸åº”çš„æ—¥æœŸè¡¨ç¤º

%X æœ¬åœ°ç›¸åº”çš„æ—¶é—´è¡¨ç¤º

%Z å½“å‰æ—¶åŒºçš„åç§°


### æ—¶åŒº

åè°ƒä¸–ç•Œæ—¶ï¼ˆUTCï¼‰,ä½¿ç”¨ UTC åç§»é‡æ¥å®šä¹‰ä¸åŒæ—¶åŒºçš„æ—¶é—´

```python
datetime.datetime.utcnow()  # è·å– UTC æ—¶é—´

# è¿›è¡ŒåŒ—äº¬æ—¶é—´ + 8 å¤„ç†
utc = datetime.datetime.utcnow()  # è·å– UTC æ—¶é—´
tzutc_8 = datetime.timezone(datetime.timedelta(hours=8))  # + 8 å°æ—¶
utc_8 = utc.astimezone(tzutc_8)  # æ·»åŠ åˆ°æ—¶åŒºä¸­
print(utc_8)
```

### æ—¶é—´æˆ³ Timestamp

```python
import pandas as pd

pd.Timestamp("2018-10-1")
pd.Timestamp(datetime.datetime.now())
pd.to_datetime("1-10-2018")
# Timestamp('2018-01-10 00:00:00') é»˜è®¤ æœˆã€æ—¥

pd.to_datetime("1-10-2018", dayfirst=True)
# Timestamp('2018-10-01 00:00:00') æ—¥ä¼˜å…ˆ
```

### æ—¶é—´ç´¢å¼• DatetimeIndex

```python
pd.to_datetime(["2018-10-1", "2018-10-2", "2018-10-3"])  # ç”Ÿæˆæ—¶é—´ç´¢å¼•
# DatetimeIndex(['2018-10-01', '2018-10-02', '2018-10-03'], dtype='datetime64[ns]', freq=None)


# å°† Seris ä¸­å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´, è¿™æ ·å¤„ç†å¾—åˆ°çš„ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„æ—¶é—´ç´¢å¼•
s = pd.Series(["2018-10-1", "2018-10-2", "2018-10-3"])
pd.to_datetime(s) 

pd.Series(index=pd.to_datetime(s)).index  # å½“æ—¶é—´ä½äºç´¢å¼•æ—¶ï¼Œå°±æ˜¯ DatetimeIndex

```

#### date_range

ç”Ÿæˆä»»ä½•ä»¥ä¸€å®šè§„å¾‹å˜åŒ–çš„æ—¶é—´ç´¢å¼•

pandas.date_range(start=None, end=None, periods=None, freq=â€™Dâ€™, tz=None, normalize=False, name=None, closed=None, **kwargs)
start= ï¼šè®¾ç½®èµ·å§‹æ—¶é—´

end= ï¼šè®¾ç½®æˆªè‡³æ—¶é—´

periods= ï¼šè®¾ç½®æ—¶é—´åŒºé—´ï¼Œè‹¥ None åˆ™éœ€è¦å•ç‹¬è®¾ç½®èµ·æ­¢å’Œæˆªè‡³æ—¶é—´ã€‚

freq= ï¼šè®¾ç½®é—´éš”å‘¨æœŸã€‚

tz= ï¼šè®¾ç½®æ—¶åŒºã€‚

ç‰¹åˆ«åœ°ï¼Œfreq= é¢‘åº¦å‚æ•°éå¸¸å…³é”®ï¼Œå¯ä»¥è®¾ç½®çš„å‘¨æœŸæœ‰ï¼š

freq='s' : ç§’

freq='min' : åˆ†é’Ÿ

freq='H' : å°æ—¶

freq='D' : å¤©

freq='w' : å‘¨

freq='m' : æœˆ

freq='BM' : æ¯ä¸ªæœˆæœ€åä¸€å¤©

freq='W' ï¼šæ¯å‘¨çš„æ˜ŸæœŸæ—¥

```python
pd.date_range("2018-10-1", "2018-10-2", freq="H")  # æŒ‰å°æ—¶é—´éš”ç”Ÿæˆæ—¶é—´ç´¢å¼•
# DatetimeIndex(['2018-10-01 00:00:00', '2018-10-01 01:00:00',
#                '2018-10-01 02:00:00', '2018-10-01 03:00:00',
#                '2018-10-01 04:00:00', '2018-10-01 05:00:00',
#                '2018-10-01 06:00:00', '2018-10-01 07:00:00',
#                '2018-10-01 08:00:00', '2018-10-01 09:00:00',
#                '2018-10-01 10:00:00', '2018-10-01 11:00:00',
#                '2018-10-01 12:00:00', '2018-10-01 13:00:00',
#                '2018-10-01 14:00:00', '2018-10-01 15:00:00',
#                '2018-10-01 16:00:00', '2018-10-01 17:00:00',
#                '2018-10-01 18:00:00', '2018-10-01 19:00:00',
#                '2018-10-01 20:00:00', '2018-10-01 21:00:00',
#                '2018-10-01 22:00:00', '2018-10-01 23:00:00',
#                '2018-10-02 00:00:00'],
#               dtype='datetime64[ns]', freq='H')

# ä» 2018-10-1 å¼€å§‹ï¼Œä»¥å¤©ä¸ºé—´éš”ï¼Œå‘åæ¨ 10 æ¬¡
pd.date_range("2018-10-1", periods=10, freq="D")

# ä» 2018-10-1 å¼€å§‹ï¼Œä»¥ 1H20min ä¸ºé—´éš”ï¼Œå‘åæ¨ 10 æ¬¡
pd.date_range("2018-10-1", periods=10, freq="1H20min")


# offset å¯¹è±¡å¯ä»¥è®©æ—¶é—´åºåˆ—è¿›è¡Œä¸€å®šçš„åç§»
from pandas import offsets

# offset å¯¹è±¡è®© time_index ä¾æ¬¡å¢åŠ  1 ä¸ªæœˆ + 2 å¤© + 3 å°æ—¶ã€‚
time_index + offsets.DateOffset(months=1, days=2, hours=3)

# ç»Ÿä¸€æ¨åä¸¤å‘¨
time_index + 2 * offsets.Week()

```

#### æ—¶é—´é—´éš” Periods

ç”ŸæˆæŒ‰ç‰¹å®šæ—¶é—´è·¨åº¦çš„å‘¨æœŸå¯¹è±¡ï¼Œåç»­è¿›è¡Œè®¡ç®—æ—¶ä¼šæ ¹æ®è¿™ä¸ªè·¨åº¦è¿›è¡Œè®¡ç®—

```python
# 1 å¹´è·¨åº¦
pd.Period("2018")

# 1 ä¸ªæœˆè·¨åº¦
pd.Period("2018-1")

# 1 å¤©è·¨åº¦
pd.Period("2018-1-1")
```

#### æ—¶é—´é—´éš”ç´¢å¼• PeriodsIndex

```python
p = pd.period_range("2018", "2019", freq="M")  # ç”Ÿæˆ 2018-1 åˆ° 2019-1 åºåˆ—ï¼ŒæŒ‰æœˆåˆ†å¸ƒ

# S: startã€E: end
p.asfreq(freq="D", how="S")  # é¢‘åº¦ä» M â†’ D
```

#### æ—¶åºæ•°æ®é€‰æ‹©ã€åˆ‡ç‰‡ã€åç§»

```python
import numpy as np

timeindex = pd.date_range("2018-1-1", periods=20, freq="M")
s = pd.Series(np.random.randn(len(timeindex)), index=timeindex)

# é€‰æ‹© 2008 å¹´çš„æ•°æ®
s["2018"]

# é€‰æ‹© 2018 å¹´ 7 æœˆåˆ° 2019 å¹´ 3 æœˆä¹‹é—´çš„æ‰€æœ‰æ•°æ®
s["2018-07":"2019-03"]

# ç´¢å¼•æ•´ä½“åç§»
s.shift(3)  # æ—¶é—´ç´¢å¼•ä»¥é»˜è®¤ Freq: M å‘ååç§» 3 ä¸ªå•ä½

s.shift(-3, freq="D")  # æ—¶é—´ç´¢å¼•ä»¥ Freq: D å‘å‰åç§» 3 ä¸ªå•ä½
```

#### æ—¶åºæ•°æ®é‡é‡‡æ ·

è°ƒæ•´æ—¶é—´ç´¢å¼•åºåˆ—çš„é¢‘ç‡

```python
dateindex = pd.period_range("2018-10-1", periods=20, freq="D")
s = pd.Series(np.random.randn(len(dateindex)), index=dateindex)

s.resample("2D").sum()  # é™é‡‡æ ·ï¼Œå¹¶å°†åˆ å»çš„æ•°æ®ä¾æ¬¡åˆå¹¶åˆ°ä¿ç•™æ•°æ®ä¸­

s.resample("2D").asfreq()  # é™é‡‡æ ·ï¼Œç›´æ¥èˆå»æ•°æ®


# openã€highã€lowã€close åˆ†åˆ«å¯¹åº”å¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·å’Œæ”¶ç›˜ä»·
s.resample("2D").ohlc()  

s.resample("H").ffill()  # å‡é‡‡æ ·ï¼Œä½¿ç”¨ç›¸åŒçš„æ•°æ®å¯¹æ–°å¢åŠ è¡Œå¡«å……

s.resample("H").ffill(limit=3)  # å‡é‡‡æ ·ï¼Œæœ€å¤šå¡«å……ä¸´è¿‘ 3 è¡Œ
```


#### æ—¶åºæ•°æ®æ—¶åŒºå¤„ç†


```python
naive_time = pd.date_range("1/10/2018 9:00", periods=10, freq="D")

# è½¬æœ¬åœ°æ—¶é—´
utc_time = naive_time.tz_localize("UTC")

# è½¬ç‰¹å®šæ—¶åŒºï¼Ÿ
utc_time.tz_convert("Asia/Shanghai")


pd.date_range("1/10/2018 9:00", periods=10, freq="D", tz="Asia/Shanghai")
```

## å­—ç¬¦ä¸²

python å­—ç¬¦ä¸² format , {x} ä¼šè¢«å½“åšå˜é‡ï¼Œ{{x}} ä¸ä¼šï¼Œæœ€ç»ˆè¾“å‡º{x}

```python
year = 1
f"{year}-ç¬¬ä¸€å­£åº¦"
```

## æ•°ç»„

x_temp = [i for i in range(0,300)]
x = [f(tvParams[0],i) for i in x_temp]
> è¯­æ³• [expression for i in iterable]

## ç±»

```python
# ç»§æ‰¿ nn.Module
class Net(nn.Module):
	pass
```

## å‡½æ•°

### sum

å¯å¯¹ä¸€ç»´æ•°ç»„è¿›è¡ŒåŠ å’Œ

### zip

å°†ä¸¤ä¸ªæ•°ç»„åˆå¹¶è¿›è¡Œéå†ï¼Ÿ

```python

cluster_names = ['KMeans', 'MiniBatchKMeans', 'AffinityPropagation', 
                 'MeanShift', 'SpectralClustering', 'AgglomerativeClustering', 'Birch', 'DBSCAN']


cluster_estimators = [
    cluster.KMeans(n_clusters=2),
    cluster.MiniBatchKMeans(n_clusters=2),
    cluster.AffinityPropagation(),
    cluster.MeanShift(),
    cluster.SpectralClustering(n_clusters=2),
    cluster.AgglomerativeClustering(n_clusters=2),
    cluster.Birch(n_clusters=2),
    cluster.DBSCAN()
]

for name,e in zip(cluster_names, cluster_estimators):
	pass
```

## å¾ªç¯

### for in

**enumerate**

å°†æ•°ç»„è¿›è¡Œè½¬åŒ–ï¼Œè·å–ç´¢å¼•

```py
for i, image in enumerate(faces.images[:5]):
    # æ—¢è·å¾—æ•°æ®åˆè·å¾— ç´¢å¼•
```

## é‡è½½

### __getitem__

é‡è½½ [] èƒ½åŠ›ï¼Œpandas é€šè¿‡é‡è½½å®ç°å¤§é‡æ•°æ® hack æ“ä½œ


## ä½¿ç”¨

### è¯·æ±‚å›¾ç‰‡å¹¶ä¸”ä¿å­˜æœ¬åœ°ï¼Œç”¨äºè¿›è¡Œæ¨ç†.....

```python
res = requests.get(IMAGE_URL)
with open("test.jpg", "wb") as f:
    f.write(res.content)
```


## åº“
### PyTorch

æœºå™¨ä¹‹å¿ƒÂ [æœ‰ä¸€ç¯‡æ–‡ç« ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650726576&idx=3&sn=4140ee7afc67928333e971062d042c59&chksm=871b24ceb06cadd8922cde50cbc5da6a04fd3f00a78964381c593b2dcf62bb78835159a00f27&scene=0#rd)Â å¯¹å„ä¸ªæ¡†æ¶ä»‹ç»çš„éå¸¸è¯¦ç»†
[PyTorch vs TensorFlowâ€Šâ€”â€Šspotting the difference](https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)


#### æ¨¡å—

| Packageï¼ˆåŒ…ï¼‰                 | Descriptionï¼ˆæè¿°ï¼‰                                                                                                                                                                                                                |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `torch`                    | å¼ é‡è®¡ç®—ç»„ä»¶, å…¼å®¹ NumPy æ•°ç»„ï¼Œä¸”å…·å¤‡å¼ºå¤§çš„ GPU åŠ é€Ÿæ”¯æŒ                                                                                                                                                                                            |
| `torch.autograd`           | è‡ªåŠ¨å¾®åˆ†ç»„ä»¶, æ˜¯ PyTorch çš„æ ¸å¿ƒç‰¹ç‚¹ï¼Œæ”¯æŒ torch ä¸­æ‰€æœ‰å¯å¾®åˆ†çš„å¼ é‡æ“ä½œ                                                                                                                                                                                   |
| `torch.nn`                 | æ·±åº¦ç¥ç»ç½‘ç»œç»„ä»¶, ç”¨äºçµæ´»æ„å»ºä¸åŒæ¶æ„çš„æ·±åº¦ç¥ç»ç½‘ç»œ<br>- å…¨è¿æ¥å±‚ï¼š`torch.nn.Linear()`<br>- MSE æŸå¤±å‡½æ•°ç±»ï¼š`torch.nn.MSELoss()`<br>- torch.nn.functional<br>  - ç¥ç»ç½‘ç»œå±‚ï¼Œæ¿€æ´»å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°<br>  - torch.nn.functional.linear()<br>  - torch.nn.functionalmse_loss()<br><br> |
| `torch.optim`              | ä¼˜åŒ–è®¡ç®—ç»„ä»¶, å›Šæ‹¬äº† SGD, RMSProp, LBFGS, Adam ç­‰å¸¸ç”¨çš„å‚æ•°ä¼˜åŒ–æ–¹æ³•                                                                                                                                                                               |
| `torch.multiprocessing`    | å¤šè¿›ç¨‹ç®¡ç†ç»„ä»¶ï¼Œæ–¹ä¾¿å®ç°ç›¸åŒæ•°æ®çš„ä¸åŒè¿›ç¨‹ä¸­å…±äº«è§†å›¾                                                                                                                                                                                                     |
| `torch.utils`              | å·¥å…·å‡½æ•°ç»„ä»¶ï¼ŒåŒ…å«æ•°æ®åŠ è½½ã€è®­ç»ƒç­‰å¸¸ç”¨å‡½æ•°                                                                                                                                                                                                          |
| `torch.legacy(.nn/.optim)` | å‘åå…¼å®¹ç»„ä»¶, åŒ…å«ç§»æ¤çš„æ—§ä»£ç                                                                                                                                                                                                                |
#### æ”¯æŒçš„ç±»å‹

https://pytorch.org/docs/stable/tensors.html

| æ•°æ®ç±»å‹ dtype            | CPU å¼ é‡               | GPU å¼ é‡                    |
| --------------------- | -------------------- | ------------------------- |
| 32-bit æµ®ç‚¹             | `torch.FloatTensor`  | `torch.cuda.FloatTensor`  |
| 64-bit æµ®ç‚¹             | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |
| 16-bit åŠç²¾åº¦æµ®ç‚¹          | N/A                  | `torch.cuda.HalfTensor`   |
| 8-bit æ— ç¬¦å·æ•´å½¢(0~255)    | `torch.ByteTensor`   | `torch.cuda.ByteTensor`   |
| 8-bit æœ‰ç¬¦å·æ•´å½¢(-128~127) | `torch.CharTensor`   | `torch.cuda.CharTensor`   |
| 16-bit æœ‰ç¬¦å·æ•´å½¢          | `torch.ShortTensor`  | `torch.cuda.ShortTensor`  |
| 32-bit æœ‰ç¬¦å·æ•´å½¢          | `torch.IntTensor`    | `torch.cuda.IntTensor`    |
| 64-bit æœ‰ç¬¦å·æ•´å½¢          | `torch.LongTensor`   | `torch.cuda.LongTensor`   |
|                       |                      |                           |

#### ä¾¿æ·æ–¹æ³•

| æ–¹æ³•                      | æè¿°                              |
| ----------------------- | ------------------------------- |
| `ones(*sizes)`          | åˆ›å»ºå…¨ä¸º 1 çš„ Tensor                 |
| `zeros(*sizes)`         | åˆ›å»ºå…¨ä¸º 0 çš„ Tensor                 |
| `eye(*sizes)`           | åˆ›å»ºå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä»–ä¸º 0 çš„ Tensor         |
| `arange(s,Â e,Â step)`    | åˆ›å»ºä» s åˆ° eï¼Œæ­¥é•¿ä¸º step çš„ Tensor     |
| `linspace(s,Â e,Â steps)` | åˆ›å»ºä» s åˆ° eï¼Œå‡åŒ€åˆ‡åˆ†æˆ steps ä»½çš„ Tensor |
| `rand/randn(*sizes)`    | åˆ›å»ºå‡åŒ€/æ ‡å‡†åˆ†å¸ƒçš„ Tensor               |
| `normal(mean,Â std)`     | åˆ›å»ºæ­£æ€åˆ†å¸ƒåˆ†å¸ƒçš„ Tensor                |
| `randperm(m)`           | åˆ›å»ºéšæœºæ’åˆ—çš„ Tensor                  |

| æ–¹æ³•                                 | æè¿°                |
| ---------------------------------- | ----------------- |
| `mean`Â /Â `sum`Â /Â `median`Â /Â `mode` | å‡å€¼ / å’Œ / ä¸­ä½æ•° / ä¼—æ•° |
| `norm`Â /Â `dist`                    | èŒƒæ•° / è·ç¦»           |
| `std`Â /Â `var`                      | æ ‡å‡†å·® / æ–¹å·®          |
| `cumsum`Â /Â `cumprod`               | ç´¯åŠ  / ç´¯ä¹˜           |

| æ–¹æ³•                                                      | æè¿°                             |
| ------------------------------------------------------- | ------------------------------ |
| `abs`Â /Â `sqrt`Â /Â `div`Â /Â `exp`Â /Â `fmod`Â /Â `log`Â /Â `pow` | ç»å¯¹å€¼ / å¹³æ–¹æ ¹ / é™¤æ³• / æŒ‡æ•° / æ±‚ä½™ / æ±‚å¹‚â€¦ |
| `cos`Â /Â `sin`Â /Â `asin`Â /Â `atan2`Â /Â `cosh`               | ä¸‰è§’å‡½æ•°                           |
| `ceil`Â /Â `round`Â /Â `floor`Â /Â `trunc`                    | ä¸Šå–æ•´ / å››èˆäº”å…¥ / ä¸‹å–æ•´ / åªä¿ç•™æ•´æ•°éƒ¨åˆ†     |
| `clamp(input,Â min,Â max)`                                | è¶…è¿‡ min å’Œ max éƒ¨åˆ†æˆªæ–­              |
| `sigmod`Â /Â `tanh`                                       | å¸¸ç”¨æ¿€æ´»å‡½æ•°                         |
#### çº¿æ€§ä»£æ•°

| æ–¹æ³•              | æè¿°          |
| --------------- | ----------- |
| `trace`         | å¯¹è§’çº¿å…ƒç´ ä¹‹å’Œ     |
| `diag`          | å¯¹è§’çº¿å…ƒç´        |
| `triu`Â /Â `tril` | ä¸Šä¸‰è§’ / ä¸‹ä¸‰è§’çŸ©é˜µ |
| `mm`            | çŸ©é˜µä¹˜æ³•        |
| `t`             | è½¬ç½®          |
| `inverse`       | æ±‚é€†çŸ©é˜µ        |
| `svd`           | å¥‡å¼‚å€¼åˆ†è§£       |

```python
# çŸ©é˜µçš„å‰ä¹˜
b.mm(a), b.matmul(a)
```


squeeze: å‡å°‘ç»´åº¦
unsqueeze: å‰é¢å¢åŠ ç»´åº¦

#### ç´¢å¼•ã€åˆ‡ç‰‡ã€å˜æ¢


`reshape()`ï¼Œ`resize()`Â å’ŒÂ `view()`ï¼Œä¸‰è€…ç›´æ¥çš„åŒºåˆ«åœ¨äºï¼š`resize()`Â å’ŒÂ `view()`Â æ‰§è¡Œå˜æ¢æ—¶å’ŒåŸ Tensor å…±äº«å†…å­˜ï¼Œå³ä¿®æ”¹ä¸€ä¸ªï¼Œå¦å¤–ä¸€ä¸ªä¹Ÿä¼šè·Ÿç€æ”¹å˜ã€‚è€ŒÂ `reshape()`Â åˆ™ä¼šå¤åˆ¶åˆ°æ–°çš„å†…å­˜åŒºå—ä¸Š

```python
c = t.rand(5, 4)

# å–ç¬¬ 1 è¡Œ
c[0]

# å–ç¬¬ 1 åˆ—
c[:, 0]

# å½¢çŠ¶åšå‡ºæ”¹å˜
c.reshape(4, 5)
c.view(4, 5)
```


#### å¼ é‡ç»“æ„

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240613003057.png)

#### è‡ªåŠ¨å¾®åˆ† Autograd

æ ¹æ®å‰å‘ä¼ æ’­è¿‡ç¨‹è‡ªåŠ¨æ„å»ºè®¡ç®—å›¾ï¼Œå¹¶è‡ªåŠ¨å®Œæˆåå‘ä¼ æ’­è€Œä¸éœ€è¦æ‰‹åŠ¨å»å®ç°åå‘ä¼ æ’­çš„è¿‡ç¨‹

torch.Tensor
- `data`ï¼šæ•°æ®ï¼Œä¹Ÿå°±æ˜¯å¯¹åº”çš„ Tensorã€‚
- `grad`ï¼šæ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯ Tensor å¯¹åº”çš„æ¢¯åº¦ï¼Œæ³¨æ„Â `grad`Â åŒæ ·æ˜¯Â `torch.Tensor`Â å¯¹è±¡ã€‚
- `grad_fn`ï¼šæ¢¯åº¦å‡½æ•°ï¼Œç”¨äºæ„å»ºè®¡ç®—å›¾å¹¶è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

#### æ•°æ®åŠ è½½å™¨

```python
import torch

# è®­ç»ƒæ•°æ®æ‰“ä¹±ï¼Œä½¿ç”¨ 64 å°æ‰¹é‡
train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=64, shuffle=True)

# æµ‹è¯•æ•°æ®æ— éœ€æ‰“ä¹±ï¼Œä½¿ç”¨ 64 å°æ‰¹é‡
test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=64, shuffle=False)
```

#### æ„å»ºç¥ç»ç½‘ç»œ

```python
# é€šè¿‡åŸºç¡€ api ä¸€æ­¥ä¸€æ­¥æ„å»º
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 512)  # 784 æ˜¯å› ä¸ºè®­ç»ƒæ˜¯æˆ‘ä»¬ä¼šæŠŠ 28*28 å±•å¹³
        self.fc2 = nn.Linear(512, 128)  # ä½¿ç”¨ nn ç±»åˆå§‹åŒ–çº¿æ€§å±‚ï¼ˆå…¨è¿æ¥å±‚ï¼‰
        self.fc3 = nn.Linear(128, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))  # ç›´æ¥ä½¿ç”¨ relu å‡½æ•°ï¼Œä¹Ÿå¯ä»¥è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ª nn ä¸‹é¢çš„ Relu ç±»ä½¿ç”¨
        x = F.relu(self.fc2(x))
        x = self.fc3(x)  # è¾“å‡ºå±‚ä¸€èˆ¬ä¸æ¿€æ´»
        return x


# Sequential å®¹å™¨ç»“æ„
model_s = nn.Sequential(
    nn.Linear(784, 512),  # çº¿æ€§ç±»
    nn.ReLU(),  # æ¿€æ´»å‡½æ•°ç±»
    nn.Linear(512, 128),
    nn.ReLU(),
    nn.Linear(128, 10),
)


```

#### åŸºç¡€ç”¨æ³•

```python
# åˆ›å»ºå¼ é‡
t.Tensor([1, 2, 3])

import numpy as np
t.Tensor(np.random.randn(3))

# é€šè¿‡Â `shape`Â æŸ¥çœ‹ Tensor çš„å½¢çŠ¶
t.Tensor([[1, 2], [3, 4], [5, 6]]).shape

# æ•°å­¦è¿ç®—
a = t.Tensor([[1, 2], [3, 4]])
b = t.Tensor([[5, 6], [7, 8]])

print(a + b)
print(a - b)

# å¯¹ a æ±‚åˆ—å¹³å‡
a.mean(dim=0)

# a ä¸­çš„æ¯ä¸ªå…ƒç´ æ±‚å¹³æ–¹
a.pow(2)


```




### TensorFlow

åŸºç¡€å±æ€§ï¼šæ•°æ®ï¼Œæ•°æ®ç±»å‹å’Œå½¢çŠ¶

- tf.Variableï¼šå˜é‡ Tensorï¼Œéœ€è¦æŒ‡å®šåˆå§‹å€¼ï¼Œå¸¸ç”¨äºå®šä¹‰å¯å˜å‚æ•°ï¼Œä¾‹å¦‚ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚

- tf.constantï¼šå¸¸é‡ Tensorï¼Œéœ€è¦æŒ‡å®šåˆå§‹å€¼ï¼Œå®šä¹‰ä¸å˜åŒ–çš„å¼ é‡ã€‚

```python
import tensorflow as tf

v = tf.Variable([[1, 2], [3, 4]])  # å½¢çŠ¶ä¸º (2, 2) çš„äºŒç»´å˜é‡
v
```

```python
c = tf.constant([[1, 2], [3, 4]])  # å½¢çŠ¶ä¸º (2, 2) çš„äºŒç»´å¸¸é‡
c
```

```python
# è¾“å‡ºå¼ é‡çš„ NumPy æ•°ç»„
c.numpy()
```

#### å¸¸ç”¨æ–¹æ³•

- tf.zerosï¼šæ–°å»ºæŒ‡å®šå½¢çŠ¶ä¸”å…¨ä¸º 0 çš„å¸¸é‡ Tensor

- tf.zeros_likeï¼šå‚è€ƒæŸç§å½¢çŠ¶ï¼Œæ–°å»ºå…¨ä¸º 0 çš„å¸¸é‡ Tensor

- tf.onesï¼šæ–°å»ºæŒ‡å®šå½¢çŠ¶ä¸”å…¨ä¸º 1 çš„å¸¸é‡ Tensor

- tf.ones_likeï¼šå‚è€ƒæŸç§å½¢çŠ¶ï¼Œæ–°å»ºå…¨ä¸º 1 çš„å¸¸é‡ Tensor

- tf.fillï¼šæ–°å»ºä¸€ä¸ªæŒ‡å®šå½¢çŠ¶ä¸”å…¨ä¸ºæŸä¸ªæ ‡é‡å€¼çš„å¸¸é‡ Tensor

- tf.linspaceï¼šåˆ›å»ºä¸€ä¸ªç­‰é—´éš”åºåˆ—ã€‚

- tf.rangeï¼šåˆ›å»ºä¸€ä¸ªæ•°å­—åºåˆ—ã€‚

- tf.matmul: çŸ©é˜µä¹˜æ³•

```python
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
b = tf.constant([7.0, 8.0, 9.0, 10.0, 11.0, 12.0], shape=[3, 2])

c = tf.linalg.matmul(a, b)  # çŸ©é˜µä¹˜æ³•
c
```

- tf.linalg.matrix_transposeï¼šè½¬ç½®çŸ©é˜µ
- tf.castï¼šæ•°æ®ç±»å‹è½¬æ¢
	- `tf.cast(tf.constant(df[["X0", "X1"]].values), tf.float32)`
- tf.nn.sigmoid: sigmoid å‡½æ•°
- tf.losses.mean_squared_errorï¼šMSE æŸå¤±å‡½æ•°
- tf.reduce_meanï¼šæ±‚å’Œ?
- tf.optimizers.SGDï¼šéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
- tf.random.normalï¼šç”Ÿæˆéšæœºæ•°
- tf.nn.reluï¼šrelu å‡½æ•°
- tf.optimizers.Adamï¼š adam ä¼˜åŒ–å™¨
- tf.argmaxï¼šå–å€¼æœ€å¤§çš„ç´¢å¼•
- matrix_inverse: 

#### å¸¸ç”¨æ¨¡å—

- tf.ï¼šåŒ…å«äº†å¼ é‡å®šä¹‰ï¼Œå˜æ¢ç­‰å¸¸ç”¨å‡½æ•°å’Œç±»ã€‚

- tf.dataï¼šè¾“å…¥æ•°æ®å¤„ç†æ¨¡å—ï¼Œæä¾›äº†åƒ tf.data.Dataset ç­‰ç±»ç”¨äºå°è£…è¾“å…¥æ•°æ®ï¼ŒæŒ‡å®šæ‰¹é‡å¤§å°ç­‰ã€‚

- tf.imageï¼šå›¾åƒå¤„ç†æ¨¡å—ï¼Œæä¾›äº†åƒå›¾åƒè£å‰ªï¼Œå˜æ¢ï¼Œç¼–ç ï¼Œè§£ç ç­‰ç±»ã€‚

- tf.kerasï¼šåŸ Keras æ¡†æ¶é«˜é˜¶ APIã€‚åŒ…å«åŸ tf.layers ä¸­é«˜é˜¶ç¥ç»ç½‘ç»œå±‚ã€‚

- tf.linalgï¼šçº¿æ€§ä»£æ•°æ¨¡å—ï¼Œæä¾›äº†å¤§é‡çº¿æ€§ä»£æ•°è®¡ç®—æ–¹æ³•å’Œç±»ã€‚

- tf.lossesï¼šæŸå¤±å‡½æ•°æ¨¡å—ï¼Œç”¨äºæ–¹ä¾¿ç¥ç»ç½‘ç»œå®šä¹‰æŸå¤±å‡½æ•°ã€‚

- tf.mathï¼šæ•°å­¦è®¡ç®—æ¨¡å—ï¼Œæä¾›äº†å¤§é‡æ•°å­¦è®¡ç®—å‡½æ•°ã€‚

- tf.saved_modelï¼šæ¨¡å‹ä¿å­˜æ¨¡å—ï¼Œå¯ç”¨äºæ¨¡å‹çš„ä¿å­˜å’Œæ¢å¤ã€‚

- tf.trainï¼šæä¾›ç”¨äºè®­ç»ƒçš„ç»„ä»¶ï¼Œä¾‹å¦‚ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡è¡°å‡ç­–ç•¥ç­‰ã€‚

- tf.nnï¼šæä¾›ç”¨äºæ„å»ºç¥ç»ç½‘ç»œçš„åº•å±‚å‡½æ•°ï¼Œä»¥å¸®åŠ©å®ç°æ·±åº¦ç¥ç»ç½‘ç»œå„ç±»åŠŸèƒ½å±‚ã€‚

- tf.estimatorï¼šé«˜é˜¶ APIï¼Œæä¾›äº†é¢„åˆ›å»ºçš„ Estimator æˆ–è‡ªå®šä¹‰ç»„ä»¶


####  tf.keras

- `tf.keras.layers.Conv1D`Â [ğŸ”—](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Conv1D)ï¼šä¸€èˆ¬ç”¨äºæ–‡æœ¬æˆ–æ—¶é—´åºåˆ—ä¸Šçš„ä¸€ç»´å·ç§¯ã€‚
    
- `tf.keras.layers.Conv2D`Â [ğŸ”—](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Conv2D)ï¼šä¸€èˆ¬ç”¨äºå›¾åƒç©ºé—´ä¸Šçš„äºŒç»´å·ç§¯ã€‚
    
- `tf.keras.layers.Conv3D`Â [ğŸ”—](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Conv3D)ã€‚ä¸€èˆ¬ç”¨äºå¤„ç†è§†é¢‘ç­‰åŒ…å«å¤šå¸§å›¾åƒä¸Šçš„ä¸‰ç»´å·ç§¯ã€‚

	- `filters`: å·ç§¯æ ¸æ•°é‡ï¼Œæ•´æ•°ã€‚
    
	- `kernel_size`: å·ç§¯æ ¸å°ºå¯¸ï¼Œå…ƒç»„ã€‚
    
	- `strides`: å·ç§¯æ­¥é•¿ï¼Œå…ƒç»„ã€‚
    
	- `padding`:Â `"valid"`Â æˆ–Â `"same"`ã€‚
		- valid: æ— æ³•è¢«å·ç§¯çš„åƒç´ å°†è¢«ä¸¢å¼ƒ
		- same: é€šè¿‡Â `0`Â å¡«è¡¥ä¿è¯æ¯ä¸€ä¸ªè¾“å…¥åƒç´ éƒ½èƒ½è¢«å·ç§¯
- tf.keras.layers.AveragePooling2D: å¹³å‡æ± åŒ–
- tf.keras.layers.Flatten()ï¼šå±•å¼€æ•°æ®ï¼Œæœ€åè¿›è¡Œå…¨è¿æ¥æ—¶ä½¿ç”¨ï¼Ÿ
- tf.keras.preprocessing.image.load_imgï¼š load image to PIL Image instanceï¼Œå¯ä»¥ç›´æ¥ç”¨äºå±•ç¤º
- tf.keras.utils.img_to_arrayï¼š Converts a PIL Image instance to a NumPy array.



##### é¡ºåºæ¨¡å‹

å¤§å¤§ç®€åŒ–äº†æ¨¡å‹å®šä¹‰è¿‡ç¨‹

[å…¨éƒ¨ä¼˜åŒ–å™¨åˆ—è¡¨](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers)
[å…¨éƒ¨æŸå¤±å‡½æ•°åˆ—è¡¨](https://tensorflow.google.cn/api_docs/python/tf/losses)
[åç§°åˆ—è¡¨](https://keras.io/zh/losses/)


```python
model = tf.keras.models.Sequential()  # å®šä¹‰é¡ºåºæ¨¡å‹

# æ·»åŠ å…¨è¿æ¥å±‚
model.add(tf.keras.layers.Dense(units=30, activation=tf.nn.relu))  # è¾“å‡º 30ï¼Œrelu æ¿€æ´»
model.add(tf.keras.layers.Dense(units=10, activation=tf.nn.softmax))  # è¾“å‡º 10ï¼Œsoftmax æ¿€æ´»

# adam ä¼˜åŒ–å™¨ + äº¤å‰ç†µæŸå¤± + å‡†ç¡®åº¦è¯„ä¼°
model.compile(
    optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
)

# æŸå¤±å‡½æ•°éœ€è¦æ ¹æ®ç½‘ç»œçš„è¾“å‡ºå½¢çŠ¶å’ŒçœŸå®å€¼çš„å½¢çŠ¶æ¥å†³å®š

# æ¨¡å‹è®­ç»ƒ
model.fit(X_train, y_train, batch_size=64, epochs=5)

# æ¨¡å‹è¯„ä¼°
model.evaluate(X_test, y_test)

# ä½¿ç”¨å‚æ•°ä¼ å…¥æµ‹è¯•æ•°æ®
model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_test, y_test))
```


**å…¨è¿æ¥å±‚**

Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

- **units**: æ­£æ•´æ•°ï¼Œè¾“å‡ºç©ºé—´ç»´åº¦ã€‚
- **activation**: æ¿€æ´»å‡½æ•°ã€‚è‹¥ä¸æŒ‡å®šï¼Œåˆ™ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°(å³ï¼Œ çº¿æ€§æ¿€æ´»:Â `a(x)Â =Â x`)ã€‚
- **use_bias**: å¸ƒå°”å€¼ï¼Œè¯¥å±‚æ˜¯å¦ä½¿ç”¨åç½®é¡¹é‡ã€‚
- **kernel_initializer**:Â `kernel`Â æƒå€¼çŸ©é˜µçš„åˆå§‹åŒ–å™¨ã€‚
- **bias_initializer**: åç½®é¡¹é‡çš„åˆå§‹åŒ–å™¨.
- **kernel_regularizer**: è¿ç”¨åˆ°Â `kernel`Â æƒå€¼çŸ©é˜µçš„æ­£åˆ™åŒ–å‡½æ•°ã€‚
- **bias_regularizer**: è¿ç”¨åˆ°åç½®é¡¹çš„æ­£åˆ™åŒ–å‡½æ•°ã€‚
- **activity_regularizer**: è¿ç”¨åˆ°å±‚çš„è¾“å‡ºçš„æ­£åˆ™åŒ–å‡½æ•°ã€‚
- **kernel_constraint**: è¿ç”¨åˆ°Â `kernel`Â æƒå€¼çŸ©é˜µçš„çº¦æŸå‡½æ•°ã€‚
- **bias_constraint**: è¿ç”¨åˆ°åç½®é¡¹é‡çš„çº¦æŸå‡½æ•°ã€‚


##### å‡½æ•°æ¨¡å‹

```python
inputs = tf.keras.Input(shape=(64,))  # è¾“å…¥å±‚
x = tf.keras.layers.Dense(units=30, activation="relu")(inputs)  # ä¸­é—´å±‚
outputs = tf.keras.layers.Dense(units=10, activation="softmax")(x)  # è¾“å‡ºå±‚

# å‡½æ•°å¼ API éœ€è¦æŒ‡å®šè¾“å…¥å’Œè¾“å‡º
model = tf.keras.Model(inputs=inputs, outputs=outputs)
model


model.compile(
    optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
)

model.fit(X_train, y_train, batch_size=64, epochs=20, validation_data=(X_test, y_test))
```


##### æ¨¡å‹ä¿å­˜

TensorFlow æ¨¡å‹ä¸€èˆ¬åŒ…å« 3 ç±»è¦ç´ ï¼Œåˆ†åˆ«æ˜¯ï¼šæ¨¡å‹æƒé‡å€¼ã€æ¨¡å‹é…ç½®ä¹ƒè‡³ä¼˜åŒ–å™¨é…ç½®

```python
model.save_weights("./weights/model")  # ä¿å­˜æ£€æŸ¥ç‚¹åç§°ä¸º modelï¼Œè·¯å¾„ä¸º ./weights

model.load_weights("./weights/model")  # æ¢å¤æ£€æŸ¥ç‚¹

# ä¿å­˜å®Œæ•´çš„æ¨¡å‹ï¼Œå³åŒ…å«æ¨¡å‹æƒé‡å€¼ã€æ¨¡å‹é…ç½®ä¹ƒè‡³ä¼˜åŒ–å™¨é…ç½®ç­‰ï¼Œæ¨¡å‹ç›´æ¥æ‹¿å»æ¨ç†
model.save("model.h5")  # ä¿å­˜å®Œæ•´æ¨¡å‹

model_ = tf.keras.models.load_model("model.h5")  # è°ƒç”¨æ¨¡å‹

# æŸ¥çœ‹ Keras æ¨¡å‹ç»“æ„ï¼ŒåŒ…å«ç¥ç»ç½‘ç»œå±‚å’Œå‚æ•°ç­‰è¯¦ç»†æ•°æ®
model_.summary()

preds = model_.predict(X_test[:3])  # é¢„æµ‹å‰ 3 ä¸ªæµ‹è¯•æ ·æœ¬
preds
```

#### Estimator é«˜é˜¶ API

TensorFlow ä¸­çš„é«˜é˜¶ APIï¼Œå®ƒå¯ä»¥å°†æ¨¡å‹çš„è®­ç»ƒã€é¢„æµ‹ã€è¯„ä¼°ã€å¯¼å‡ºç­‰æ“ä½œå°è£…åœ¨ä¸€èµ·ï¼Œæ„æˆä¸€ä¸ª Estimator


ä¸€èˆ¬æ­¥éª¤ï¼š
1. åˆ›å»ºä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å‡½æ•°ã€‚
2. å®šä¹‰æ¨¡å‹çš„ç‰¹å¾åˆ—ã€‚
3. å®ä¾‹åŒ– Estimatorï¼ŒæŒ‡å®šç‰¹å¾åˆ—å’Œå„ç§è¶…å‚æ•°ã€‚
4. åœ¨ Estimator å¯¹è±¡ä¸Šè°ƒç”¨ä¸€ä¸ªæˆ–å¤šä¸ªæ–¹æ³•ï¼Œä¼ é€’é€‚å½“çš„è¾“å…¥å‡½æ•°ä½œä¸ºæ•°æ®çš„æ¥æº


#### api æ€»ç»“

å°è£…ç¨‹åº¦ï¼š tf.estimator > tf.keras > tf.nn


### mlxtend

#### mlxtend.preprocessing

**TransactionEncoder**

å°†åˆ—è¡¨æ•°æ®è½¬æ¢ä¸º Apriori ç®—æ³• API å¯ç”¨çš„æ ¼å¼

ç±»ä¼¼äºç‹¬çƒ­ç¼–ç ï¼Œå¯ä»¥æå–æ•°æ®é›†ä¸­çš„ä¸é‡å¤é¡¹ï¼Œå¹¶å°†æ¯ä¸ªæ•°æ®è½¬æ¢ä¸ºç­‰é•¿åº¦çš„å¸ƒå°”å€¼è¡¨ç¤º

#### mlxtend.frequent_patterns

- aprioriï¼šå¯»æ‰¾é¢‘ç¹é¡¹é›†
- association_rulesï¼šç”Ÿæˆå…³è”è§„åˆ™

### matplotlib

#### matplotlib.pyplot

- plot: ç”»çº¿
- scatterï¼šç”»æ•£ç‚¹
  - alphaï¼š ç»˜åˆ¶ç‚¹çš„é€æ˜åº¦
  - c: ç»˜åˆ¶ç‚¹çš„é¢œè‰²ï¼Ÿ
  - cmapï¼šæŒ‰ç±»åˆ«è¿›è¡Œé¢œè‰²åˆå¹¶ï¼Ÿ
- legend: ç»˜åˆ¶LabeL
- imshowï¼šç»˜åˆ¶å›¾åƒï¼Œåªè¦æ˜¯äºŒç»´æ•°æ®å³å¯ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†
- ylimï¼šé™å®š y è½´èŒƒå›´

```py
from matplotlib import pyplot as plt

# ä¸å¼¹çª—ç”»å›¾ï¼Œjupyter bookä¸­ï¼Ÿ
%matplotlib inline
```


### numpy

- numpy.sum: å¯ä»¥å¤„ç†å¤šç»´æ•°ç»„çš„åŠ å’Œ
- np.random.shuffleï¼šæ´—ç‰Œç®—æ³•?
- poly1dï¼šæ¥æ”¶æ•°ç»„ç”Ÿæˆå¤šé¡¹å¼
- np.linspace: é€šè¿‡å®šä¹‰å‡åŒ€é—´éš”åˆ›å»ºæ•°å€¼åºåˆ—,
  - linspace(start,end,size) sizeæ˜¯é—´éš”ï¼Œç®—ä¸Šç»ˆç‚¹å’Œèµ·å§‹
  - 0,100,11, å³ 0 ~ 100ï¼Œå¹¶ä¸”åŠ ä¸Š0ã€80æ€»å…±11ä¸ªå€¼ï¼Œ åˆšå¥½æ˜¯0ã€1ã€2ã€3ã€4ã€5ã€6ã€7ã€8ã€9ã€10ï¼Œ 11ä¸ªæ•°å€¼
- np.martix.I: é€†çŸ©é˜µ A^-1
- argmin: æ’åºåæœ€å°çš„å…ƒç´ çš„ç´¢å¼•
- argsortï¼šä»å°åˆ°å¤§æ’åºåçš„ç´¢å¼•æ•°ç»„
- covï¼šåæ–¹å·®çŸ©é˜µè®¡ç®—
- matï¼šè½¬çŸ©é˜µï¼Ÿ
- np.linalg.eigï¼šè®¡ç®—ç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡
- np.linalg.norm: è®¡ç®—æ¬§æ°è·ç¦»
- dot: ç‚¹ä¹˜
- onesï¼šæ ¹æ®shapeï¼Œç”Ÿæˆå…¨ 1 çš„æ•°ç»„
- subtract: å‡æ³•
- mean: å¹³å‡å€¼
- expand_dimsï¼šåœ¨æŒ‡å®šçš„ä½ç½®æ’å…¥ä¸€ä¸ªæ–°çš„è½´ï¼Œä»è€Œæ‰©å±•æ•°ç»„çš„ç»´åº¦ï¼Œå‡è®¾ image_a åŸæ¥çš„å½¢çŠ¶æ˜¯ (height, width, channels)ï¼Œé‚£ä¹ˆç»è¿‡ np.expand_dims ä¹‹åï¼Œå®ƒçš„å½¢çŠ¶å°†å˜æˆ (1, height, width, channels)

### pandas

#### è¿ç®—ç¬¦

- [[]]ï¼šå–å‡ºå¤šåˆ—,æ•°æ®ç±»å‹æ˜¯dataframeï¼Œåˆ—åä¼šä½œä¸ºè¡¨å¤´ï¼Œåœ¨å±•ç¤ºæ—¶ä½¿ç”¨ï¼Œè®¡ç®—æ—¶ä¼šå¿½ç•¥åˆ—å
- []ï¼šå–å‡ºå•åˆ—,ç±»å‹æ˜¯Seriesï¼Œåˆ—ååªä½œä¸ºæ ‡è®°ï¼Ÿä¸ä¼šè¢«å±•ç¤º

#### å¸¸ç”¨æ–¹æ³•

- get_dummiesï¼šç‹¬çƒ­ç¼–ç 
- headï¼šè¯»å–å‰5è¡Œ
- tail: è¯»å–æœ€å5è¡Œ
- columns: è¯»å–æˆ–è€…æ›´æ–°è¡¨å¤´
- describe: å®ƒç”¨äºç”Ÿæˆæœ‰å…³æ•°æ®çš„ç»Ÿè®¡æ‘˜è¦ã€‚è¿™ä¸ªç»Ÿè®¡æ‘˜è¦åŒ…æ‹¬äº†æ•°æ®åˆ—çš„æ•°é‡ã€å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°å€¼ã€25% åˆ†ä½æ•°ã€ä¸­ä½æ•°ï¼ˆ50% åˆ†ä½æ•°ï¼‰ã€75% åˆ†ä½æ•°å’Œæœ€å¤§å€¼
- 

**locã€iloc**

locï¼šæŒ‰ç…§è¡Œã€åˆ— labelè¿›è¡Œé€‰å–
loc[[False, False, True]]ï¼šFalseæ„ä¸ºè·³è¿‡ï¼Œä¸é€‰å–ï¼ŒTrueè‡ªç„¶ä¸ºé€‰å–
iloc æ˜¯åŸºäºè¡Œåˆ—çš„ä½ç½®ï¼Œè€Œélabel,æ‰€æœ‰è¡Œï¼Œåˆ° åˆ—ä½ç½® -1ï¼Ÿ

ature_data = lilac_data.iloc[:, :-1]
loc(): åŸºäºlabelï¼ˆæˆ–è€…æ˜¯booleanæ•°ç»„ï¼‰è¿›è¡Œæ•°æ®é€‰æ‹©
iloc(): åŸºäºposition(æ•´æ•°-integer)è¿›è¡Œæ•°æ®é€‰æ‹©

**read_csv**

```python
pd.read_csv(xx) # è¯»å–æ•°æ®ï¼Œè¡Œé»˜è®¤ä»¥æ•°å­—ä½œä¸ºç´¢å¼•
pd.read_csv(xxï¼Œhead=None) # å¿½ç•¥è¡Œï¼Ÿ

    # è¯»å–æ•°æ®ï¼Œå¹¶ä¸”ä»¥ç¬¬ä¸€åˆ—ä½œä¸ºç´¢å¼•ï¼Œä¸è¦é»˜è®¤çš„ 1ã€2ã€3.....äº†
    df = pd.read_csv("GOOGL.csv", index_col=0)
```

**resample**

```python
# é‡é‡‡æ ·ï¼Œå¯¹æ¯ä¸€åˆ—è¿›è¡Œèšåˆï¼Œå–å¹³å‡æˆ–åˆ™æ€»å’Œï¼Œ
# Q æ˜¯æŒ‰å­£åº¦çš„æ„æ€
df = df.resample('Q').agg({"Open": 'mean', "High": 'mean', "Low": 'mean',
                               "Close": 'mean', "Adj Close": 'mean', "Volume": 'sum'})
```
**sort_values**

```python
# æŒ‰ Volume æ’åº
df = df.sort_values(by='Volume', ascending=False)
```

**to_numeric**

```python

# å°†æŸä¸€åˆ—æ•°æ®è½¬æˆæ•°å€¼
df['Rings'] = pd.to_numeric(df['Rings'])
```

**drop**

```python

# æ ¹æ®indexåˆ é™¤ï¼Ÿåˆ é™¤æœ€åä¸€è¡Œ
dx.drop(dx.index[-1])
```

**replace**

```python
# å°†æŸä¸€åˆ—çš„æŸäº›å€¼è¿›è¡Œæ›´æ–°ï¼Œreplaceæ–¹æ³•å·²ç»è¢«åºŸå¼ƒï¼
df['Sex'] = df.Sex.replace({'M':0, 'F':1, 'I':2})
```

**cut**

```python

# æ ¹æ®binså¯¹æ•°æ®å€¼åˆ’åˆ†åŒºé—´ï¼Œç„¶ååˆ†åˆ«æ›¿æ¢
df['Rings'] = pd.cut(df.Rings, bins=[0, 10, 20, 30], labels=['small','middle','large'])
```

**concat**

axis é»˜è®¤ä¸º0,ä¹Ÿå°±æ˜¯çºµå‘ä¸Šè¿›è¡Œåˆå¹¶ã€‚æ²¿ç€è¿æ¥çš„è½´,1 å°±æ˜¯æ¨ªå‘åˆå¹¶
1ï¼š concatå°±æ˜¯è¡Œå¯¹é½ï¼Œç„¶åå°†ä¸åŒåˆ—åç§°çš„ä¸¤å¼ è¡¨åˆå¹¶
print(pd.concat([features, target], axis=1).head())

#### pandas.plotting

- autocorrelation_plotï¼šç»˜åˆ¶è‡ªç›¸å…³å›¾

### jupyter notebook

å‘½ä»¤jupyter notebookè¿è¡Œ

- `%matplotlib inline`:çš„ä½œç”¨æ˜¯å°†Matplotlibå›¾å½¢åµŒå…¥åˆ°Notebookå•å…ƒæ ¼ä¸­ï¼Œä½¿å¾—å›¾å½¢èƒ½å¤Ÿåœ¨Notebookä¸­ç›´æ¥æ˜¾ç¤ºï¼Œè€Œä¸æ˜¯åœ¨æ–°çª—å£ä¸­å¼¹å‡º
- input('xxx'): åœ¨ note book ä¸Šæ˜¾ç¤ºè¾“å…¥æ¡†


### sklearn

#### sklearn.cluster

- MeanShiftï¼šå‡å€¼æ¼‚ç§»èšç±»

- AffinityPropagationï¼šäº²å’Œä¼ æ’­èšç±»
  - dampingï¼šé˜»å°¼å› å­ï¼Œé¿å…æ•°å€¼æŒ¯è¡ã€‚
  - max_iterï¼šæœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚
  - affinityï¼šäº²å’Œè¯„ä»·æ–¹æ³•ï¼Œé»˜è®¤ä¸ºæ¬§å¼è·ç¦»ã€‚
- SpectralClusteringï¼šè°±èšç±»
- DBSCANï¼šå¯†åº¦èšç±»
- hierarchy
- linkageï¼šè¿›è¡Œå±‚æ¬¡èšç±»/å‡èšèšç±»
- dendrogramï¼šç»˜åˆ¶èšç±»æ•°
- Birchï¼šBirch èšç±»
- MiniBatchKMeans

**AgglomerativeClustering**

å±‚æ¬¡èšç±»

n_clusters: è¡¨ç¤ºæœ€ç»ˆè¦æŸ¥æ‰¾ç±»åˆ«çš„æ•°é‡ï¼Œä¾‹å¦‚ä¸Šé¢çš„ 2 ç±»ã€‚

metric: æœ‰ euclideanï¼ˆæ¬§å¼è·ç¦»ï¼‰, l1ï¼ˆL1 èŒƒæ•°ï¼‰, l2ï¼ˆL2 èŒƒæ•°ï¼‰, manhattanï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼‰ç­‰å¯é€‰ã€‚

linkage: è¿æ¥æ–¹æ³•ï¼šwardï¼ˆå•è¿æ¥ï¼‰, completeï¼ˆå…¨è¿æ¥ï¼‰, averageï¼ˆå¹³å‡è¿æ¥ï¼‰å¯é€‰ã€‚

**k_means**

èšç±»ç›´æ¥å®ç°

- `X`ï¼šè¡¨ç¤ºéœ€è¦èšç±»çš„æ•°æ®ã€‚
    
- `n_clusters`ï¼šè¡¨ç¤ºèšç±»çš„ä¸ªæ•°ï¼Œä¹Ÿå°±æ˜¯ K å€¼ã€‚


#### sklearn.decomposition

**PCA**

æ•°æ®é™ç»´

n_components= è¡¨ç¤ºéœ€è¦ä¿ç•™ä¸»æˆåˆ†ï¼ˆç‰¹å¾ï¼‰çš„æ•°é‡ã€‚

copy= è¡¨ç¤ºé’ˆå¯¹åŸå§‹æ•°æ®é™ç»´è¿˜æ˜¯é’ˆå¯¹åŸå§‹æ•°æ®å‰¯æœ¬é™ç»´ã€‚å½“å‚æ•°ä¸º False æ—¶ï¼Œé™ç»´åçš„åŸå§‹æ•°æ®ä¼šå‘ç”Ÿæ”¹å˜ï¼Œè¿™é‡Œé»˜è®¤ä¸º Trueã€‚

whiten= ç™½åŒ–è¡¨ç¤ºå°†ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§é™ä½ï¼Œå¹¶ä½¿å¾—æ¯ä¸ªç‰¹å¾å…·æœ‰ç›¸åŒçš„æ–¹å·®ã€‚

svd_solver= è¡¨ç¤ºå¥‡å¼‚å€¼åˆ†è§£ SVD çš„æ–¹æ³•ã€‚æœ‰ 4 å‚æ•°ï¼Œåˆ†åˆ«æ˜¯ï¼šauto, full, arpack, randomizedã€‚


#### sklearn.linear_model

- LogisticRegression
- mean_absolute_error: mae
- mean_squared_error: mse
**Ridge**

å²­å›å½’

```py
ridge_model = Ridge(fit_intercept=False)  # å‚æ•°ä»£è¡¨ä¸å¢åŠ æˆªè·é¡¹
ridge_model.fit(x, y)
ridge_model.coef_  # æ‰“å°æ¨¡å‹å‚æ•°
```

**Lasso**

```py
lasso = Lasso(alpha=a, fit_intercept=False)
lasso.fit(x, y)
lasso.coef_
```

**LinearRegression**

çº¿æ€§å›å½’æ¨¡å‹

```py
x = np.array([56, 72, 69, 88, 102, 86, 76, 79, 94, 74])
y = np.array([92, 102, 86, 110, 130, 99, 96, 102, 105, 92])

model = LinearRegression()
model.fit(x.reshape(x.shape[0], 1), y)  # è®­ç»ƒ, reshape æ“ä½œæŠŠæ•°æ®å¤„ç†æˆ fit èƒ½æ¥å—çš„å½¢çŠ¶

# å¾—åˆ°æ¨¡å‹æ‹Ÿåˆå‚æ•°
model.intercept_, model.coef_
```

#### sklearn.model_selection

- KFoldï¼šè¿›è¡ŒKæŠ˜æ•°æ®
- cross_val_scoreï¼škæŠ˜æ•°æ®ï¼Œäº¤å‰éªŒè¯


#### sklearn.naive_bayes

ä¼¯åŠªåˆ©æ¨¡å‹

**train_test_split**

```py
# X_train,X_test, y_train, y_test åˆ†åˆ«è¡¨ç¤ºï¼Œåˆ‡åˆ†åçš„ç‰¹å¾çš„è®­ç»ƒé›†ï¼Œç‰¹å¾çš„æµ‹è¯•é›†ï¼Œæ ‡ç­¾çš„è®­ç»ƒé›†ï¼Œæ ‡ç­¾çš„æµ‹è¯•é›†ï¼›å…¶ä¸­ç‰¹å¾å’Œæ ‡ç­¾çš„å€¼æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚

# train_data,train_targetåˆ†åˆ«è¡¨ç¤ºä¸ºå¾…åˆ’åˆ†çš„ç‰¹å¾é›†å’Œå¾…åˆ’åˆ†çš„æ ‡ç­¾é›†ã€‚

# test_sizeï¼šæµ‹è¯•æ ·æœ¬æ‰€å æ¯”ä¾‹ã€‚

# random_stateï¼šéšæœºæ•°ç§å­,åœ¨éœ€è¦é‡å¤å®éªŒæ—¶ï¼Œä¿è¯åœ¨éšæœºæ•°ç§å­ä¸€æ ·æ—¶èƒ½å¾—åˆ°ä¸€ç»„ä¸€æ ·çš„éšæœºæ•°ã€‚

X_train, X_test, y_train, y_test = train_test_split(
    feature_data, label_data, test_size=0.3, random_state=2
)
```


#### sklearn.neural_network


**MLPClassifier**

å®ç°äº†å…·æœ‰åå‘ä¼ æ’­ç®—æ³•çš„å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„

- hidden_layer_sizes: å®šä¹‰éšå«å±‚åŠåŒ…å«çš„ç¥ç»å…ƒæ•°é‡ï¼Œ(20, 20) ä»£è¡¨ 2 ä¸ªéšå«å±‚å„æœ‰ 20 ä¸ªç¥ç»å…ƒã€‚
- activation: æ¿€æ´»å‡½æ•°ï¼Œæœ‰ identityï¼ˆçº¿æ€§ï¼‰, logistic, tanh, relu å¯é€‰ã€‚
- solver: æ±‚è§£æ–¹æ³•ï¼Œæœ‰ lbfgsï¼ˆæ‹Ÿç‰›é¡¿æ³•ï¼‰ï¼Œsgdï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰ï¼Œadamï¼ˆæ”¹è¿›å‹ sgdï¼‰ å¯é€‰ã€‚adam åœ¨ç›¸å¯¹è¾ƒå¤§çš„æ•°æ®é›†ä¸Šæ•ˆæœæ¯”è¾ƒå¥½ï¼ˆä¸Šåƒä¸ªæ ·æœ¬ï¼‰ï¼Œå¯¹å°æ•°æ®é›†è€Œè¨€ï¼Œlbfgs æ”¶æ•›æ›´å¿«æ•ˆæœä¹Ÿå¾ˆå¥½ã€‚ 
- alpha: æ­£åˆ™åŒ–é¡¹å‚æ•°ã€‚
- learning_rate: å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œconstantï¼ˆä¸å˜ï¼‰ï¼Œinvscalingï¼ˆé€æ­¥å‡å°ï¼‰ï¼Œadaptiveï¼ˆè‡ªé€‚åº”ï¼‰ å¯é€‰ã€‚
- learning_rate_init: åˆå§‹å­¦ä¹ ç‡ï¼Œç”¨äºéšæœºæ¢¯åº¦ä¸‹é™æ—¶æ›´æ–°æƒé‡ã€‚
- max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚
- shuffle: å†³å®šæ¯æ¬¡è¿­ä»£æ˜¯å¦é‡æ–°æ‰“ä¹±æ ·æœ¬ã€‚
- random_state: éšæœºæ•°ç§å­ã€‚
- tol: ä¼˜åŒ–æ±‚è§£çš„å®¹å¿åº¦ï¼Œå½“ä¸¤æ¬¡è¿­ä»£æŸå¤±å·®å€¼å°äºè¯¥å®¹å¿åº¦æ—¶ï¼Œæ¨¡å‹è®¤ä¸ºè¾¾åˆ°æ”¶æ•›å¹¶ä¸”è®­ç»ƒåœæ­¢

#### sklearn.datasets

- fetch_california_housingï¼šåŠ å·æˆ¿ä»·æ•°æ®
- make_moonsï¼šç”Ÿæˆæœˆç‰™çŠ¶æ•°æ®
- make_circlesï¼šç”Ÿæˆçº¿æ€§ä¸å¯åˆ†æ•°æ®

**load_digits**

imagesï¼š8x8 çŸ©é˜µï¼Œè®°å½•æ¯å¼ æ‰‹å†™å­—ç¬¦å›¾åƒå¯¹åº”çš„åƒç´ ç°åº¦å€¼

dataï¼šå°† images å¯¹åº”çš„ 8x8 çŸ©é˜µè½¬æ¢ä¸ºè¡Œå‘é‡

targetï¼šè®°å½• 1797 å¼ å½±åƒå„è‡ªä»£è¡¨çš„æ•°å­—

æ•°æ®é›†ï¼šåŒ…å«ç”± 1797 å¼ æ•°å­— 0 åˆ° 9 çš„æ‰‹å†™å­—ç¬¦å½±åƒè½¬æ¢åçš„æ•°å­—çŸ©é˜µï¼Œç›®æ ‡å€¼æ˜¯ 0-9

**make_blobs**

ç”Ÿæˆç‰¹å®šçš„å›¢çŠ¶æ•°æ®

- `n_samples`ï¼šè¡¨ç¤ºç”Ÿæˆæ•°æ®æ€»ä¸ªæ•°,é»˜è®¤ä¸º 100 ä¸ªã€‚
    
- `n_features`ï¼šè¡¨ç¤ºæ¯ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾ä¸ªæ•°ï¼Œé»˜è®¤ä¸º 2 ä¸ªã€‚
    
- `centers`ï¼šè¡¨ç¤ºä¸­å¿ƒç‚¹çš„ä¸ªæ•°ï¼Œé»˜è®¤ä¸º 3 ä¸ªã€‚
    
- `center_box`ï¼šè¡¨ç¤ºæ¯ä¸€ä¸ªä¸­å¿ƒçš„è¾¹ç•Œ,é»˜è®¤ä¸º -10.0åˆ°10.0ã€‚
    
- `random_state`ï¼šè¡¨ç¤ºç”Ÿæˆæ•°æ®çš„éšæœºæ•°ç§å­ã€‚


### scipy

#### scipy.linalg

**hilbert**

```py
from scipy.linalg import hilbert

x = hilbert(10)
```

#### scipy.optimize

**leastsq**

```py
from scipy.optimize import leastsq

func = lambda p, x: np.dot(x, p)  # å‡½æ•°å…¬å¼
err_func = lambda p, x, y: func(p, x) - y  # æ®‹å·®å‡½æ•°
p_init = np.random.randint(1, 2, 10)  # å…¨éƒ¨å‚æ•°åˆå§‹åŒ–ä¸º 1

parameters = leastsq(err_func, p_init, args=(x, y))  # æœ€å°äºŒä¹˜æ³•æ±‚è§£
```

#### sklearn.preprocessing

**scale**

è§„èŒƒåŒ–å¤„ç†

å°†ç‰¹å¾æ•°æ®çš„åˆ†å¸ƒè°ƒæ•´æˆæ ‡å‡†æ­£å¤ªåˆ†å¸ƒï¼Œä¹Ÿå«é«˜æ–¯åˆ†å¸ƒ
å³ä½¿å¾—æ•°æ®çš„å‡å€¼ç»´0ï¼Œæ–¹å·®ä¸º1

**PolynomialFeatures**

æ„é€ ç‰¹å¾çŸ©é˜µ

```py
from sklearn.preprocessing import PolynomialFeatures
X = [2, -1, 3]
X_reshape = np.array(X).reshape(len(X), 1)  # è½¬æ¢ä¸ºåˆ—å‘é‡
# ä½¿ç”¨ PolynomialFeatures è‡ªåŠ¨ç”Ÿæˆç‰¹å¾çŸ©é˜µ
PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_reshape)

x = np.array(x).reshape(len(x), 1)  # è½¬æ¢ä¸ºåˆ—å‘é‡
y = np.array(y).reshape(len(y), 1)

# ä½¿ç”¨ sklearn å¾—åˆ° 2 æ¬¡å¤šé¡¹å¼å›å½’ç‰¹å¾çŸ©é˜µ
poly_features = PolynomialFeatures(degree=2, include_bias=False)
poly_x = poly_features.fit_transform(x)
```

#### sklearn.pipeline

- make_pipelineï¼šå¤šæ¨¡å‹ç»„åˆ

#### sklearn.tree

- DecisionTreeClassifierï¼šå»ºç«‹ å†³ç­–æ ‘

#### sklearn.ensemble

- BaggingClassifierï¼šå»ºç«‹ Bagging Tree
- RandomForestClassifierï¼šå»ºç«‹éšæœºæ£®æ—
- AdaBoostClassifierï¼šå»ºç«‹ AdaBoost 
- GradientBoostingClassifierï¼šæ¢¯åº¦æå‡æ ‘ GBDT
- VotingClassifierï¼šæŠ•ç¥¨åˆ†ç±»å™¨ï¼Œç»„åˆå¤šä¸ªåˆ†ç±»å™¨è¿›è¡ŒæŠ•ç¥¨

#### sklearn.metrics

- accuracy_scoreï¼šåˆ¤æ–­è¾“å…¥ä¸¤ä¸ªæ•°æ®é—´çš„ç›¸åŒç‡ï¼Ÿ
  - åˆ¤æ–­æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®ç‡
- precision_scoreï¼šæŸ¥å‡†ç‡è®¡ç®—
- recall_scoreï¼šè®¡ç®—å¬å›ç‡
- f1_scoreï¼šf1è®¡ç®—
- roc_curveï¼šè®¡ç®—ROCæ›²çº¿
- aucï¼šè®¡ç®—auc

**r2_score**

Ræ–¹è®¡ç®—

```py
from sklearn.metrics import r2_score

# åˆ†åˆ«ä¼ å…¥çœŸå®è§‚æµ‹å€¼å’Œæ¨¡å‹é¢„æµ‹å€¼
r2_score(y1, model1.predict(x)), r2_score(y2, model2.predict(x))
```

#### sklearn.svm

- SVCï¼šæ”¯æŒå‘é‡æœºåˆ†ç±»å™¨

### joblib

ä¿å­˜æ¨¡å‹ï¼Œæ¨¡å‹å­˜ä¸ºÂ `.pkl`Â äºŒè¿›åˆ¶æ–‡ä»¶

### statsmodels

#### statsmodels.tsa.stattools

- arma_order_select_ic

#### statsmodels.stats.diagnostic

**acorr_ljungbox**

éšæœºåºåˆ—åˆ¤æ–­

è®¡ç®— LB ç»Ÿè®¡é‡ï¼Œé»˜è®¤ä¼šè¿”å› LB ç»Ÿè®¡é‡å’Œ LB ç»Ÿè®¡é‡çš„ P å€¼ã€‚å¦‚æœ LB ç»Ÿè®¡é‡çš„ P å€¼å°äºÂ `0.05`ï¼Œæˆ‘ä»¬åˆ™è®¤ä¸ºè¯¥åºåˆ—ä¸ºééšæœºåºåˆ—ï¼Œå¦åˆ™å°±ä¸ºéšæœºåºåˆ—

#### statsmodels.graphics.tsaplots

- plot_acfï¼šç»˜åˆ¶è‡ªç›¸å…³å›¾çš„å‡½æ•°
- OLSï¼šæ™®é€šæœ€å°äºŒä¹˜æ³•

#### statsmodels.formula.api

**smf**

```py
import statsmodels.formula.api as smf

model_smf_full = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data)
results_smf_full = model_smf_full.fit()

results_smf_full.summary2()  # è¾“å‡ºæ¨¡å‹æ‘˜è¦
```

### jieba

ç»“å·´åˆ†è¯æ¨¡å—

### re

æ­£åˆ™

### tqdm

é€šè¿‡å­çº¿ç¨‹å®ç°è¿›åº¦æ˜¾ç¤ºï¼Ÿ

### gensim

#### gensim.models

- Word2Vecï¼šæ–‡å­—è½¬å‘é‡

#### gensim.ipywidgets

**interact**

å…è®¸åœ¨å›¾è¡¨ä¸­ï¼Œå¢åŠ å¯äº¤äº’çš„å¯è°ƒèŠ‚å‚æ•°ï¼Œçœ‹ä¸åŒå‚æ•°ä¸‹æ•ˆæœ

```py
def change_c(c):
    linear_svc.C = c
    linear_svc.fit(x, y)
    plt.figure(figsize=(10, 8))
    plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap="bwr")
    svc_plot(linear_svc)


interact(change_c, c=[1, 10000, 1000000])
```

### torchvision

#### torchvision.transforms

- ToPILImageï¼šåŠ è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸º PIL IMAGE
- Resizeï¼šå°ºå¯¸å˜å½¢
- RandomCropï¼šéšæœºè£å‰ª
- CenterCropï¼šå±…ä¸­è£å‰ª
- ToTensorï¼šè½¬å¼ é‡
- Normalizeï¼šæ ‡å‡†åŒ–ï¼Ÿ
- Composeï¼šç»„åˆåŸºç¡€æ–¹æ³•
  - `composed = transforms.Compose([transforms.Resize(256), transforms.RandomCrop(224)])`
