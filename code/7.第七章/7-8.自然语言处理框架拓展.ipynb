{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1eb9c-b5db-4965-9f5f-e8676c8cb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0600ae-d3aa-4fc9-a74c-d5d190273688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91658\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # 下载英文分词所需拓展包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3369c47-0c9d-4633-8ba9-7997760cd6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'English', ']', 'is', 'a', 'West', 'Germanic', 'language', 'that', 'was', 'first', 'spoken', 'in', 'early', 'medieval', 'England', 'and', 'eventually', 'became', 'a', 'global', 'lingua', 'franca', '.', 'It', 'is', 'named', 'after', 'the', '<', 'Angles', '>', ',', 'one', 'of', 'the', 'Germanic', 'tribes', 'that', 'migrated', 'to', 'the', 'area', 'of', 'Great', 'Britain', 'that', 'later', 'took', 'their', 'name', ',', 'as', 'England', '.']\n"
     ]
    }
   ],
   "source": [
    "# 使用 nltk.tokenize.word_tokenize 完成英文文本分词\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"\"\"\n",
    "[English] is a West Germanic language that was first spoken in early\n",
    "medieval England and eventually became a global lingua franca.\n",
    "It is named after the <Angles>, one of the Germanic tribes that\n",
    "migrated to the area of Great Britain that later took their name,\n",
    "as England.\n",
    "\"\"\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e16b8c9-0570-4e15-926c-cb9a9ebca592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n[English] is a West Germanic language that was first spoken in early\\nmedieval England and eventually became a global lingua franca.',\n",
       " 'It is named after the <Angles>, one of the Germanic tribes that\\nmigrated to the area of Great Britain that later took their name,\\nas England.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本断句\n",
    "from nltk import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5dea930-3ccc-4932-bf50-fd4ee8b8ad1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'English',\n",
       " ']',\n",
       " 'is',\n",
       " 'a',\n",
       " 'West',\n",
       " 'Germanic',\n",
       " 'language',\n",
       " 'that',\n",
       " 'was',\n",
       " 'first',\n",
       " 'spoken',\n",
       " 'in',\n",
       " 'early',\n",
       " 'medieval',\n",
       " 'England',\n",
       " 'and',\n",
       " 'eventually',\n",
       " 'became',\n",
       " 'a',\n",
       " 'global',\n",
       " 'lingua',\n",
       " 'franca',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " '<',\n",
       " 'Angles',\n",
       " '>',\n",
       " ',',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Germanic',\n",
       " 'tribes',\n",
       " 'that',\n",
       " 'migrated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'area',\n",
       " 'of',\n",
       " 'Great',\n",
       " 'Britain',\n",
       " 'that',\n",
       " 'later',\n",
       " 'took',\n",
       " 'their',\n",
       " 'name',\n",
       " ',',\n",
       " 'as',\n",
       " 'England',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本过滤。例如去除文本中的标点符号，通过遍历分词结果，仅保留英文内容\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e944738c-f311-40f5-928d-b1f44abd5539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English', 'is', 'a', 'West', 'Germanic', 'language', 'that', 'was', 'first', 'spoken', 'in', 'early', 'medieval', 'England', 'and', 'eventually', 'became', 'a', 'global', 'lingua', 'franca', 'It', 'is', 'named', 'after', 'the', 'Angles', 'one', 'of', 'the', 'Germanic', 'tribes', 'that', 'migrated', 'to', 'the', 'area', 'of', 'Great', 'Britain', 'that', 'later', 'took', 'their', 'name', 'as', 'England']\n"
     ]
    }
   ],
   "source": [
    "# 仅保留 alphabetic 仅保留英文内容\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee917aa-85c2-4ab5-b24d-61cd9710a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91658\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 去除英文停用词\n",
    "# 停用词大拳：Dutch, German, Italian, Portuguese, Swedish, Arabic, English, Greek, Kazakh, Romanian, Turkish Azerbaijani, Finnish, Hungarian, Nepali, Russian, Danish, French, Indonesian, Norwegian, Spanish 等语言的停用词\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') # 安装停用词拓展包\n",
    "stop_words = stopwords.words(\"english\") # 加载英文停用词\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708d443c-be46-47e6-a9fe-d51f927d9bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English', 'West', 'Germanic', 'language', 'first', 'spoken', 'early', 'medieval', 'England', 'eventually', 'became', 'global', 'lingua', 'franca', 'It', 'named', 'Angles', 'one', 'Germanic', 'tribes', 'migrated', 'area', 'Great', 'Britain', 'later', 'took', 'name', 'England']\n"
     ]
    }
   ],
   "source": [
    "words_ = [w for w in words if not w in stop_words]\n",
    "print(words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240941c9-dc5b-47fd-8e5c-e8ea6098907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'that': 3, 'the': 3, 'is': 2, 'a': 2, 'Germanic': 2, 'England': 2, '.': 2, ',': 2, 'of': 2, '[': 1, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.FreqDist 按降序返回词频字典\n",
    "from nltk import FreqDist\n",
    "FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b2852f3-f30a-43ec-af3b-3e17adcb1666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'english', ']', 'is', 'a', 'west', 'german', 'languag', 'that', 'wa', 'first', 'spoken', 'in', 'earli', 'mediev', 'england', 'and', 'eventu', 'becam', 'a', 'global', 'lingua', 'franca', '.', 'it', 'is', 'name', 'after', 'the', '<', 'angl', '>', ',', 'one', 'of', 'the', 'german', 'tribe', 'that', 'migrat', 'to', 'the', 'area', 'of', 'great', 'britain', 'that', 'later', 'took', 'their', 'name', ',', 'as', 'england', '.']\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer 实现对句子中词干的提取。词干提取是语言形态学中的概念，词干提取的目的是去除词缀得到词根，例如 Germanic 的词干为 german\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597ef15-67dc-4690-a21c-946c5a8b715d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b467668-59ce-43e0-8557-2d6d0bbf1cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c9d621-c9c5-4361-a4d6-4c963de2df23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[55]: \" [English] is a West Germanic language that was first spoken in early medieval England and eventually became a global lingua franca. It is named after the <Angles>, one of the Germanic tribes that migrated to the area of Great Britain that later took their name, as England.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "text = \"\"\"\n",
    "[English] is a West Germanic language that was first spoken in early\n",
    "medieval England and eventually became a global lingua franca.\n",
    "It is named after the <Angles>, one of the Germanic tribes that\n",
    "migrated to the area of Great Britain that later took their name,\n",
    "as England.\n",
    "\"\"\"\n",
    "\n",
    "# Flair 会自动按照空格识别每一个 Sentence 包含的 Tokens 数量\n",
    "sentence = Sentence(text)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a019c5-941e-43fe-85e2-319f175ab8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"[\"\n",
      "Token[1]: \"English\"\n",
      "Token[2]: \"]\"\n",
      "Token[3]: \"is\"\n",
      "Token[4]: \"a\"\n",
      "Token[5]: \"West\"\n",
      "Token[6]: \"Germanic\"\n",
      "Token[7]: \"language\"\n",
      "Token[8]: \"that\"\n",
      "Token[9]: \"was\"\n",
      "Token[10]: \"first\"\n",
      "Token[11]: \"spoken\"\n",
      "Token[12]: \"in\"\n",
      "Token[13]: \"early\"\n",
      "Token[14]: \"medieval\"\n",
      "Token[15]: \"England\"\n",
      "Token[16]: \"and\"\n",
      "Token[17]: \"eventually\"\n",
      "Token[18]: \"became\"\n",
      "Token[19]: \"a\"\n",
      "Token[20]: \"global\"\n",
      "Token[21]: \"lingua\"\n",
      "Token[22]: \"franca\"\n",
      "Token[23]: \".\"\n",
      "Token[24]: \"It\"\n",
      "Token[25]: \"is\"\n",
      "Token[26]: \"named\"\n",
      "Token[27]: \"after\"\n",
      "Token[28]: \"the\"\n",
      "Token[29]: \"<\"\n",
      "Token[30]: \"Angles\"\n",
      "Token[31]: \">\"\n",
      "Token[32]: \",\"\n",
      "Token[33]: \"one\"\n",
      "Token[34]: \"of\"\n",
      "Token[35]: \"the\"\n",
      "Token[36]: \"Germanic\"\n",
      "Token[37]: \"tribes\"\n",
      "Token[38]: \"that\"\n",
      "Token[39]: \"migrated\"\n",
      "Token[40]: \"to\"\n",
      "Token[41]: \"the\"\n",
      "Token[42]: \"area\"\n",
      "Token[43]: \"of\"\n",
      "Token[44]: \"Great\"\n",
      "Token[45]: \"Britain\"\n",
      "Token[46]: \"that\"\n",
      "Token[47]: \"later\"\n",
      "Token[48]: \"took\"\n",
      "Token[49]: \"their\"\n",
      "Token[50]: \"name\"\n",
      "Token[51]: \",\"\n",
      "Token[52]: \"as\"\n",
      "Token[53]: \"England\"\n",
      "Token[54]: \".\"\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e51cfd-e6c4-40e4-8207-f9bfe36dd130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"[\"\n",
      "Token[1]: \"English\"\n",
      "Token[2]: \"]\"\n",
      "Token[3]: \"is\"\n",
      "Token[4]: \"a\"\n",
      "Token[5]: \"West\"\n",
      "Token[6]: \"Germanic\"\n",
      "Token[7]: \"language\"\n",
      "Token[8]: \"that\"\n",
      "Token[9]: \"was\"\n",
      "Token[10]: \"first\"\n",
      "Token[11]: \"spoken\"\n",
      "Token[12]: \"in\"\n",
      "Token[13]: \"early\"\n",
      "Token[14]: \"medieval\"\n",
      "Token[15]: \"England\"\n",
      "Token[16]: \"and\"\n",
      "Token[17]: \"eventually\"\n",
      "Token[18]: \"became\"\n",
      "Token[19]: \"a\"\n",
      "Token[20]: \"global\"\n",
      "Token[21]: \"lingua\"\n",
      "Token[22]: \"franca\"\n",
      "Token[23]: \".\"\n",
      "Token[24]: \"It\"\n",
      "Token[25]: \"is\"\n",
      "Token[26]: \"named\"\n",
      "Token[27]: \"after\"\n",
      "Token[28]: \"the\"\n",
      "Token[29]: \"<\"\n",
      "Token[30]: \"Angles\"\n",
      "Token[31]: \">\"\n",
      "Token[32]: \",\"\n",
      "Token[33]: \"one\"\n",
      "Token[34]: \"of\"\n",
      "Token[35]: \"the\"\n",
      "Token[36]: \"Germanic\"\n",
      "Token[37]: \"tribes\"\n",
      "Token[38]: \"that\"\n",
      "Token[39]: \"migrated\"\n",
      "Token[40]: \"to\"\n",
      "Token[41]: \"the\"\n",
      "Token[42]: \"area\"\n",
      "Token[43]: \"of\"\n",
      "Token[44]: \"Great\"\n",
      "Token[45]: \"Britain\"\n",
      "Token[46]: \"that\"\n",
      "Token[47]: \"later\"\n",
      "Token[48]: \"took\"\n",
      "Token[49]: \"their\"\n",
      "Token[50]: \"name\"\n",
      "Token[51]: \",\"\n",
      "Token[52]: \"as\"\n",
      "Token[53]: \"England\"\n",
      "Token[54]: \".\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 指定 use_tokenizer=True，就会自动调用 segtok 完成英文分词\n",
    "sentence = Sentence(text, use_tokenizer=True)\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01826b98-3c91-43af-be5e-3c1ad1de519c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence[6]: \"机器 学习 是 一个 好 工具\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对 Sentence 进行 Word Embedding 词嵌入\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "# 初始化 embedding\n",
    "embedding = WordEmbeddings('zh') # 自行实现时请替换为 `zh`\n",
    "# 创建 sentence，中文需传入分词后用空格间隔的语句，才能被 Flair 识别出 tokens\n",
    "sentence = Sentence(\"机器 学习 是 一个 好 工具\")\n",
    "# 词嵌入\n",
    "embedding.embed(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d74e7c50-d38b-4105-a77e-7132c6e75cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"机器\"\n",
      "tensor([ 0.0143,  0.5811, -0.6224,  0.2212,  0.7530, -0.1874, -0.6064,  0.0659,\n",
      "         0.2285,  0.0601,  0.8601, -0.0285,  1.1349, -0.5639, -0.1153, -0.0566,\n",
      "         0.7801, -0.0867, -0.6968, -0.5147, -0.3374,  1.1837,  0.7827,  0.0867,\n",
      "         0.4255,  0.1987,  0.8387, -0.0374,  0.3309, -0.0280,  0.8692, -0.9097,\n",
      "        -0.8766, -0.6566, -0.4730,  1.0071,  0.7562, -0.4000, -0.0652,  0.9994,\n",
      "         0.9919,  0.4734,  0.8127, -0.4761, -0.1291, -0.5706, -0.7824, -0.3793,\n",
      "         0.1278, -1.0881,  0.6386, -0.4776, -0.7002, -0.8154,  0.1790, -0.6806,\n",
      "        -1.2060, -1.0734, -2.0394,  0.4766,  0.9346,  0.0028,  0.5399,  0.8536,\n",
      "         0.1003,  0.5261, -0.6837, -0.5685, -0.5339,  0.1208,  0.8826, -0.4829,\n",
      "         1.1641, -0.2419, -0.7891, -0.1125, -0.1593, -0.8578, -0.6621, -1.1855,\n",
      "         0.0431,  0.0583,  0.7011, -0.7517, -0.7582, -0.9517, -0.0285,  0.3103,\n",
      "         0.1624, -0.9033, -0.7867,  0.4230, -0.2775, -0.0805, -0.3226,  0.7330,\n",
      "         0.3128,  0.1851, -0.1853, -0.0596,  0.4414,  0.2600, -0.7027,  0.8328,\n",
      "        -0.4970,  0.3798,  0.2092, -0.7503, -0.5770, -0.0128, -0.1826, -0.1387,\n",
      "         0.3124,  0.0187,  0.0387,  0.3218, -0.6264, -0.0517, -0.8444, -0.2013,\n",
      "        -0.5843,  0.4578,  0.3557,  0.3344, -0.3998, -0.3747,  0.8146,  0.5117,\n",
      "        -0.8563,  0.2704,  0.3490,  0.5117, -0.7002,  0.7740, -1.1578,  0.4763,\n",
      "        -0.1603, -0.2892, -0.6538, -0.2876,  0.0559,  0.3469,  0.6359, -1.0277,\n",
      "        -0.3468,  0.6848, -1.1921, -0.2028, -0.2787, -0.0375,  0.3030,  0.2835,\n",
      "        -0.4877,  0.5015,  0.6387, -0.7885,  0.5213, -0.1034, -0.4846,  0.7212,\n",
      "         0.1653,  0.1738,  0.3371,  0.2896, -0.5367, -0.0286, -0.3308,  0.4364,\n",
      "         0.8609,  0.2810,  0.4085,  0.3831,  1.1185, -0.0573,  0.1359, -0.0791,\n",
      "         0.4720,  0.7635, -0.0476, -0.3413, -0.4208, -0.4342,  0.0646,  0.7787,\n",
      "        -0.2778,  0.5125, -0.1750, -0.0067, -0.6191,  0.6051, -0.3996, -0.1800,\n",
      "        -0.3747, -0.5957, -0.0768, -0.3267,  0.1453, -0.8712, -0.0167, -0.2440,\n",
      "         0.0361, -0.1182, -0.0665, -0.2876,  0.3599,  0.2551,  0.3388, -0.4155,\n",
      "         0.2375,  0.2611, -0.9597,  0.6817, -0.2156,  0.1333, -0.4112,  0.4606,\n",
      "         0.2891, -0.1833,  0.6388,  0.4233, -0.3933,  0.9661,  0.4108, -0.4213,\n",
      "        -0.5075,  0.4503, -0.3346, -0.2201, -0.3898,  0.0812, -0.8379, -0.5047,\n",
      "         0.2715, -0.3409, -0.4785,  1.0817,  0.3356, -0.6770,  0.0201, -0.2554,\n",
      "         0.6776, -0.4254,  0.1542, -0.8496, -0.3390,  0.2657, -0.7995, -0.1938,\n",
      "        -0.5448,  0.7467, -0.6824, -0.0090, -1.0278,  0.4611, -0.2736, -0.2581,\n",
      "         0.4046,  0.3983, -1.0149, -0.2133,  0.2896, -0.4678, -0.6328, -0.1350,\n",
      "        -0.1115, -0.0133, -1.0436, -0.6583, -1.6365, -0.9621, -0.5890,  0.0709,\n",
      "         1.2525, -0.6565,  0.2980, -0.3386,  0.3507,  0.5609,  0.2868, -0.8079,\n",
      "         0.8194,  1.6471,  0.2568,  0.1511,  0.1190, -0.2075, -0.0378, -0.0687,\n",
      "        -1.0916,  0.0909, -1.0063, -0.8164, -0.9036, -0.6002,  0.2261, -0.6284,\n",
      "         0.1163,  0.2058,  1.0010, -0.0158], device='cuda:0')\n",
      "Token[1]: \"学习\"\n",
      "tensor([-3.2496e-01,  6.4774e-01, -3.8768e-01,  3.2548e-02,  7.6574e-01,\n",
      "        -5.1088e-01, -7.9309e-01,  3.1974e-01,  1.4386e-01, -2.3689e-01,\n",
      "         4.8327e-01, -6.3284e-02,  1.5617e+00, -6.6433e-01,  1.1605e-01,\n",
      "        -2.2726e-01,  7.7193e-01, -2.1452e-01, -5.9669e-01, -6.7123e-01,\n",
      "        -4.8646e-01,  1.0104e+00,  7.3959e-01, -6.9452e-02,  6.9977e-01,\n",
      "        -5.0222e-01,  8.8357e-01, -2.6706e-02,  5.9556e-01,  2.4998e-01,\n",
      "         8.0008e-01, -7.0294e-01, -6.3508e-01, -4.5956e-01, -7.2117e-01,\n",
      "         6.0594e-01,  5.8690e-01, -3.0229e-01,  1.0712e-02,  1.4117e+00,\n",
      "         9.3205e-01,  9.1340e-01,  7.4644e-01, -7.8001e-01, -3.1752e-01,\n",
      "        -4.2588e-01, -6.7553e-01, -4.7500e-01,  1.2541e-01, -5.2286e-01,\n",
      "         1.2321e+00, -4.2871e-01, -2.3411e-01, -7.4918e-01,  2.6468e-01,\n",
      "        -9.1652e-01, -1.0878e+00, -1.3424e+00, -2.3272e+00,  2.0264e-01,\n",
      "         7.5792e-01, -1.0008e-01,  6.2414e-01,  8.5643e-01,  2.8045e-01,\n",
      "         6.6526e-01, -6.2577e-01, -4.4871e-01, -2.1029e-01,  5.2435e-03,\n",
      "         9.8931e-01, -3.3600e-01,  1.0255e+00, -5.4760e-01, -4.6953e-01,\n",
      "        -1.2598e-01, -1.1644e-01, -6.8195e-01, -2.6941e-01, -1.0325e+00,\n",
      "        -4.2845e-01,  1.5560e-01,  8.3749e-01, -7.6646e-01, -6.7090e-01,\n",
      "        -9.9258e-01, -1.9242e-01,  6.7083e-01,  2.2316e-01, -6.9702e-01,\n",
      "        -5.0593e-01,  4.5782e-01, -1.6225e-01, -4.4178e-02, -4.5914e-01,\n",
      "         9.3188e-01,  2.8645e-01,  3.5577e-01,  9.9708e-02, -5.0715e-03,\n",
      "         3.0289e-01,  4.0837e-01, -3.1080e-01,  5.4059e-01, -5.3086e-01,\n",
      "         4.7741e-02,  9.2715e-02, -7.3871e-01, -6.6761e-01, -2.4710e-01,\n",
      "         1.1328e-02, -3.9275e-01, -8.4853e-03,  6.7699e-01,  4.8520e-01,\n",
      "         2.2267e-01, -5.9829e-01, -1.2634e-01, -7.5148e-01, -4.0789e-01,\n",
      "        -3.9861e-01,  4.6634e-01,  5.2215e-01,  9.5104e-02, -5.8386e-01,\n",
      "        -3.6987e-01,  5.0411e-01,  2.6521e-01, -7.4881e-01,  1.3841e-01,\n",
      "         5.5953e-01, -3.5650e-01, -5.7487e-01,  1.0268e+00, -1.0020e+00,\n",
      "         4.0540e-01,  6.9844e-03, -4.0649e-02, -7.5194e-01, -1.7583e-01,\n",
      "        -2.3509e-02,  6.2793e-01,  7.7491e-01, -9.5466e-01, -3.5790e-01,\n",
      "         3.3733e-01, -9.7010e-01, -2.7844e-01, -4.7630e-01, -8.6698e-03,\n",
      "         4.8556e-01,  5.4333e-01, -5.6352e-01,  4.5409e-01,  6.4429e-01,\n",
      "        -8.2720e-01,  1.9464e-01, -3.3808e-02, -3.2662e-01,  6.3361e-01,\n",
      "         5.6221e-01,  4.0578e-01, -5.3711e-03,  2.4223e-01, -7.3461e-02,\n",
      "         2.6014e-01, -1.2481e-01,  1.1112e+00,  3.2438e-01,  2.6632e-01,\n",
      "         4.4040e-01,  4.5628e-01,  1.1011e+00, -3.0905e-01,  2.0793e-01,\n",
      "        -5.1031e-01,  8.0338e-01,  7.4910e-01, -1.2676e-01, -1.9419e-01,\n",
      "        -5.3962e-01, -4.4887e-01,  5.0762e-02,  2.4368e-01, -1.9830e-01,\n",
      "         5.7638e-01, -2.5450e-01, -2.6344e-01, -7.1175e-01,  8.4950e-01,\n",
      "        -1.1203e-01, -5.1713e-02,  7.3786e-02, -4.6739e-01, -2.6118e-01,\n",
      "        -1.0008e+00, -2.1247e-01, -7.9742e-01, -2.0798e-01,  2.9983e-01,\n",
      "        -1.6070e-01, -1.1373e-01,  2.7128e-01, -9.5071e-01, -4.7413e-02,\n",
      "         9.7961e-02,  1.5148e-01, -5.8094e-01,  1.8383e-02,  1.2603e-01,\n",
      "        -7.1018e-01, -5.5209e-03, -3.5366e-01,  9.1064e-02, -8.4823e-01,\n",
      "         1.4066e-01,  3.3259e-01, -4.3188e-01,  6.9093e-01,  5.9725e-01,\n",
      "        -4.7339e-01,  1.4482e-02,  4.2865e-01, -1.2653e-01, -6.9798e-01,\n",
      "         5.1845e-01,  1.8231e-02, -5.2561e-01, -7.6256e-01,  4.4506e-02,\n",
      "        -7.8633e-01, -8.0979e-01,  1.5287e-01, -4.3581e-01, -4.7554e-01,\n",
      "         5.5389e-01,  4.3198e-01, -1.1809e+00, -3.1034e-02, -1.5329e-01,\n",
      "         6.3897e-01, -6.1526e-01,  6.6176e-01, -1.1912e-01,  6.6673e-02,\n",
      "         1.5720e-01, -9.1384e-01, -7.0507e-02, -2.9597e-02,  8.7810e-01,\n",
      "        -4.2138e-01,  6.7716e-02, -6.7661e-01,  6.9992e-01, -3.4975e-01,\n",
      "        -6.0683e-02,  4.2290e-01,  6.0106e-01, -8.1242e-01, -1.7345e-01,\n",
      "         8.1558e-01, -1.9420e-04, -2.6439e-01, -6.7547e-02, -4.9000e-01,\n",
      "        -1.0618e-01, -4.5975e-01, -3.5768e-01, -1.3467e+00, -7.7125e-01,\n",
      "        -3.1377e-01,  1.7904e-01,  7.0509e-01, -5.1039e-01, -1.6599e-01,\n",
      "         1.1125e-02,  1.6963e-01,  2.1902e-01, -1.9176e-02, -6.2030e-01,\n",
      "         5.8062e-01,  1.5223e+00,  3.6571e-02,  7.5735e-01,  5.1125e-01,\n",
      "        -7.2188e-02, -8.3230e-03, -6.8004e-02, -1.3448e+00, -6.3490e-02,\n",
      "        -8.8260e-01, -8.2197e-01, -7.2176e-01, -5.2066e-01, -3.0366e-01,\n",
      "        -2.6454e-01,  6.6962e-01,  7.0614e-01,  9.2428e-01,  1.4701e-01],\n",
      "       device='cuda:0')\n",
      "Token[2]: \"是\"\n",
      "tensor([-2.9470e-01,  9.6263e-01, -6.4208e-01,  2.6904e-01,  8.1669e-01,\n",
      "        -4.1970e-01, -7.6458e-01,  2.2026e-01,  3.9823e-01, -4.4368e-02,\n",
      "         9.0519e-01,  5.8803e-02,  1.5186e+00, -1.0610e+00,  3.7516e-02,\n",
      "        -8.5236e-01,  7.1213e-01, -4.0752e-01, -1.4830e+00, -6.0035e-01,\n",
      "        -1.1404e+00,  1.2292e+00,  1.2193e+00, -1.3205e-02,  7.5028e-01,\n",
      "        -6.4071e-01,  1.0560e+00,  2.9746e-01,  6.5827e-01,  2.9021e-01,\n",
      "         1.0365e+00, -8.8943e-01, -8.4054e-01, -9.1078e-01, -8.1180e-01,\n",
      "         1.2037e+00,  1.1838e+00, -6.8467e-01,  1.3043e-01,  1.4297e+00,\n",
      "         8.4812e-01,  1.2296e+00,  1.2645e+00, -1.3474e+00, -4.6647e-01,\n",
      "        -5.0325e-01, -7.8006e-01, -2.9556e-01,  4.6694e-01, -8.6471e-01,\n",
      "         1.1513e+00, -8.3127e-02, -5.6439e-01, -5.4749e-01,  2.4472e-01,\n",
      "        -1.1987e+00, -1.1071e+00, -1.4940e+00, -3.5805e+00,  3.5054e-01,\n",
      "         1.1822e+00, -1.4338e-01,  9.4363e-01,  1.4257e+00,  5.7719e-01,\n",
      "         1.1155e+00, -1.1568e+00, -7.1422e-01, -5.9231e-01, -1.5217e-01,\n",
      "         1.5964e+00, -8.5672e-01,  1.3706e+00, -7.5333e-01, -9.3702e-01,\n",
      "         3.4216e-01, -1.8062e-02, -9.4624e-01, -4.6339e-01, -1.2441e+00,\n",
      "        -2.4588e-01, -1.4186e-02,  9.0451e-01, -9.0349e-01, -8.4130e-01,\n",
      "        -8.8856e-01,  1.9713e-01,  7.0343e-01,  6.6668e-02, -1.2076e+00,\n",
      "        -9.1207e-01,  8.8654e-01, -1.5411e-01, -3.1390e-02, -8.8110e-01,\n",
      "         1.3894e+00,  8.1058e-01,  6.1890e-01,  1.3928e-01,  1.5015e-01,\n",
      "         6.2773e-01,  5.2557e-01, -6.0964e-01,  1.1134e+00, -6.4656e-01,\n",
      "         4.3897e-01, -1.0560e-01, -8.1528e-01, -7.9294e-01,  1.1758e-02,\n",
      "         2.2930e-01, -8.3421e-01, -1.2414e-01,  6.0844e-01,  2.7743e-01,\n",
      "         4.4911e-01, -6.5487e-01,  1.2584e-01, -6.7680e-01, -1.1650e+00,\n",
      "        -7.6231e-01,  1.0157e+00,  7.0824e-01,  7.2998e-02, -7.8838e-01,\n",
      "        -7.8291e-01,  8.3730e-01,  8.5906e-01, -1.1207e+00,  3.9082e-01,\n",
      "         7.1783e-01,  2.2138e-01, -9.7172e-01,  1.2464e+00, -1.2574e+00,\n",
      "         3.4886e-01, -4.2288e-01, -4.6324e-01, -9.6657e-01, -6.5494e-01,\n",
      "         3.6606e-02,  8.7073e-01,  1.4635e+00, -1.1964e+00, -9.0294e-01,\n",
      "         2.8447e-01, -1.4719e+00, -1.9801e-01, -7.5471e-01, -4.4003e-01,\n",
      "         5.0558e-01,  1.0695e+00, -2.0375e-01,  2.6232e-01,  1.0458e+00,\n",
      "        -7.8943e-01,  2.9110e-01, -4.2868e-01,  5.1662e-04,  7.1790e-01,\n",
      "         2.8826e-01,  3.6638e-01,  3.5106e-01, -2.6241e-01,  5.8454e-02,\n",
      "         2.4675e-01, -3.5126e-01,  8.2303e-01,  7.0827e-01,  5.1579e-01,\n",
      "         6.3620e-01,  6.3905e-01,  1.0002e+00, -2.7073e-01,  1.2566e-01,\n",
      "         7.3826e-02,  7.0752e-01,  8.8663e-01,  1.8877e-02, -6.4460e-01,\n",
      "        -7.2626e-01, -4.3245e-01,  2.9192e-01, -2.4789e-02, -5.5305e-01,\n",
      "         8.3379e-01,  1.0214e-02, -7.8934e-01, -4.7006e-01,  8.4025e-01,\n",
      "        -3.2540e-01, -5.3938e-02, -6.4136e-01, -4.4979e-01, -2.0078e-02,\n",
      "        -3.9118e-01, -1.1196e+00, -7.6324e-01, -7.3820e-03,  5.5478e-01,\n",
      "         4.8309e-02, -5.1268e-01, -1.5172e-01, -5.8643e-01,  1.2819e-01,\n",
      "         2.3156e-01,  1.3604e-01, -5.1435e-01, -2.1252e-01,  2.9178e-01,\n",
      "        -9.6110e-01,  2.3519e-01, -5.3226e-01,  4.5818e-01, -6.5265e-01,\n",
      "         2.3695e-02,  4.7478e-01, -1.1026e-01,  1.0374e+00,  6.8898e-01,\n",
      "        -6.5905e-01,  8.3337e-01,  3.8156e-01, -3.0748e-01, -1.1250e+00,\n",
      "         8.5038e-01, -2.1900e-01, -1.5465e-01, -1.0664e+00,  1.8646e-01,\n",
      "        -8.0464e-01, -2.3705e-01,  5.1165e-01, -8.5804e-01, -1.1827e-01,\n",
      "         1.0432e+00,  5.5366e-01, -1.2645e+00, -7.6407e-02, -2.3647e-01,\n",
      "         9.5911e-01, -1.0385e+00,  6.9627e-01, -3.1721e-02, -3.7161e-01,\n",
      "         8.4316e-03, -6.1770e-01, -3.5963e-01, -8.2659e-01, -6.2814e-02,\n",
      "        -4.9534e-01,  1.0803e-01, -8.3316e-01,  9.2093e-02, -4.6147e-01,\n",
      "        -6.0965e-02,  5.5664e-01,  6.5784e-01, -1.5682e+00, -3.8237e-02,\n",
      "         6.4389e-01, -3.5463e-01, -5.1613e-01, -3.1236e-01, -2.3338e-01,\n",
      "        -6.3333e-02, -8.1927e-01, -8.9823e-01, -2.0185e+00, -1.4186e+00,\n",
      "        -1.0503e-01,  4.2784e-01,  1.1059e+00, -7.7548e-01, -1.7601e-01,\n",
      "        -1.9003e-01,  1.5087e-01,  1.0027e+00, -1.0793e-01, -7.0910e-01,\n",
      "         6.4077e-01,  1.2628e+00,  1.6852e-01,  1.0921e+00,  3.0727e-02,\n",
      "        -5.8835e-01,  3.9650e-01,  4.5892e-01, -8.2997e-01,  8.4672e-02,\n",
      "        -7.3598e-01, -9.9018e-01, -8.3517e-01, -6.9301e-01,  1.2126e-01,\n",
      "        -9.2381e-01,  6.5063e-01,  3.7096e-01,  8.6223e-01,  4.4922e-01],\n",
      "       device='cuda:0')\n",
      "Token[3]: \"一个\"\n",
      "tensor([-0.0737,  0.5082, -0.6935, -0.0789,  0.3819, -0.2839, -0.5039,  0.0415,\n",
      "         0.1331, -0.3137,  0.6677, -0.1970,  0.9833, -0.6265,  0.0319, -0.4018,\n",
      "         0.5840,  0.3428, -1.0544, -0.6864, -0.5543,  0.7148,  0.5921, -0.1479,\n",
      "         0.4243, -0.4255,  0.5889,  0.0828,  0.6716,  0.2270,  0.8467, -0.3782,\n",
      "        -0.3355, -0.5189, -0.7975,  0.7559,  1.2961, -0.6540,  0.0932,  1.1062,\n",
      "         0.7401,  0.8103,  0.6879, -0.7346, -0.5576, -0.1967, -0.6064, -0.2394,\n",
      "        -0.0572, -0.7842,  0.6955, -0.1724, -0.6147, -0.8001,  0.4576, -0.6661,\n",
      "        -0.9749, -1.3075, -1.7487,  0.3943,  0.7408, -0.1462,  0.6588,  0.7658,\n",
      "         0.2726,  0.9435, -0.6261, -0.2634, -0.3476, -0.2783,  1.0468, -0.3697,\n",
      "         0.7806, -0.3242, -0.7213, -0.0714,  0.1008, -0.9476, -0.1250, -0.8386,\n",
      "        -0.2752,  0.0817,  0.3290, -0.5047, -0.5321, -0.8082,  0.1103,  0.6936,\n",
      "        -0.0974, -0.9505, -0.5493,  0.7136, -0.0415, -0.1809, -0.4456,  1.1020,\n",
      "         0.3636,  0.6953, -0.2080, -0.1619,  0.1121,  0.2361, -0.3169,  0.4728,\n",
      "        -0.6969,  0.0054,  0.2143, -0.7620, -0.6029, -0.1606, -0.1066, -0.2429,\n",
      "         0.0950,  0.3300,  0.0370,  0.0145, -0.3844,  0.0283, -0.5727, -0.5766,\n",
      "        -0.5473,  0.4859,  0.1930,  0.2058, -0.6213, -0.4933,  0.1494,  0.3724,\n",
      "        -0.8091,  0.1388,  0.5616,  0.1015, -0.7299,  0.8991, -0.9057,  0.1638,\n",
      "        -0.0946, -0.2198, -0.7337, -0.4277,  0.0348,  0.7408,  0.8802, -0.8790,\n",
      "        -0.5132,  0.3095, -1.1776,  0.1631, -0.4547, -0.0234,  0.3326,  0.4331,\n",
      "        -0.1048,  0.2179,  0.7793, -0.5711,  0.3321,  0.0602, -0.3515,  0.4355,\n",
      "         0.3082,  0.3437,  0.3638,  0.2393,  0.0977, -0.0503, -0.1972,  0.4905,\n",
      "         0.6600,  0.3427,  0.3623,  0.2477,  1.0571, -0.2453, -0.0974,  0.1054,\n",
      "         0.5306,  0.7143, -0.0602, -0.1745, -0.4580, -0.1708,  0.0271,  0.0232,\n",
      "        -0.1978,  0.1574, -0.0032, -0.3223, -0.4930,  0.7378, -0.1691, -0.2887,\n",
      "        -0.3597, -0.4060, -0.0719, -0.2593, -0.8465, -0.3307, -0.1629,  0.3394,\n",
      "         0.0405, -0.4944, -0.1151, -0.4372,  0.2526, -0.1608,  0.3314, -0.6572,\n",
      "        -0.0946,  0.2913, -0.6903,  0.2566, -0.0576,  0.0383, -0.4663,  0.1108,\n",
      "         0.1470,  0.1758,  0.6486,  0.2232, -0.3011,  0.3376,  0.3911,  0.1646,\n",
      "        -0.4874,  0.9991, -0.1202, -0.3505, -0.6318,  0.1634, -0.6370, -0.2258,\n",
      "         0.4027, -0.4303, -0.1014,  0.7300,  0.4224, -0.6519,  0.2385, -0.0709,\n",
      "         0.5122, -0.5937,  0.5370, -0.5672, -0.0740,  0.1552, -0.5209, -0.1279,\n",
      "        -0.4684,  0.3429, -0.4396,  0.0586, -0.7579,  0.2429, -0.3678,  0.1680,\n",
      "         0.5940,  0.2853, -1.1939, -0.1144,  0.0490, -0.2518, -0.2717, -0.4434,\n",
      "        -0.2550, -0.2255, -0.8746, -0.8969, -0.9644, -1.0754, -0.5249,  0.1892,\n",
      "         1.0731, -0.8210,  0.1116,  0.1603,  0.0713,  0.4434,  0.3396, -0.5897,\n",
      "         0.5703,  1.1991, -0.0383,  0.5015,  0.0743, -0.3986, -0.0771,  0.0730,\n",
      "        -0.3178,  0.5026, -0.7956, -0.7972, -0.4207, -0.4523,  0.2691, -0.0433,\n",
      "         0.7785,  0.2873,  0.7858,  0.2248], device='cuda:0')\n",
      "Token[4]: \"好\"\n",
      "tensor([-9.8929e-02,  1.2491e+00, -8.4758e-01,  5.3167e-01,  1.0574e+00,\n",
      "        -5.2330e-01, -6.3454e-01,  6.6102e-02,  2.8577e-01, -2.8122e-01,\n",
      "         1.0163e+00,  9.8894e-02,  1.4946e+00, -1.1385e+00,  3.3889e-01,\n",
      "        -7.4407e-01,  1.1756e+00, -4.6333e-01, -1.3732e+00, -9.0615e-01,\n",
      "        -1.1597e+00,  1.2152e+00,  1.4630e+00, -9.5854e-02,  9.3928e-01,\n",
      "        -2.7229e-01,  1.3321e+00, -4.2433e-02,  8.4669e-01,  3.3059e-01,\n",
      "         1.0210e+00, -1.1766e+00, -1.1104e+00, -7.9354e-01, -9.3404e-01,\n",
      "         1.0778e+00,  1.3256e+00, -8.2726e-01,  3.6350e-01,  1.4105e+00,\n",
      "         7.5099e-01,  1.2105e+00,  1.0801e+00, -1.4571e+00, -4.3122e-01,\n",
      "        -5.5185e-01, -8.6859e-01, -2.4268e-01,  6.3586e-01, -1.0281e+00,\n",
      "         1.2762e+00, -4.9458e-01, -4.5010e-01, -4.6716e-01,  2.1048e-01,\n",
      "        -1.1757e+00, -1.1276e+00, -1.5405e+00, -4.3190e+00,  4.7489e-01,\n",
      "         1.3342e+00, -1.8817e-01,  8.5426e-01,  1.2849e+00,  3.4079e-01,\n",
      "         7.8430e-01, -9.6167e-01, -5.7700e-01, -5.0483e-01, -4.4884e-01,\n",
      "         1.4872e+00, -6.9987e-01,  1.5548e+00, -8.7175e-01, -7.7090e-01,\n",
      "         7.0658e-01,  5.1195e-02, -1.0701e+00, -6.4907e-01, -1.3436e+00,\n",
      "        -3.7279e-01,  2.2611e-01,  1.1259e+00, -7.7517e-01, -8.9178e-01,\n",
      "        -1.2251e+00,  2.6167e-01,  6.9696e-01,  2.2853e-01, -1.0625e+00,\n",
      "        -6.8215e-01,  6.7655e-01, -2.3570e-01, -3.6551e-01, -1.1400e+00,\n",
      "         1.3774e+00,  7.5486e-01,  9.9982e-01,  7.5766e-02,  8.7429e-02,\n",
      "         6.8929e-01,  9.4955e-01, -5.4822e-01,  1.1555e+00, -6.5018e-01,\n",
      "         9.5104e-01, -3.0029e-02, -6.3037e-01, -1.1969e+00,  1.8153e-01,\n",
      "         5.4985e-01, -8.9267e-01, -1.9155e-01,  6.1675e-01,  2.0949e-01,\n",
      "         6.4118e-01, -7.3601e-01,  2.1812e-01, -9.4216e-01, -1.0408e+00,\n",
      "        -9.5387e-01,  8.3657e-01,  7.9016e-01, -2.0435e-01, -7.4658e-01,\n",
      "        -1.2543e+00,  1.0448e+00,  8.6996e-01, -1.0093e+00,  4.6790e-01,\n",
      "         9.8485e-01,  3.6014e-01, -8.9233e-01,  1.0195e+00, -1.4158e+00,\n",
      "         2.4657e-01, -5.8987e-01, -6.0092e-01, -1.0628e+00, -4.0283e-01,\n",
      "         2.9971e-02,  8.7130e-01,  1.2866e+00, -1.2866e+00, -9.3108e-01,\n",
      "         1.8117e-01, -1.4708e+00, -5.1765e-02, -7.2803e-01, -2.6360e-01,\n",
      "         8.2880e-01,  1.0917e+00, -3.2472e-01,  1.7605e-01,  1.1482e+00,\n",
      "        -9.1457e-01,  4.5168e-01, -4.1548e-01, -2.9865e-01,  8.7758e-01,\n",
      "         5.7283e-01,  4.0329e-01,  4.1626e-01, -4.9962e-03, -3.0370e-01,\n",
      "         2.3634e-01, -9.3532e-02,  1.2920e+00,  5.0149e-01,  4.6592e-01,\n",
      "         6.0240e-01,  7.2581e-01,  1.3470e+00, -1.8658e-01,  6.0634e-02,\n",
      "        -8.4577e-02,  7.9268e-01,  1.0364e+00, -1.8741e-01, -3.8072e-01,\n",
      "        -6.4939e-01, -6.9535e-01,  3.1566e-01,  7.5466e-02, -4.4083e-01,\n",
      "         8.4603e-01, -1.2895e-01, -7.4151e-01, -6.2922e-01,  9.0151e-01,\n",
      "        -4.1163e-01, -2.1922e-01, -6.0280e-01, -8.4718e-01, -4.3876e-02,\n",
      "        -8.6079e-01, -9.4760e-01, -8.7113e-01,  6.8971e-02,  5.3052e-01,\n",
      "        -2.1489e-01, -5.0863e-01,  3.2898e-02, -8.4323e-01, -1.0329e-01,\n",
      "         1.7324e-01,  4.5164e-01, -5.0305e-01,  2.7276e-02,  3.8356e-01,\n",
      "        -1.0494e+00,  2.5880e-01, -5.8057e-01,  5.2955e-01, -9.6895e-01,\n",
      "         4.0608e-01,  4.5989e-01, -3.9216e-01,  1.1082e+00,  7.3078e-01,\n",
      "        -4.8874e-01,  1.0780e+00,  8.8200e-02, -3.1825e-04, -1.0974e+00,\n",
      "         8.8000e-01, -3.7661e-01, -5.4520e-01, -1.0127e+00,  5.8841e-02,\n",
      "        -9.7452e-01, -8.1897e-01,  4.0137e-01, -6.5215e-01, -1.9860e-01,\n",
      "         1.2709e+00,  5.3455e-01, -1.2475e+00, -2.9769e-01, -5.5092e-01,\n",
      "         7.9281e-01, -6.9952e-01,  7.4329e-01, -6.3450e-02, -3.0502e-01,\n",
      "         8.0636e-02, -5.2637e-01, -3.3847e-01, -7.7989e-01,  2.5192e-01,\n",
      "        -2.0292e-01, -1.5657e-01, -1.1926e+00,  3.0141e-01, -5.3737e-01,\n",
      "        -1.6424e-01,  4.7523e-01,  5.2591e-01, -1.7125e+00,  7.9079e-03,\n",
      "         5.0052e-01, -3.5621e-01, -3.8273e-01,  2.6603e-02, -4.5296e-01,\n",
      "        -3.0302e-01, -8.4407e-01, -1.0795e+00, -2.1210e+00, -9.9769e-01,\n",
      "         6.3477e-03,  4.1831e-01,  1.0916e+00, -6.7977e-01, -2.6044e-01,\n",
      "        -4.0029e-01, -1.9433e-01,  8.1853e-01,  9.3558e-03, -7.8074e-01,\n",
      "         6.8278e-01,  1.7029e+00,  1.8010e-01,  1.0896e+00, -1.8944e-01,\n",
      "        -1.6505e-01,  6.6493e-01,  5.8992e-01, -1.1811e+00,  4.4594e-02,\n",
      "        -8.6748e-01, -1.3192e+00, -6.1864e-01, -6.5932e-01, -9.3218e-02,\n",
      "        -1.0111e+00,  7.4332e-02,  9.3045e-01,  1.1787e+00, -4.2524e-02],\n",
      "       device='cuda:0')\n",
      "Token[5]: \"工具\"\n",
      "tensor([ 7.7626e-02,  3.4500e-01, -8.4805e-01,  6.4822e-01,  8.4264e-01,\n",
      "        -1.5225e-01, -5.6491e-01,  7.3676e-02,  2.8573e-01, -3.7650e-01,\n",
      "         7.1349e-01,  7.4695e-02,  1.0345e+00, -8.3189e-01,  2.7686e-01,\n",
      "        -3.8717e-01,  5.5416e-01, -2.2696e-01, -7.5008e-01, -3.8934e-01,\n",
      "        -7.8667e-01,  7.1259e-01,  8.0246e-01,  1.9120e-01,  6.8034e-01,\n",
      "        -2.1005e-01,  7.8897e-01,  2.9680e-02,  4.6992e-01,  1.0709e-01,\n",
      "         6.7642e-01, -5.0523e-01, -8.2933e-01, -1.9451e-01, -4.4002e-01,\n",
      "         7.7130e-01,  8.2206e-01, -4.6257e-01,  2.5625e-01,  1.1233e+00,\n",
      "         6.1146e-01,  6.2149e-01,  6.3909e-01, -1.0478e+00, -3.1095e-01,\n",
      "        -3.7674e-01, -3.0834e-01, -1.0151e-01,  2.0864e-01, -6.2303e-01,\n",
      "         9.2810e-01, -5.5475e-01, -2.2680e-01, -4.8252e-01,  2.0128e-01,\n",
      "        -7.6970e-01, -8.0237e-01, -9.6166e-01, -2.5902e+00,  3.4217e-01,\n",
      "         7.6201e-01,  2.2369e-01,  7.2643e-01,  8.1148e-01,  4.1677e-01,\n",
      "         7.6627e-01, -7.6495e-01, -6.5571e-01, -5.0893e-01,  7.2132e-04,\n",
      "         8.7514e-01, -3.1716e-01,  7.0562e-01, -5.0139e-01, -5.0363e-01,\n",
      "         9.8056e-02,  9.5462e-02, -3.7544e-01, -5.1002e-01, -1.0472e+00,\n",
      "        -1.8728e-01,  1.9773e-02,  9.3435e-01, -2.1193e-01, -7.1982e-01,\n",
      "        -6.3043e-01,  3.9934e-01,  3.1178e-01,  2.8568e-01, -2.9688e-01,\n",
      "        -4.2226e-01,  4.1676e-01, -3.2123e-01, -6.7405e-02, -6.3319e-01,\n",
      "         1.0140e+00,  6.4152e-01,  3.5480e-01, -1.5926e-01,  2.8371e-01,\n",
      "         4.9172e-01,  3.4924e-01, -3.7351e-01,  7.2828e-01, -6.3493e-01,\n",
      "         6.4960e-01,  4.2411e-02, -6.6279e-01, -6.1671e-01,  1.1741e-01,\n",
      "        -3.3209e-02, -3.5541e-01, -2.1580e-01,  1.2344e-01,  6.5102e-02,\n",
      "         2.8030e-01, -3.7041e-01,  8.7519e-02, -2.9778e-01, -7.4646e-01,\n",
      "        -4.6600e-01,  7.0127e-01,  7.1335e-01, -1.6944e-01, -6.1457e-01,\n",
      "        -3.8265e-01,  6.4486e-01,  5.8761e-01, -7.5519e-01,  5.6657e-02,\n",
      "         8.4512e-01,  3.8494e-01, -5.9174e-01,  6.1456e-01, -9.0165e-01,\n",
      "         4.1715e-01, -2.1881e-01, -7.0829e-03, -5.1822e-01, -3.5971e-01,\n",
      "         1.0889e-01,  5.2257e-01,  1.1090e+00, -1.1411e+00, -3.4349e-01,\n",
      "         2.2106e-01, -1.3681e+00, -9.8239e-02, -9.5574e-01, -1.0046e-01,\n",
      "         3.8280e-01,  4.3341e-01, -5.4691e-01,  7.9324e-02,  4.5687e-01,\n",
      "        -6.1989e-01,  2.8172e-01, -1.7621e-01, -1.9494e-01,  4.9175e-01,\n",
      "         1.8155e-01,  2.0121e-01,  3.5332e-01, -1.0972e-01, -2.4886e-01,\n",
      "        -6.8137e-03,  1.7329e-02,  6.3899e-01,  5.2208e-01,  1.5924e-01,\n",
      "         4.9881e-01,  3.2426e-02,  1.0455e+00, -1.0736e-01, -9.9695e-02,\n",
      "         1.1880e-02,  1.6766e-01,  4.9924e-01, -4.2403e-01, -5.1307e-01,\n",
      "        -5.0723e-01, -7.0273e-01,  1.2938e-01,  2.0587e-01, -2.4804e-01,\n",
      "         4.3666e-01, -3.2109e-01, -4.9457e-01, -7.0252e-01,  4.8740e-01,\n",
      "        -2.5155e-01, -1.2607e-01, -5.1706e-01, -1.3906e-01, -3.1923e-01,\n",
      "        -5.9507e-01, -5.9941e-01, -5.9291e-01,  8.3765e-02,  2.2275e-01,\n",
      "         5.0059e-02, -2.0300e-01, -1.8935e-01, -7.1984e-01,  3.8764e-01,\n",
      "         1.5202e-01,  2.4962e-01,  5.9814e-02,  5.6661e-02,  4.8815e-01,\n",
      "        -1.1389e+00,  3.6501e-01, -3.7702e-01,  3.8402e-01, -6.4953e-01,\n",
      "         2.4076e-01,  2.2848e-01, -1.0029e-01,  3.6977e-01,  6.5855e-01,\n",
      "        -5.9567e-01,  5.0346e-01,  4.5110e-01, -2.2932e-01, -7.9340e-01,\n",
      "         7.2282e-01, -1.1029e-01, -2.9329e-01, -8.3050e-01, -1.7477e-01,\n",
      "        -7.8086e-01, -3.5417e-01,  3.3621e-01, -6.3124e-01, -4.5237e-01,\n",
      "         1.2538e+00,  5.0721e-01, -4.7086e-01, -3.9326e-01, -3.0356e-01,\n",
      "        -2.5768e-02, -8.8868e-01,  2.1007e-01, -2.5571e-01, -4.9784e-01,\n",
      "         1.9029e-01, -7.6648e-01, -5.4100e-01, -3.0252e-01,  5.7354e-01,\n",
      "        -4.8085e-01, -3.4596e-03, -9.9778e-01,  6.6623e-02, -4.7766e-01,\n",
      "        -1.2646e-01,  1.8771e-01,  1.5432e-01, -1.0605e+00, -1.3565e-01,\n",
      "         5.2770e-01, -7.4257e-01, -1.7400e-01,  2.8087e-01, -4.0230e-01,\n",
      "         3.1949e-02, -5.6044e-01, -6.3482e-01, -1.6851e+00, -1.0976e+00,\n",
      "        -5.4208e-01,  3.5632e-01,  8.1533e-01, -3.7334e-01,  6.0636e-02,\n",
      "        -3.8516e-01, -3.2269e-02,  4.6939e-01,  1.1107e-01, -6.4306e-01,\n",
      "         4.4179e-01,  1.1898e+00,  6.1829e-02, -1.0223e-01,  4.0794e-02,\n",
      "         2.7965e-03, -1.9893e-01,  3.7425e-01, -6.7358e-01,  1.4465e-02,\n",
      "        -7.2238e-01, -1.0501e+00, -6.1100e-01, -7.7291e-01,  3.6434e-01,\n",
      "        -6.0364e-01,  2.5729e-01,  1.8586e-01,  1.1608e+00,  4.6043e-02],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding) # 输出词嵌入后向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b2a9504-eba0-41b9-a29b-98131a198735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"人工智能\"\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "# 某个词不包含在 FastText 提供的预训练模型中，那么词向量将全部为 0\n",
    "sentence = Sentence(\"人工智能\")\n",
    "embedding.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "004cde33-f699-48a8-a3ab-910f77f1a358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c888698e93463887f7c2425b17c7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a8209d28e94e5d9877a7b7b572a56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cfd90006ba4eaeabf5fa680950c392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ad20dd250c43f4a048677e1a8005e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b45e5cfebf4338b5bc6e48d362568f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FastText 提供的中文词嵌入向量长度为 300\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "\n",
    "# 词嵌入向量长度为 3072\n",
    "embedding = TransformerWordEmbeddings('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ae37205-4899-4c80-a5af-ebd2219f60ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disagreed</td>\n",
       "      <td>千叶湖八岁孩子不想去学英语，跳楼了「辟谣」千叶湖八岁孩子跳楼了为谣言信息</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agreed</td>\n",
       "      <td>喝酸奶真的能补充益生菌吗？喝酸奶来补充益生菌，靠谱么？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agreed</td>\n",
       "      <td>刚刚马云终于出手了！房价要跌，扬言房地产中介都要失业了最新消息马云终于出手了！扬言房地产中介...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unrelated</td>\n",
       "      <td>直击“冯乡长”李正春追悼会：赵本山全程操办，赵四刘能现场祭奠昆明会议直击“活摘”谣言</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disagreed</td>\n",
       "      <td>李雨桐爆薛之谦离婚内幕，说到底就是网红之间的恩怨情仇嘛薛之谦前女友李雨桐再次发微博爆料，薛之...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0  disagreed               千叶湖八岁孩子不想去学英语，跳楼了「辟谣」千叶湖八岁孩子跳楼了为谣言信息\n",
       "1     agreed                        喝酸奶真的能补充益生菌吗？喝酸奶来补充益生菌，靠谱么？\n",
       "2     agreed  刚刚马云终于出手了！房价要跌，扬言房地产中介都要失业了最新消息马云终于出手了！扬言房地产中介...\n",
       "3  unrelated         直击“冯乡长”李正春追悼会：赵本山全程操办，赵四刘能现场祭奠昆明会议直击“活摘”谣言\n",
       "4  disagreed  李雨桐爆薛之谦离婚内幕，说到底就是网红之间的恩怨情仇嘛薛之谦前女友李雨桐再次发微博爆料，薛之..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/wsdm_mini.csv\")\n",
    "df[\"text\"] = df[[\"title1_zh\", \"title2_zh\"]].apply(\n",
    "    lambda x: \"\".join(x), axis=1\n",
    ")  # 合并文本数据列\n",
    "data = df.drop(df.columns[[0, 1]], axis=1)  # 删除原文本列\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "213f8107-a922-4be5-807d-535454d4cff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__disagreed</td>\n",
       "      <td>千叶湖八岁孩子不想去学英语，跳楼了「辟谣」千叶湖八岁孩子跳楼了为谣言信息</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__agreed</td>\n",
       "      <td>喝酸奶真的能补充益生菌吗？喝酸奶来补充益生菌，靠谱么？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__agreed</td>\n",
       "      <td>刚刚马云终于出手了！房价要跌，扬言房地产中介都要失业了最新消息马云终于出手了！扬言房地产中介...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__unrelated</td>\n",
       "      <td>直击“冯乡长”李正春追悼会：赵本山全程操办，赵四刘能现场祭奠昆明会议直击“活摘”谣言</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__disagreed</td>\n",
       "      <td>李雨桐爆薛之谦离婚内幕，说到底就是网红之间的恩怨情仇嘛薛之谦前女友李雨桐再次发微博爆料，薛之...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                label                                               text\n",
       "0  __label__disagreed               千叶湖八岁孩子不想去学英语，跳楼了「辟谣」千叶湖八岁孩子跳楼了为谣言信息\n",
       "1     __label__agreed                        喝酸奶真的能补充益生菌吗？喝酸奶来补充益生菌，靠谱么？\n",
       "2     __label__agreed  刚刚马云终于出手了！房价要跌，扬言房地产中介都要失业了最新消息马云终于出手了！扬言房地产中介...\n",
       "3  __label__unrelated         直击“冯乡长”李正春追悼会：赵本山全程操办，赵四刘能现场祭奠昆明会议直击“活摘”谣言\n",
       "4  __label__disagreed  李雨桐爆薛之谦离婚内幕，说到底就是网红之间的恩怨情仇嘛薛之谦前女友李雨桐再次发微博爆料，薛之..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flair 提供了用于文本分类非常高阶的 API,标签列需要全部添加 __label__ 标记\n",
    "data[\"label\"] = \"__label__\" + data[\"label\"].astype(str)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d038b5c-6d24-480c-9cec-36d4d76924e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4686b7b6c14790bab81381a6f0b689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\91658\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.431 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__disagreed</td>\n",
       "      <td>千叶 湖 八岁 孩子 不想 去学 英语 跳楼 「 辟谣 千叶 湖 八岁 孩子 跳楼 谣言 信息</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__agreed</td>\n",
       "      <td>喝 酸奶 真的 补充 益生菌 喝 酸奶 补充 益生菌 谱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__agreed</td>\n",
       "      <td>刚刚 马云 终于 出手 房价 跌 扬言 房地产 中介 失业 最新消息 马云 终于 出手 扬言...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__unrelated</td>\n",
       "      <td>直击 冯 乡长 李正春 追悼会 赵本山 全程 操办 赵四 刘能 现场 祭奠 昆明 会议 直击...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__disagreed</td>\n",
       "      <td>李雨桐 爆 薛之谦 离婚 内幕 说到底 网红 之间 恩怨 情仇 薛之谦 前女友 李雨桐 发微...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                label                                               text\n",
       "0  __label__disagreed    千叶 湖 八岁 孩子 不想 去学 英语 跳楼 「 辟谣 千叶 湖 八岁 孩子 跳楼 谣言 信息\n",
       "1     __label__agreed                       喝 酸奶 真的 补充 益生菌 喝 酸奶 补充 益生菌 谱\n",
       "2     __label__agreed  刚刚 马云 终于 出手 房价 跌 扬言 房地产 中介 失业 最新消息 马云 终于 出手 扬言...\n",
       "3  __label__unrelated  直击 冯 乡长 李正春 追悼会 赵本山 全程 操办 赵四 刘能 现场 祭奠 昆明 会议 直击...\n",
       "4  __label__disagreed  李雨桐 爆 薛之谦 离婚 内幕 说到底 网红 之间 恩怨 情仇 薛之谦 前女友 李雨桐 发微..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, \"r\", encoding='UTF-8') as f:\n",
    "        stopwords = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "stopwords = load_stopwords(\"../../data/stopwords.txt\")\n",
    "\n",
    "corpus = []\n",
    "for line in tqdm(data[\"text\"]):\n",
    "    words = []\n",
    "    seg_list = list(jieba.cut(line))  # 分词\n",
    "    for word in seg_list:\n",
    "        if word in stopwords:  # 删除停用词\n",
    "            continue\n",
    "        words.append(word)\n",
    "    corpus.append(\" \".join(words))\n",
    "\n",
    "data[\"text\"] = corpus  # 将结果赋值到 DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81d4a3e1-065f-4c21-9c98-dc23fa3c00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分 训练集和测试集\n",
    "# 设置 index=False, header=False 去除索引列和数据列名。同时，使用 \\t 用以间隔标签和特征\n",
    "data.iloc[0 : int(len(data) * 0.8)].to_csv(\n",
    "    \"train.csv\", sep=\"\\t\", index=False, header=False\n",
    ")\n",
    "data.iloc[int(len(data) * 0.8) :].to_csv(\n",
    "    \"test.csv\", sep=\"\\t\", index=False, header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de591769-4d73-455d-bd1e-3cb21d40b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 01:30:14,977 Reading data from ..\\..\\data\\bert\n",
      "2024-08-19 01:30:14,978 Train: ..\\..\\data\\bert\\train.csv\n",
      "2024-08-19 01:30:14,978 Dev: None\n",
      "2024-08-19 01:30:14,979 Test: ..\\..\\data\\bert\\test.csv\n",
      "2024-08-19 01:30:15,528 No dev split found. Using 0% (i.e. 1200 samples) of the train split as dev data\n",
      "2024-08-19 01:30:15,528 Initialized corpus ..\\..\\data\\bert (label type name is 'class')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flair.datasets.document_classification.ClassificationCorpus at 0x1ff31071310>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.datasets import ClassificationCorpus\n",
    "from pathlib import Path\n",
    "\n",
    "# 加载处理好的语料数据\n",
    "corpus = ClassificationCorpus(Path(\"../../data/bert/\"), test_file=\"test.csv\", train_file=\"train.csv\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7145e92-3066-47a0-b103-ef7c16723668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[17]: \"千叶 湖 八岁 孩子 不想 去学 英语 跳楼 「 辟谣 千叶 湖 八岁 孩子 跳楼 谣言 信息\" → disagreed (1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[0]  # 加载第一条训练语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab617b25-e805-4e36-a5af-5deb59833d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91658\\Miniconda3\\envs\\ml\\Lib\\site-packages\\flair\\models\\language_model.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(str(model_file), map_location=flair.device)\n"
     ]
    }
   ],
   "source": [
    "#  Document Embeddings\n",
    "# 类似于 将一段文本中每一个词嵌入向量求和作为整段文本的特征，Flair 通过了更为丰富的文档嵌入方法\n",
    "# Flair 提供了 Pooling 的文档嵌入方法。实际上就是将词嵌入后的向量取平均、最大或最小值作为整段文本的嵌入向量\n",
    "\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings\n",
    "\n",
    "# initialize the word embeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# initialize the document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
    "                                              flair_embedding_backward,\n",
    "                                              flair_embedding_forward])\n",
    "document_embeddings.embed(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8f5d55f-47d2-490e-9f4f-f8d5f082e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentRNNEmbeddings RNN 文档嵌入方法得到文本锻炼的嵌入向量。DocumentRNNEmbeddings 实际上就是构建一个简单的 RNN 网络，输入为词嵌入向量，输出则视为文档嵌入\n",
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "word_embeddings = [WordEmbeddings(\"zh\")]  # 词嵌入\n",
    "document_embeddings = DocumentRNNEmbeddings(\n",
    "    word_embeddings,\n",
    "    hidden_size=512,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=256,\n",
    ")  # 文档嵌入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28207820-de1d-460e-af98-d5c0e8c92562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 01:38:43,434 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "10800it [00:03, 2947.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 01:38:47,110 Dictionary created for label 'class' with 3 values: disagreed (seen 3620 times), unrelated (seen 3605 times), agreed (seen 3575 times)\n",
      "2024-08-19 01:38:47,119 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,121 Model: \"TextClassifier(\n",
      "  (embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings(\n",
      "        'zh'\n",
      "        (embedding): Embedding(332647, 300)\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=300, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-08-19 01:38:47,149 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,151 Corpus: 10800 train + 1200 dev + 3000 test sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\91658\\Miniconda3\\envs\\ml\\Lib\\site-packages\\flair\\trainers\\trainer.py:499: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 01:38:47,152 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,153 Train:  10800 sentences\n",
      "2024-08-19 01:38:47,154         (train_with_dev=False, train_with_test=False)\n",
      "2024-08-19 01:38:47,155 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,156 Training Params:\n",
      "2024-08-19 01:38:47,156  - learning_rate: \"0.1\" \n",
      "2024-08-19 01:38:47,158  - mini_batch_size: \"32\"\n",
      "2024-08-19 01:38:47,159  - max_epochs: \"1\"\n",
      "2024-08-19 01:38:47,160  - shuffle: \"True\"\n",
      "2024-08-19 01:38:47,160 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,161 Plugins:\n",
      "2024-08-19 01:38:47,162  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
      "2024-08-19 01:38:47,164 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,166 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-08-19 01:38:47,166  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-08-19 01:38:47,167 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,168 Computation:\n",
      "2024-08-19 01:38:47,169  - compute on device: cuda:0\n",
      "2024-08-19 01:38:47,170  - embedding storage: cpu\n",
      "2024-08-19 01:38:47,171 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,172 Model training base path: \"..\\..\\data\\bert\"\n",
      "2024-08-19 01:38:47,172 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:47,173 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:38:49,756 epoch 1 - iter 33/338 - loss 1.18455763 - time (sec): 2.58 - samples/sec: 409.33 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:38:52,275 epoch 1 - iter 66/338 - loss 1.15127265 - time (sec): 5.10 - samples/sec: 414.23 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:38:54,894 epoch 1 - iter 99/338 - loss 1.12981383 - time (sec): 7.72 - samples/sec: 410.47 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:38:57,425 epoch 1 - iter 132/338 - loss 1.11711542 - time (sec): 10.25 - samples/sec: 412.16 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:38:59,736 epoch 1 - iter 165/338 - loss 1.11101582 - time (sec): 12.56 - samples/sec: 420.39 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:39:03,347 epoch 1 - iter 198/338 - loss 1.10263623 - time (sec): 16.17 - samples/sec: 391.83 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:39:05,807 epoch 1 - iter 231/338 - loss 1.09998552 - time (sec): 18.63 - samples/sec: 396.76 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:39:08,197 epoch 1 - iter 264/338 - loss 1.09256635 - time (sec): 21.02 - samples/sec: 401.91 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:39:10,622 epoch 1 - iter 297/338 - loss 1.08618486 - time (sec): 23.45 - samples/sec: 405.36 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:39:12,758 epoch 1 - iter 330/338 - loss 1.08473369 - time (sec): 25.58 - samples/sec: 412.80 - lr: 0.100000 - momentum: 0.000000\n",
      "2024-08-19 01:39:13,267 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:39:13,269 EPOCH 1 done: loss 1.0844 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:01<00:00,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 01:39:15,258 DEV : loss 1.1387922763824463 - f1-score (micro avg)  0.4833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 01:39:15,589  - 0 epochs without improvement\n",
      "2024-08-19 01:39:15,590 saving best model\n",
      "2024-08-19 01:39:17,795 ----------------------------------------------------------------------------------------------------\n",
      "2024-08-19 01:39:17,796 Loading model from best epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 47/47 [00:03<00:00, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 01:39:23,497 \n",
      "Results:\n",
      "- F-score (micro) 0.4777\n",
      "- F-score (macro) 0.3804\n",
      "- Accuracy 0.4777\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      agreed     0.4262    0.9912    0.5961      1023\n",
      "   disagreed     0.7047    0.4172    0.5241       978\n",
      "   unrelated     0.2619    0.0110    0.0211       999\n",
      "\n",
      "    accuracy                         0.4777      3000\n",
      "   macro avg     0.4643    0.4731    0.3804      3000\n",
      "weighted avg     0.4623    0.4777    0.3812      3000\n",
      "\n",
      "2024-08-19 01:39:23,498 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.4776666666666667}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 初始化分类器\n",
    "classifier = TextClassifier(\n",
    "    document_embeddings,\n",
    "    label_dictionary=corpus.make_label_dictionary(label_type='class'),\n",
    "    multi_label=False,\n",
    "    label_type='class'\n",
    ")\n",
    "# 训练分类器\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train(\"../../data/bert/\", max_epochs=1)  # 分类器模型及日志输出在当前目录\n",
    "\n",
    "# 训练结束之后，Flair 会在当前目录下方保存最终模型 final-model.pt 和最佳模型 best-model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9489a791-ca25-4277-8dfa-22c2f92e9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence[16]: \"千叶 湖 八岁 孩子 不想 去学 英语 跳楼 辟谣 千叶 湖 八岁 孩子 跳楼 谣言 信息\"'/'disagreed' (0.572)]\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load(\"../../data/bert/best-model.pt\")  # 加载最佳模型\n",
    "sentence = Sentence(\"千叶 湖 八岁 孩子 不想 去学 英语 跳楼 辟谣 千叶 湖 八岁 孩子 跳楼 谣言 信息\")\n",
    "classifier.predict(sentence)  # 模型推理\n",
    "print(sentence.labels)  # 输出推理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e780da-5b2e-4001-a7b8-781ce842bcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
