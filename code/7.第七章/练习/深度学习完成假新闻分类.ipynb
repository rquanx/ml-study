{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17625754-f8d2-4129-b1a1-4b948226050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../../data/wsdm_mini.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b4821-4d83-45d4-9bf4-a248b5c2fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并文本，然后对文本进行分词处理 \n",
    "\n",
    "df['title_zh'] = df[['title1_zh', 'title2_zh']].apply(lambda x: ''.join(x), axis=1)  # 合并文本数据列\n",
    "df_merge = df.drop(df.columns[[0, 1]], axis=1)  # 删除原文本列\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc463d49-b65f-46db-8286-2eb1968c0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        stopwords = [line.strip('\\n') for line in f.readlines()]\n",
    "    return stopwords\n",
    "stopwords = load_stopwords('../../../data/stopwords.txt')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e6458-bb55-41f8-9421-252ceac10a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 利用 jieba 进行分词，并且取出停用词\n",
    "\n",
    "corpus = []\n",
    "for line in tqdm(df['title_zh']):\n",
    "    words = []\n",
    "    seg_list = list(jieba.cut(line))  # 分词\n",
    "    for word in seg_list:\n",
    "        if word in stopwords:  # 删除停用词\n",
    "            continue\n",
    "        words.append(word)\n",
    "    corpus.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb5315-8e6b-4cb7-942e-d078d00e0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(corpus)  # 词嵌入，默认 size=100\n",
    "\n",
    "# 整个句子所有单词向量的和，得到空间向量上这个句子的向量\n",
    "def sum_vec(text):\n",
    "    vec = np.zeros(100).reshape((1, 100))  # 初始化一个和 Word2Vec 嵌入等长度的 0 向量\n",
    "    for word in text:\n",
    "        # 得到句子中每个词的词向量并累加在一起\n",
    "        if word in list(model.wv.index_to_key):\n",
    "            vec += model.wv.get_vector(word).reshape((1, 100))\n",
    "        else:\n",
    "            pass\n",
    "    return vec\n",
    "\n",
    "# 将词向量保存为 Ndarray\n",
    "X = np.concatenate([sum_vec(z) for z in tqdm(corpus)])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5c112-f13b-48ff-9ce1-a9be9faa929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 数据集拆分\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df.label, test_size=0.2)\n",
    "\n",
    "# 随机森林算法进行分类\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
