{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50a042c-f622-4ec3-be16-19c3194d3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 该数据集共有 50000 条评论数据，并被打上了积极（1）或消极（0）的标签。数据集中的每一条评论都经过预处理，并编码为词索引（整数）的序列表示。词索引的意思是，将词按数据集中出现的频率进行索引，例如整数 3 编码了数据中第三个最频繁的词。一般情况下，IMDB 数据集会被划分为训练集和测试集各占一半，斯坦福研究人员在 2011 年发布该数据集时，得到的预测准确率为 88.89%\n",
    "# 加载数据, num_words 表示只考虑最常用的 n 个词语，代表本次所用词汇表大小\n",
    "MAX_DICT = 1000\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    num_words=MAX_DICT\n",
    ")\n",
    "\n",
    "# X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599afcd8-2d48-4893-ab10-fc479b3ae06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  14,  22,  16,  43, 530, 973,   2,   2,  65, 458,   2,  66,\n",
       "         2,   4, 173,  36, 256,   5,  25, 100,  43, 838, 112,  50, 670,\n",
       "         2,   9,  35, 480, 284,   5, 150,   4, 172, 112, 167,   2, 336,\n",
       "       385,  39,   4, 172,   2,   2,  17, 546,  38,  13, 447,   4, 192,\n",
       "        50,  16,   6, 147,   2,  19,  14,  22,   4,   2,   2, 469,   4,\n",
       "        22,  71,  87,  12,  16,  43, 530,  38,  76,  15,  13,   2,   4,\n",
       "        22,  17, 515,  17,  12,  16, 626,  18,   2,   5,  62, 386,  12,\n",
       "         8, 316,   8, 106,   5,   4,   2,   2,  16, 480,  66,   2,  33,\n",
       "         4, 130,  12,  16,  38, 619,   5,  25, 124,  51,  36, 135,  48,\n",
       "        25,   2,  33,   6,  22,  12, 215,  28,  77,  52,   5,  14, 407,\n",
       "        16,  82,   2,   8,   4, 107, 117,   2,  15, 256,   4,   2,   7,\n",
       "         2,   5, 723,  36,  71,  43, 530, 476,  26, 400, 317,  46,   7,\n",
       "         4,   2,   2,  13, 104,  88,   4, 381,  15, 297,  98,  32,   2,\n",
       "        56,  26, 141,   6, 194,   2,  18,   4, 226,  22,  21, 134, 476,\n",
       "        26, 480,   5, 144,  30,   2,  18,  51,  36,  28, 224,  92,  25,\n",
       "       104,   4, 226,  65,  16,  38,   2,  88,  12,  16, 283,   5,  16,\n",
       "         2, 113, 103,  32,  15,  16,   2,  19, 178,  32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这是一段话里，每个单词的索引\n",
    "np.array(X_train[0])  # 直接运行\n",
    "# array([  1,  14,  22,  16,  43, 530, 973,   2,   2,  65, 458,   2,  66,\n",
    "#          2,   4, 173,  36, 256,   5,  25, 100,  43, 838, 112,  50, 670,\n",
    "#          2,   9,  35, 480, 284,   5, 150,   4, 172, 112, 167,   2, 336,\n",
    "#        385,  39,   4, 172,   2,   2,  17, 546,  38,  13, 447,   4, 192,\n",
    "#         50,  16,   6, 147,   2,  19,  14,  22,   4,   2,   2, 469,   4,\n",
    "#         22,  71,  87,  12,  16,  43, 530,  38,  76,  15,  13,   2,   4,\n",
    "#         22,  17, 515,  17,  12,  16, 626,  18,   2,   5,  62, 386,  12,\n",
    "#          8, 316,   8, 106,   5,   4,   2,   2,  16, 480,  66,   2,  33,\n",
    "#          4, 130,  12,  16,  38, 619,   5,  25, 124,  51,  36, 135,  48,\n",
    "#         25,   2,  33,   6,  22,  12, 215,  28,  77,  52,   5,  14, 407,\n",
    "#         16,  82,   2,   8,   4, 107, 117,   2,  15, 256,   4,   2,   7,\n",
    "#          2,   5, 723,  36,  71,  43, 530, 476,  26, 400, 317,  46,   7,\n",
    "#          4,   2,   2,  13, 104,  88,   4, 381,  15, 297,  98,  32,   2,\n",
    "#         56,  26, 141,   6, 194,   2,  18,   4, 226,  22,  21, 134, 476,\n",
    "#         26, 480,   5, 144,  30,   2,  18,  51,  36,  28, 224,  92,  25,\n",
    "#        104,   4, 226,  65,  16,  38,   2,  88,  12,  16, 283,   5,  16,\n",
    "#          2, 113, 103,  32,  15,  16,   2,  19, 178,  32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55cbb64-7b55-4c86-9d6a-d992f36edc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tf.keras.datasets.imdb.get_word_index()  # 获取词索引表\n",
    "\n",
    "# 生成 index -> word 映射\n",
    "# {34701: 'fawn',\n",
    "#  52006: 'tsukino'}\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebb5f0a-3383-4839-802c-64751468f4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# this film was just brilliant casting # # story direction # really # the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same # # as myself so i loved the fact there was a real # with this film the # # throughout the film were great it was just brilliant so much that i # the film as soon as it was released for # and would recommend it to everyone to watch and the # # was amazing really # at the end it was so sad and you know what they say if you # at a film it must have been good and this definitely was also # to the two little # that played the # of # and paul they were just brilliant children are often left out of the # # i think because the stars that play them all # up are such a big # for the whole film but these children are amazing and should be # for what they have done don't you think the whole story was so # because it was true and was # life after all that was # with us all\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为什么-3, 0、1、2是为“padding”（填充）、“start of sequence”（序列开始）、“unknown”（未知词）分别保留的索引\n",
    "comment = \" \".join(\n",
    "    [reverse_index.get(i - 3, \"#\") for i in X_train[0]]\n",
    ")  # 还原第 1 条评论\n",
    "comment\n",
    "# \"# this film was just brilliant casting # # story direction # really # the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same # # as myself so i loved the fact there was a real # with this film the # # throughout the film were great it was just brilliant so much that i # the film as soon as it was released for # and would recommend it to everyone to watch and the # # was amazing really # at the end it was so sad and you know what they say if you # at a film it must have been good and this definitely was also # to the two little # that played the # of # and paul they were just brilliant children are often left out of the # # i think because the stars that play them all # up are such a big # for the whole film but these children are amazing and should be # for what they have done don't you think the whole story was so # because it was true and was # life after all that was # with us all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c8bf1b-f7d6-461f-bbee-6f1e96948514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000, 200))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 神经网络输入时，必须保证每一条数据的形状是一致的，所以这里需要对数据进行预处理\n",
    "MAX_LEN = 200  # 设定句子最大长度\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, MAX_LEN)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, MAX_LEN)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780326c-d5a5-44b5-a5a9-8a5384ccc49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "# 词索引特征化手段\n",
    "# 字典只能单纯将词处理成数值，但 Embedding 却可以让词与词直接产生联系: 向量距离\n",
    "\n",
    "# tf.keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "# - input_dim：int > 0，词汇表大小。\n",
    "# - output_dim：int >= 0，词向量的维度。\n",
    "# - input_length：输入序列的长度，当它是固定的时候。如果你需要连接 Flatten 和 Dense 层，则这个参数是必须的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1d85ed-3eac-44de-9fda-d7f35cd5a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91658\\Miniconda3\\envs\\ml\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                     </span>┃<span style=\"font-weight: bold\"> Output Shape            </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)            │ ?                       │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────┼─────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                │ ?                       │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────┼─────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                       │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────┴─────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)            │ ?                       │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────┼─────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                │ ?                       │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────┼─────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                    │ ?                       │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────┴─────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 搭建一个简单的全连接网络来完成评论情绪分类\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# 将整数形式的词典索引转换为固定大小的稠密向量\n",
    "model.add(tf.keras.layers.Embedding(MAX_DICT, 16, input_length=MAX_LEN))\n",
    "\n",
    "# 添加一个展平层（Flatten Layer），将嵌入层的输出展平为一维向量，以便连接到全连接层\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# 添加一个全连接层（Dense Layer），输出维度为1，使用 sigmoid 激活函数来输出二分类概率\n",
    "# 输出是一个二分类，情绪是正向 or 逆向\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65bc2e8-5f8a-4c50-8a4f-7674b9b7f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "287a1a2e-0a10-414b-8fc3-64ea1fd4a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5965 - loss: 0.6513 - val_accuracy: 0.8327 - val_loss: 0.3888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21f3ec6bed0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "# X_train 是索引，对索引进行训练\n",
    "model.fit(X_train, y_train, BATCH_SIZE, EPOCHS, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c046a72-645a-4341-b2cb-fdb1af64c804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfc3e3-0724-4d9f-9948-b165ebcb42d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0843e69-51e1-45a8-9b80-b0dd7031ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单 RNN 神经网络：全连接的 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a861a-680d-4f43-8f51-7a45e14ecf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tf.keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)`\n",
    "# - units: 正整数，输出空间的维度。\n",
    "# - activation: 要使用的激活函数。如果传入 None，则使用线性激活。\n",
    "# - use_bias: 布尔值，该层是否使用偏置项量。\n",
    "# - dropout: 在 0 和 1 之间的浮点数。\n",
    "# - return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。\n",
    "\n",
    "# Dropout: 深度学习中经常会接触到的概念，其经常以 tf.keras.layers.Dropout 🔗 这样的网络层出现。Dropout 主要的作用是防止过拟合，实现原理是以一定概率（Dropout 参数值）断开神经元之间的连接\n",
    "\n",
    "model_RNN = tf.keras.Sequential()\n",
    "model_RNN.add(tf.keras.layers.Embedding(MAX_DICT, 32))\n",
    "# dropout 是层与层之前的 dropout 数值，recurrent_dropout 是上个时序与这个时序的 dropout 值\n",
    "model_RNN.add(tf.keras.layers.SimpleRNN(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_RNN.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba13b59-1e97-4d71-a15d-fde5e4d8f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_RNN.fit(X_train, y_train, BATCH_SIZE, EPOCHS, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee41941-cf37-4cac-a201-2e5a1391aa08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db05337-e1de-4021-8ba2-ce82d6fbec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d44d7e-2217-45a8-bb34-46e4ef308d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = tf.keras.Sequential()\n",
    "# 将整数形式的词典索引转换为固定大小的稠密向量\n",
    "model_LSTM.add(tf.keras.layers.Embedding(MAX_DICT, 32))\n",
    "model_LSTM.add(tf.keras.layers.LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_LSTM.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_LSTM.summary()\n",
    "# LSTM 比起简单 RNN 会多学到一些参数，但这些参数帮助我们规避了梯度消失等致命性问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a8546-d739-44ea-a9a7-ca4d64c8c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_LSTM.fit(X_train, y_train, BATCH_SIZE, EPOCHS, validation_data=(X_test, y_test))\n",
    "# LSTM 做为一个循环 神经网络的模块，设计非常巧妙，通过遗忘门和输入门对记忆单元不断更新，消除了循环 神经网络训练时梯度消失的致命问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48a677-a8f9-48c6-935e-5872f048992b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ab1a7-4b36-4c90-8710-61ecf9009063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fc76e-51a0-45cd-98f5-4df4c2a20982",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = tf.keras.Sequential()\n",
    "model_GRU.add(tf.keras.layers.Embedding(MAX_DICT, 32))\n",
    "model_GRU.add(tf.keras.layers.GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_GRU.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378fbca-769a-46f2-848a-b3bb922a300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_GRU.fit(X_train, y_train, BATCH_SIZE, EPOCHS, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
