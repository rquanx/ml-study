{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1762507a-ebdc-4e1f-8d11-e152baf8ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91658\\AppData\\Local\\Temp\\ipykernel_3860\\512922365.py:4: FutureWarning: The 'delim_whitespace' keyword in pd.read_table is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_table('./trec06c/full/index', header=None,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>../data/000/000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>../data/000/001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>../data/000/002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>../data/000/003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>../data/000/004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                1\n",
       "0  spam  ../data/000/000\n",
       "1   ham  ../data/000/001\n",
       "2  spam  ../data/000/002\n",
       "3  spam  ../data/000/003\n",
       "4  spam  ../data/000/004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.获取数据\n",
    "# 2.理解数据内容，进行预处理（数据清洗）\n",
    "#  - 邮件由两部分组成，第一部分包含了邮件的信息，例如发件人，标题等等，第二部分才是邮件正文。这些文件不是 UTF-8 编码的，所以需要将其转为 UTF-8 编码\n",
    "#  - 实验会用到的只有邮件正文内容，所以需要去除第一部分，另外正文部分还有许多链接等其他不需要的内容\n",
    "#    - 转换源数据编码格式为 UTF-8 格式。\n",
    "#    - 过滤字符：去除所有非中文字符，如标点符号、英文字符、数字、网站链接等特殊字符。\n",
    "#    - 过滤停用词。\n",
    "#    - 对邮件内容进行分词处理。\n",
    "# 2.1.数据清洗\n",
    "#  - 只保留汉字：通过正则表达式滤掉了所有英文，数字，标点符号，特殊符号，内容里还存在一些长相奇怪的文字，我们通过 Unicode 中文编码范围 0x4e00-0x9fff 过滤\n",
    "#  - 文本进行分词：利用 结巴分词 模块进行分词\n",
    "#    - 在中文中，有很多的非实意词语或者其他并没有实际作用的词语，这些词语必须在分词之后进行过滤，这个环节也就是过滤停用词\n",
    "#    - 分词之后需剔除只有一个字的结果，因为一个字基本上没有什么内容\n",
    "#  - 文字转向量：文字无法直接被算法理解\n",
    "# 3.数据划分及建模\n",
    "#  - 划分训练集和测试集\n",
    "#  - \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# https://cdn.aibydoing.com/hands-on-ai/files/trec06c.zip\n",
    "data = pd.read_table('./trec06c/full/index', header=None,\n",
    "                     encoding='gb2312', delim_whitespace=True)\n",
    "data.head()\n",
    "# 标记 spam 是垃圾邮件，标记 ham 是正常邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf2f92fb-4978-487e-a9d5-41320f48c5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91658\\AppData\\Local\\Temp\\ipykernel_3860\\78917340.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = data.replace(['spam', 'ham'], [0, 1])  # 0 替代 spam，1 替代 ham\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1\n",
       "0      \n",
       "0  6595\n",
       "1  3405"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.replace(['spam', 'ham'], [0, 1])  # 0 替代 spam，1 替代 ham\n",
    "df = df.replace(regex=[\"\\..\"], value='trec06c')  # 替换掉文件路径\n",
    "df = df.sample(len(df), random_state=1, )[:10000]  # 打乱样本并取前 1 万条数据\n",
    "df.groupby(0).count()  # 统计样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d591ec-2c24-4d3b-961f-c8e2fc407587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例垃圾邮件内容\n",
    "\n",
    "# Received: from 163.con ([61.141.165.252])\n",
    "# \tby spam-gw.ccert.edu.cn (MIMEDefang) with ESMTP id j7CHJ2B9028021\n",
    "# \tfor <xing@ccert.edu.cn>; Sun, 14 Aug 2005 10:04:03 +0800 (CST)\n",
    "# Message-ID: <200508130119.j7CHJ2B9028021@spam-gw.ccert.edu.cn>\n",
    "# From: =?GB2312?B?1cW6o8TP?= <jian@163.con>\n",
    "# Subject: =?gb2312?B?uavLvtK1zvEutPq/qreixrGjoQ==?=\n",
    "# To: xing@ccert.edu.cn\n",
    "# Content-Type: text/plain;charset=\"GB2312\"\n",
    "# Date: Sun, 14 Aug 2005 10:17:57 +0800\n",
    "# X-Priority: 2\n",
    "# X-Mailer: Microsoft Outlook Express 5.50.4133.2400\n",
    "\n",
    "# 尊敬的贵公司(财务/经理)负责人您好！  \n",
    "#         我是深圳金海实业有限公司（广州。东莞）等省市有分公司。  \n",
    "#     我司有良好的社会关系和实力，因每月进项多出项少现有一部分  \n",
    "#     发票可优惠对外代开税率较低，增值税发票为5%其它国税.地税.     \n",
    "#     运输.广告等普通发票为1.5%的税点，还可以根据数目大小来衡  \n",
    "#     量优惠的多少，希望贵公司.商家等来电商谈欢迎合作。\n",
    "   \n",
    "#        本公司郑重承诺所用票据可到税务局验证或抵扣！\n",
    "#     欢迎来电进一步商谈。\n",
    "#     电话：13826556538（24小时服务）\n",
    "#     信箱：szlianfen@163.com\n",
    "#     联系人：张海南\n",
    "\n",
    "               \n",
    "#        顺祝商祺   \n",
    "                 \n",
    "\n",
    "#                    深圳市金海实业有限公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2315d75-2d14-4b69-8be0-624eba608e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(line):\n",
    "    # 清理邮件，替换不需要的字符串\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\n",
    "        \"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75210e4a-d5a8-4713-b839-d82a17b1ca1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '#', '$', '%']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_stopwords(file_path):\n",
    "    # 加载停用词\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        stopwords = [line.strip('\\n') for line in f.readlines()]\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stopwords('./stopwords.txt')\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e856ae4-f765-44c3-a971-f4ee77a6a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def process(file_path, test_mode=False):\n",
    "    # 清洗一封邮件\n",
    "    '''\n",
    "    - file_path: 文本文件路径\n",
    "    - test_mode: 测试模式，后文我们会将一个字符串写入文件(utf-8 编码)，而训练文件以 GBK 编码，\n",
    "                 如果自己实现分类，请注意编码格式，通常为 utf-8\n",
    "    - return: words, 处理、分词之后的有效词语\n",
    "    '''\n",
    "    words = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            if not test_mode:\n",
    "                line = line.strip().decode(\"gbk\", 'ignore')\n",
    "            else:\n",
    "                line = line.strip().decode(\"utf-8\", 'ignore')\n",
    "            line = clean_str(line)\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            seg_list = list(jieba.cut(line, cut_all=False))\n",
    "            for x in seg_list:\n",
    "                if len(x) <= 1:\n",
    "                    continue\n",
    "                if x in stopwords:\n",
    "                    continue\n",
    "                words.append(x)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c098ad2-5dea-4b03-8e91-1167fc2b8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行单个邮件处理\n",
    "# words = process('trec06c/data/000/000')\n",
    "# \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7918474-8240-4c60-af88-e23cdcab01ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0/10000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\91658\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.654 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████████████████████████████| 10000/10000 [01:03<00:00, 158.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37029</th>\n",
       "      <td>1</td>\n",
       "      <td>trec06c/data/123/129</td>\n",
       "      <td>[恋爱, 第三次, 告诉, 再见面, 时间, 我要, 考研, 考到, 北京, 是否是, 喜欢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>0</td>\n",
       "      <td>trec06c/data/007/157</td>\n",
       "      <td>[欣欣, 签约, 推出, 中国, 第一个, 彩铃, 歌手, 稀稀, 龙乐, 公司, 签约, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50881</th>\n",
       "      <td>1</td>\n",
       "      <td>trec06c/data/169/181</td>\n",
       "      <td>[男生, 思路, 简单, 心痛, 直说, 原因, 不让, 担心, 他累, 不去, 撒娇, 撒...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10843</th>\n",
       "      <td>0</td>\n",
       "      <td>trec06c/data/036/043</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>0</td>\n",
       "      <td>trec06c/data/015/189</td>\n",
       "      <td>[本港, 会计师, 权威机构, 香港, 瑞丰, 会计师, 事务所, 注册, 海外, 国际, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                     1  \\\n",
       "37029  1  trec06c/data/123/129   \n",
       "2257   0  trec06c/data/007/157   \n",
       "50881  1  trec06c/data/169/181   \n",
       "10843  0  trec06c/data/036/043   \n",
       "4689   0  trec06c/data/015/189   \n",
       "\n",
       "                                                   words  \n",
       "37029  [恋爱, 第三次, 告诉, 再见面, 时间, 我要, 考研, 考到, 北京, 是否是, 喜欢...  \n",
       "2257   [欣欣, 签约, 推出, 中国, 第一个, 彩铃, 歌手, 稀稀, 龙乐, 公司, 签约, ...  \n",
       "50881  [男生, 思路, 简单, 心痛, 直说, 原因, 不让, 担心, 他累, 不去, 撒娇, 撒...  \n",
       "10843                                                 []  \n",
       "4689   [本港, 会计师, 权威机构, 香港, 瑞丰, 会计师, 事务所, 注册, 海外, 国际, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()  # 使用 tqdm 显示进度\n",
    "# 将 apply 函数替换为 progress_apply 以使用 tqdm 显示处理进度\n",
    "df[\"words\"] = df[1].progress_apply(process)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7447634e-d761-47f3-b47b-f46c85faa51f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\91658\\Miniconda3\\envs\\ml\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_notebook\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 移除一些不必要的警告\u001b[39;00m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\ml\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\ml\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\ml\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\ml\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\ml\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\91658\\Miniconda3\\envs\\ml\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# 移除一些不必要的警告\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 导入上面保存的分词数组\n",
    "data = df[\"words\"]\n",
    "\n",
    "# 训练 Word2Vec 浅层神经网络模型\n",
    "w2v_model = Word2Vec(vector_size=100, min_count=10)\n",
    "w2v_model.build_vocab(data)\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=5)\n",
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0398e67-6f18-4cbf-9b18-027d2d05b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将词向量保存为 Ndarray\n",
    "data_vec = np.concatenate([sum_vec(z) for z in tqdm_notebook(data)])\n",
    "data_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24962dd6-276d-445b-8b42-390ef6d2b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_data = data_vec\n",
    "label_data = df[0].values\n",
    "# 分割数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_data, label_data, test_size=0.2, random_state=4\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b7f59-e280-4c4f-86fe-7fb9378eb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha 表示平滑参数，如拉普拉斯平滑则 alpha=1。\n",
    "\n",
    "# fit_prior 表示是否使用先验概率，默认为 True。\n",
    "\n",
    "# class_prior 表示类的先验概率。\n",
    "\n",
    "# 常用方法:\n",
    "\n",
    "# fit(x,y)选择合适的贝叶斯分类器。\n",
    "\n",
    "# predict(X) 对数据集进行预测返回预测结果。\n",
    "\n",
    "# sklearn.naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629f4ee-c765-41af-8c5e-88bbf5b5eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()  # 定义伯努利模型分类器\n",
    "model.fit(X_train, y_train)  # 模型训练\n",
    "y_pred = model.predict(X_test)  # 模型预测\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20acdc01-db81-447d-b836-834f717151ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571143a-7059-48a9-9319-9738ec460b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
