机器学习 Machine Learning 是人工智能的一个分支，其核心构成为机器学习算法，并通过从数据中获取经验来改善自身的性能。

[动手实战人工智能 AI By Doing](https://aibydoing.com/intro)

# 概念

**人工智能**：最先出现，并且涵盖最广的
**机器学习**：为了实现人工智能的手段
**深度学习**：机器学习下的一种方式

## 机器学习

大致包含四大类
- 监督学习
- 无监督学习
- 半监督学习
- 强化学习

## 监督学习

监督学习是基于示例输入-输出数据对，在输入和输出数据之间建立数学函数的机器学习任务，而该数学函数来源于对有标签训练数据集的学习过程。
- 输入：训练数据集中的特征变量
- 输出：标签
- 数学函数：机器学习预测模型

监督学习的特点是：训练数据集有标签

应用：
- 分类：动物的种类判断、植物的种类判断，表现为标签
- 回归：股票价格预测，房价预测，洪水水位线预测，表现为数值

### 线性回归

通过找到一条直线去拟合数据点的分布趋势的过程，就是线性回归的过程

找到最适合的那一条直线，是线性回归中需要解决的目标问题

#### 一元线性回归

表达式：y = w0 + w1 * x
```py
def f(x: list, w0: float, w1: float):
    """一元一次函数表达式"""
    y = w0 + w1 * x
    return y
```

#### 平方损失函数

**残差（损失）**

真实值和预测值之间的偏离程度

每个数据点的损失：
![17124282270991712428226182.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282270991712428226182.png)


损失总和：
![17124282611011712428261010.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282611011712428261010.png)

一般使用残差的平方和来表示所有样本点的误差，即平方损失函数
![17124282840981712428283141.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282840981712428283141.png)

为什么要用平方和：能保证损失始终是累加的正数，而不会存在正负残差抵消的问题

使平方和的值最小，就能得到拟合的w0、w1

```py
def square_loss(x: np.ndarray, y: np.ndarray, w0: float, w1: float):
    """平方损失函数"""
    loss = sum(np.square(y - (w0 + w1 * x)))
    return loss
```


#### 最小二乘法代数求解

用于求解线性回归拟合参数的一种常用方法
- 二乘: 代表平方
- 平方: 平方损失函数
- 最小二乘: 最小平方，使平方损失函数最小

为了求得参数，分别对未知数进行求导：
![17124286380981712428637687.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124286380981712428637687.png)

再令导数值为0
![17124287260981712428725502.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124287260981712428725502.png)

```py
def least_squares_algebraic(x: np.ndarray, y: np.ndarray):
    """最小二乘法代数求解"""
    n = x.shape[0]
    w1 = (n * sum(x * y) - sum(x) * sum(y)) / (n * sum(x * x) - sum(x) * sum(x))
    w0 = (sum(x * x) * sum(y) - sum(x) * sum(x * y)) / (
        n * sum(x * x) - sum(x) * sum(x)
    )
    return w0, w1
```

**为什么要令导数为0**

要使平方损失函数得到最小值，由于平方损失函数是二次函数，且 >= 0,即是开口向上的抛物线，此时导数为0处即为极小值

**为什么要偏导**

二元函数求极值（二元函数是三维的图像）
多元函数的极值求法

多元函数的极值点（想象在山峰最高或者山谷最低），此时在临界点处不管往哪个方向都会是导数为0（高度不会再变化），所以偏导值为0，是多元函数极值的一个条件
对于一元线性回归，一阶偏导为0，即为最低点


#### 最小二乘法矩阵求解

一元线性函数表示为矩阵
![17125138433131712513842390.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17125138433131712513842390.png)

y = XW
- X 是输入数据的矩阵（经过补充矩阵维度）[1,x1]
- W 是参数 w0、w1的矩阵 [w0,w1]T

[1,x1] * [w0,w1]T = w0 + w1 * x

以矩阵形式表示损失函数
![17125139543111712513953369.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17125139543111712513953369.png)
- y 为实际值的集合（数组）
- XW = yi 为拟合的值
- XW 是 10 行 1 列矩阵
- (y - XW)T： 10 行 1 列矩阵转置，为 1行，10列
- (y - XW)T * (y - XW)：自己的转置 * 自己 即为求自己的平方和
  - 1 x 10 矩阵 * 10 * 1 矩阵 = 1 * 1即为总和

通过分配率，进一步计算
![17126027113111712602711252.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126027113111712602711252.png)
![17126027856401712602784724.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126027856401712602784724.png)
- yT * XW： 1 行 10 列矩阵 * 10 行 1 列矩阵 = 一个数
- (XW)T * y: 同上 === 一个数，且之与 yT * XW 一样
- 所以合并

对矩阵进行求偏导（一次性对所有参数进行求导）
![17126052286381712605228529.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126052286381712605228529.png)
- yT * y 是一个常数对于 W 的偏导数为0
- −2(XW)Ty 的偏导数是 −2XT * y 因为 (XW) T * y 相对于 W 的导数是 XT * y
- (XW)T * XW: 设 XW = Q => QT * Q,应用链式法则......，很复杂

![17126052786371712605277830.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126052786371712605277830.png)

```py
def least_squares_matrix(x: np.matrix, y: np.matrix):
    """最小二乘法矩阵求解"""
    w = (x.T * x).I * x.T * y
    return w
```
#### 训练

训练一个机器学习预测模型时，我们通常会将数据集划分为 70% 和 30%
- 70% 的部分被称之为训练集，用于模型训练： 用于从训练集中找到最佳拟合参数的值
- 30% 被称为测试集：对比预测的目标值与真实目标值之间的差异，评估模型的预测性能

- 提取出训练数据集（作为变量x的数据）：训练特征
- 提取出数据结果（期望输出的数据）：训练目标
- 提取出测试数据集
- 提取出测试数据结果
- 模型训练（选择模型、数据处理、传入） ==> 得到拟合参数
- 输入测试集进行预测校验
  - 平均绝对误差（MAE）
  - 均方误差（MSE）

预测结果不对的原因
- 数据集是否精挑细选
- 模型选择是否合理：对于一些复杂的数据，无法通过线性方程来拟合

##### 平均绝对误差（MAE）

绝对误差的平均值：测试集预期结果 - 预测结果的差的绝对值的总和平均值

![17126864706391712686469843.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126864706391712686469843.png)

```py
def mae_solver(y_true: np.ndarray, y_pred: np.ndarray):
    """MAE 求解"""
    n = len(y_true)
    mae = sum(np.abs(y_true - y_pred)) / n
    return mae
```

##### 均方误差（MSE）

表示误差的平方的期望值：测试集预期结果 - 预测结果的差的绝对值的平方总和平均值，平方后数值会更明显？

![17126866576371712686657585.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126866576371712686657585.png)

```py
def mse_solver(y_true: np.ndarray, y_pred: np.ndarray):
    """mse 求解"""
    n = len(y_true)
    mse = sum(np.square(y_true - y_pred)) / n
    return mse
```

##### 平均绝对百分比误差 MAPE

MAPE 是一个百分比值，比其他统计量更容易理解,如果 MAPE 为 5，则表示预测结果较真实结果平均偏离 5%

![17128502516381712850250700.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128502516381712850250700.png)

```py
def mape_slover(y_true: np.ndarray, y_pred: np.ndarray):
    """mape 求解"""
    n = len(y_true)
    return np.sum(np.abs((y_true - y_pred) / y_true)) / n * 100 
```

### 多项式回归（非线性回归）

多项式：多个未知数的
多元：多次方


#### 多项式回归相当于线性回归的特殊形式

对于: w0 + w1 * x + w2 * x^2,进行替代 x = x1, x^2 = x2
等式就变成： w0 + w1 * x1 + w2 * x2，即多元线性

一元高次多项式  -> 多元一次多项式

一般变量数据都是有多个，所以 x1 可以是列向量，x1、x2 可以构成特征矩阵来计算
![17128556746381712855673846.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128556746381712855673846.png)

根据least_squares_matrix，X代入构造的特征矩阵进行计算

**计算过程**

一元线性函数可表示为：![17128543938941712854393865.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128543938941712854393865.png)
- y = XW

一元二次 => 二元线性回归：![17128545686381712854568382.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128545686381712854568382.png)
- 也让 y = XW
- X = [1,x1,x2] => [1,x,x^2]
- W = [w0,w1,w2]T

接上线性矩阵计算......

#### 一元高阶多项式

![17128518155691712851815547.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128518155691712851815547.png)


#### 练习

一个题目，进行线性回归、二次多项式回归......

不知道选择哪一种模型，可以直接跑多次，看误差数据变化（mae、mse......）,当误差图像稳定，取第一个开始稳定的点，就是相对好的点（防止过拟合）

### 其他

#### 普通最小二乘法的局限性

二范数：欧几里得范数或L2范数,向量的平方和再开根
|X|2 = sqrt(x1^2 + x2^2......)
![17130834936921713083493655.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130834936921713083493655.png)

参数解：
![17130837166371713083716258.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130837166371713083716258.png)

参数解成立的条件就是 |XT * X|不能为 0。而当变量之间的相关性较强（多重共线性），或者 m 大于 n 时，式中的 X 不是满秩（rank(A) ≠ dim(x)）矩阵。从而使得 |XT * X| 的结果趋近于 0，造成拟合参数的数值不稳定性增加，这也就是普通最小二乘法的局限。
> 满秩: 矩阵秩等于行数，称为行满秩,矩阵秩等于列数，称为列满秩,既是行满秩又是列满秩则为n阶矩阵即n阶方阵
> k阶子式：从矩阵中任取k行k列，即为矩阵的k阶子式（从矩阵中拆除方阵）
> 矩阵求秩：经过初等变化后（最简矩阵），能从矩阵中拆出来的最大k阶子式(子式不为0)，那么秩就是K
> 初等变换：行交换、列交换、某一行 * k（k ≠ 0）、某一列 * k、某一行加到另一行上、某一列加到另一列上（分别是行变换、列变换各3种）
> 最简矩阵：一般指的是行最简形矩阵
>   最简行矩阵：非零行的第一个非零元素全是1，且非零行的第一个元素1所在的列的其余元素全是0的矩阵（每一行的第一个数是1，且所在的列其余为0）

![17130852817571713085281730.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130852817571713085281730.png)
> |XT * X| 是矩阵的行列式值
> |XT * X| 趋近于0： 意味着矩阵 X 的列向量之间存在极高的线性相关性
>   - 奇异矩阵：没有解
>   - 多重共性：当模型的解释变量高度相关时，即一个变量可以通过其他变量的线性组合来近似表达，对于输入数据中的小波动或噪声非常敏感，从而造成最小二乘法求解的系数会不稳定
> 行列式(determinant): 一个数字，表示| X |
>   - 对于二阶矩阵，所围平面的面积、对于三阶矩阵，所围成的六面体的体积、对于n维矩阵，它就是n维立体的体积


**无法处理的场景**
 
数据集的列（特征）数量 > 数据量（行数量），即 X 不是列满秩。

数据集列（特征）数据之间存在较强的线性相关性，即模型容易出现过拟合。

##### 希尔伯特矩阵 OLS 线性拟合

希尔伯特矩阵是一种系数都是单位分数的方块矩阵: H(i,j) = 1 / (i + j - 1)
![17130857436371713085743607.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130857436371713085743607.png)

希尔伯特矩阵每列数据之间存在较强的线性相关性,满足 XT * X 趋近于 0，可用来验证局限性

##### 皮尔逊相关系数（Pearson Correlation Coefficient）

通常用于度量两个变量 x 和 y 之间的线性相关程度，其值介于 -1 与 1 之间。其中，数值越趋近于 1 表示正相关程度越高，趋近于 0 表示线性相关度越低，趋近于 -1 则表示负相关程度越高

##### 岭回归（Ridge Regression）

为了解决普通最小二乘法局限的方法，改良的最小二乘法

通过向损失函数中添加 L2 正则项（2-范数）有效防止模型出现过拟合

损失函数：
![17131036466371713103645733.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036466371713103645733.png)

向量表示：
![17131036666371713103666108.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036666371713103666108.png)

参数解：
![17131036976371713103697332.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036976371713103697332.png)
> 通过给 XT * X 增加一个单位矩阵，从而使得矩阵变成满秩，完善普通最小二乘法的不足



##### LASSO

LASSO 回归同样是通过添加正则项来改进普通最小二乘法，不过这里添加的是 L1 正则项

![17131052146381713105214376.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131052146381713105214376.png)

##### 正则项

正则化（Regularization）是一种常用的技术，它通过在目标函数中增加一个 惩罚项 来控制模型的复杂度，从而防止过拟合问题的出现

正则化项通常添加在模型的损失函数（目标函数）中

方式：
- L1正则化(LASSO)：系数向量w的L1范数，即绝对值之和
  - 使部分系数变为0，从而实现特征选择和降维
- L2正则化(岭回归)：系数向量w的L2范数的平方，即平方和开根号
  - 使系数向量w的每个分量都尽可能小，从而防止过拟合问题的出现

正则化系数 λ 可以通过交叉验证等方法来确定，通常取值范围为 0到1 之间的实数，数值越大，正则化项的惩罚力度越强，模型越倾向于选择较小的系数

### 回归模型评价与检验

在一元线性回归模型中，一般需要：拟合优度检验、变量的显著性检验及参数的区间估计
> 一元线性回归中的 T 检验和 F 检验一致

多元线性回归模型中有：拟合优度检验、变量的显著性检验（T 检验）、方程的显著性检验（F 检验）及参数的置信区间

#### 可解释和不可解释

在回归分析中，可解释和不可解释部分指的是数据变异性（即变化）的两个来源：

可解释部分（Explained Variation）：这是由回归模型捕捉的变异性。它反映了因变量（响应变量）的变化中可以通过模型中的自变量（解释变量）来解释的部分。
在线性回归中，可解释部分通常通过计算拟合值和因变量平均值的差来得到。
这些差的平方和就是总变异中可以被模型解释的部分。它显示了模型如何有效地利用自变量来预测因变量。

不可解释部分（Unexplained Variation）：这是回归模型无法捕捉的变异性，通常被认为是随机误差或噪声。
这些变异性可能是由数据中未观测到的因素、测量误差、或模型不完全贴合真实关系等原因造成的。
在线性回归中，不可解释部分由残差表示，即实际观测值与模型预测值之间的差异。

总变异（Total Variation）是可解释和不可解释部分的总和。在确定模型的好坏时，我们希望可解释部分尽可能大，不可解释部分尽可能小。一般而言，如果模型具有高度的解释能力，可解释变异会占总变异的较大比例，这通常通过 R^2 统计量（决定系数）来衡量，R^2 值越接近1，表示模型的解释能力越强。

#### 一元线性回归的拟合优度检验

一般会使用判定系数 R^2 作为度量拟合优度的指标

离差：y(数据) - y(数据平均值)

![17131070566361713107056275.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131070566361713107056275.png)
> 往离差中 - 拟合值 + 拟合值 得到, 再分别拆分为e(i) 和 Y(拟合)
>   - e(i)：实际观测值与样本回归拟合值之差，也就是「残差」
>       - e(i) 为0时即拟合值等于观察值，此时拟合最佳，并且离差与残差无关，误差来自于回归线本身数据，所以不能解释
>   - Y(拟合)：样本回归拟合值与观测均值之差

![17131086431091713108643080.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131086431091713108643080.png)
- TSS：离差平方,总体平方和(Total Sum of Squares)
- ESS: Y(拟合)平方,回归平方和(Explained Sum of Squares)
- RSS: e(i)平方,残差平方和(Residual Sum of Squares)

![17131087216371713108721198.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131087216371713108721198.png)
> [TSS 和 ESS、RSS的关系 TSS = ESS + RSS](https://aibydoing.com/notebooks/chapter01-08-lab-evaluation-and-validation-of-regression-models)

当 TSS 不变，实际观测点离样本回归拟合线越近，则 ESS 在 TSS 中占的比重越大。
因此，我们定义拟合优度等于回归平方和 ESS 与 y 的总离差 TSS 的比值。
![17131091926401713109192021.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131091926401713109192021.png)
当 RSS 越小时，R^2 就越趋近于 1，那么代表模型的解释力越强
- 一般有效值在 0 ~ 1 间，值越大，模型拟合度越好
- scikit-learn 提供的 API 有可能计算出 R^2 值为负数的情况,这时候 TSS = RSS + ESS 会失效，具体 R^2 值取多少需要视情况而定，不同问题不一样


#### 变量显著性检验

回归分析中判断解释变量 x 是否是被解释变量 y 的一个显著性的影响因素,利用了数学的假设检验知识

假设检验的原理：通过事先对总体参数或总体分布形式作出假设。然后，利用样本信息来判断原假设是否合理。也就是说，通过判断样本信息与原假设是否有显著差异，从而决定是否接受或否定原假设

细节见代码


### 逻辑回归

Logistic Regression,逻辑斯蒂回归,是一种分类方法

#### 线性可分和不可分

二维平面内，如果只使用一条直接就可以将样本分开，则称为线性可分，否则为线性不可分
三维空间则是一个平面去分

#### 线性回归分类

通过拟合一条直线去预测更多的连续值

二分类问题：只有两种类别，也可以称之为：0 - 1 分类问题

#### Sigmoid 分布函数

![17131914946371713191493684.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131914946371713191493684.png)

```py
def sigmoid(z):
    sigmoid = 1 / (1 + np.exp(-z))
    return sigmoid
```

图像呈现出完美的 S 型（Sigmoid 的含义）。它的取值仅介于 0 和 1 之间，且关于 x = 0 轴中心对称。同时当 x 越大时，y 越接近于 1，而 x 越小时，y 越接近于 0

- 输出范围：Sigmoid函数将任意实数映射到(0,1)区间内，非常适合表示概率。在二分类问题中，可以将这个值解释为属于某一类的概率。
- 形状特性：Sigmoid函数形状为"S"型，当输入接近0时，输出变化敏感，而输入值很大或很小的时候，输出趋于平稳，这使得它在区分两个类别时表现良好。
- 梯度特性：在学习过程中，Sigmoid函数的导数（梯度）表达简单，便于在算法中使用梯度下降法进行优化

数学定义: 如果一组连续随机变量符合 Sigmoid 函数样本分布，就称作为逻辑分布。

结合数学定义和特性：把线性函数拟合的结果使用 Sigmoid 函数压缩到 (0,1) 之间。如果线性函数的 y  值越大，也就代表概率越接近于 1，反之接近于 0

![17131920702691713192070243.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131920702691713192070243.png)

对于二分类，值要么是 0 要么是 1，所以概率分布可以这样设定
![17131922076371713192207398.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131922076371713192207398.png)


似然函数：在统计学中用来表示一个模型参数值下观察到的数据概率。
总概率：当多个事件独立发生时，这些事件同时发生的总概率是每个事件发生概率的乘积
> 在逻辑回归中，每个样本产生观察结果的概率是独立的，所以通过所有样本的概率乘积来构造似然函数
将似然函数转化为对数似然函数，从累乘转为累加
![17131923926391713192392307.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131923926391713192392307.png)

#### 对数损失函数

对数似然函数衡量了事件发生的总概率
根据最大似然估计原理，只需要通过对 L(w) 求最大值，即得到 w 的估计值。而在机器学习问题中，我们需要一个损失函数，并通过求其最小值来进行参数优化
> 最大似然估计（MLE）原理基于这样的想法：给定观测数据，在所有可能的参数值中，最有可能产生这些数据的参数值是最优的。
> 实际上，MLE是寻找一组参数，使得观测数据出现的概率（即似然）最大。如果参数值使得已知数据的似然最大，那么在统计意义上，这些参数就是最符合数据的。因此，最大化似然函数可以视为是在寻找最能“解释”观测数据的参数值，这些值通常被认为是这个模型在给定数据下的最优参数估计

除以m，获得平均损失，乘以 -1，将求最大值，转化为求最小值
![17131929726401713192972563.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131929726401713192972563.png)

```py
def loss(h, y):
    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    return loss
```

**为什么不用线性回归的平方损失函数**

我们要求损失最小的最优模型，只有凸函数能求得全局最小值，非凸函数一般只能得到局部最优解

#### 梯度下降法

梯度下降法用处：求取极小值
梯度：一个向量，表示某一函数在该点处的方向的导数沿着该方向取得最大值，即沿着该方向变化最快，变化率最大
- 对于一元函数：是某点的导数
- 对于多元函数：是某点的偏导组成的向量

梯度下降：即沿着下降方向（梯度的反方向）去寻找损失函数的极小值

对对数损失函数进行求导
![17131935956361713193594680.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131935956361713193594680.png)
![17131936226381713193621952.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131936226381713193621952.png)
![17131936386371713193638429.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131936386371713193638429.png)

将导数 * 常量a，即得到了每次梯度下降的步长，a 通常成为 学习率

每次权重的变化为 w = w - a * 导数

```py
def gradient(X, h, y):
    gradient = np.dot(X.T, (h - y)) / y.shape[0]
    return gradient
```

线性回归方法之所以使用普通最小二乘法来求解，是因为我们可以很方便地求出损失函数的最小值。但是，机器学习中的很多问题，往往会面对非常复杂的损失函数，这些损失函数一般无法直接求得最小值，只能使用迭代方法来求取局部或全局极小值

### K 近邻算法

在解决分类问题的过程中，K 近邻算法（简称：KNN）是一种简单而且实用的方法

#### 最近邻算法

最近邻算法（Nearest Neighbor，简称：NN）：针对未知类别数据 x，在训练集中找到与 x 最相似的训练样本 ，用 y 的样本对应的类别作为未知类别数据 x 的类别，从而达到分类的效果

![17132816910641713281691013.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132816910641713281691013.png)

#### K 近邻算法

K 近邻（K-Nearest Neighbors，简称：KNN）算法是最近邻（NN）算法的一个推广
> NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好

![17132818178801713281817150.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132818178801713281817150.png)

##### 步骤

1.数据准备：通过数据清洗，数据处理，将每条数据整理成向量。
2.计算距离：计算测试数据与训练数据之间的距离。
3.寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。
4.决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。

![17132818678781713281867547.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132818678781713281867547.png)

##### 距离度量

在计算两个样本间的相似度时，可以通过计算样本之间特征值的距离进行表示。

常用的两个距离：曼哈顿距离 和 欧式距离

![17132820608781713282060575.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132820608781713282060575.png)
> 直线距离和轴线距离？

**曼哈顿距离**

又称马氏距离，是计算距离最简单的方式之一

![17132821208781713282120695.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132821208781713282120695.png)
> 将两个数据 X 和 Y 中每一个对应特征值之间差值的绝对值，再求和，便得到曼哈顿距离。

```py
def d_man(x, y):
    d = np.sum(np.abs(x - y))
    return d
```

**欧式距离**

源自 N 维欧氏空间中两点之间的距离公式

![17132822468781713282246297.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132822468781713282246297.png)
> 将两个数据 X 和 Y 中的每一个对应特征值之间差值的平方，再求和，最后开平方

```py
def d_euc(x, y):
    d = np.sqrt(np.sum(np.square(x - y)))
    return d
```

##### 决策规则

根据数据特征对决策规则进行选取,从而通过 K 个邻居来判断测试样本的最终类别

- 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。
- 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。

##### KNN实现

```py
def d_euc(x, y):
    d = np.sqrt(np.sum(np.square(x - y)))
    return d
  
def majority_voting(class_count):
    # 多数表决函数
    sorted_class_count = sorted(
        class_count.items(), key=operator.itemgetter(1), reverse=True
    )
    return sorted_class_count

def knn_classify(test_data, train_data, labels, k):
    # KNN 方法完整实现
    distances = np.array([])  # 创建一个空的数组用于存放距离

    for each_data in train_data:  # 使用欧式距离计算数据相似度
        d = d_euc(test_data, each_data)
        distances = np.append(distances, d)

    sorted_distance_index = distances.argsort()  # 获取按距离从小到大排序后的索引
    sorted_distance = np.sort(distances)
    r = (sorted_distance[k] + sorted_distance[k - 1]) / 2  # 计算

    class_count = {}
    for i in range(k):  # 多数表决
        vote_label = labels[sorted_distance_index[i]]
        class_count[vote_label] = class_count.get(vote_label, 0) + 1

    final_label = majority_voting(class_count)
    return final_label, r
```

knn算法中，取的k值不同会极大的影响分类结果

#### KD 树算法

如果目前所在点比目前最佳点更靠近输入点，则将其变为目前最佳点。
检查另一边子树有没有更近的点，如果有则从该节点往下找

### K 近邻回归

- 数据准备：通过数据清洗，数据处理，将每条数据整理成向量。

- 计算距离：计算测试数据与训练数据之间的距离。

- 寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。

- 决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。


**「决策分类」是决定未知样本类别的关键步骤**

- 分类问题：根据 K 个邻居的类别，多数表决得到未知样本的类别。

- 回归问题：根据 K 个邻居的目标值，计算平均值得到未知样本的预测值。

###  朴素贝叶斯

朴素贝叶斯是以概率论作为基础的算法
- 实现简单
- 分类效率很高

#### 前置知识

##### 条件概率

![17136023711471713602371072.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136023711471713602371072.png)

条件概率就是指事件 A 在另外一个事件 B 已经发生条件下的概率

P(AB) = P(A) * P(B) // 同时发生A 和 B 的概率
P(AB) / P(B) = P(A|B) // 已发生b后a的发生概率

##### 贝叶斯定理

![17136026412091713602640971.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136026412091713602640971.png)

P(A|B) = P(AB) / P(B)
P(B|A) = P(AB) / P(A)

##### 先验概率

根据以往经验和分析得到的概率，例如抛硬币，根据经验就是0.5

##### 后验概率

事件发生后求的反向条件概率；即基于先验概率通过贝叶斯公式求得的反向条件概率,即反推

#### 朴素贝叶斯原理

将贝叶斯原理以及条件独立结合而成的算法

![17136028122111713602811839.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136028122111713602811839.png)

P(类别|特征) = P(特征|类别) * P(类别) / P(特征)


朴素贝叶斯中的「朴素」，即条件独立：将各个特征割裂开，认定特征之间相互独立
> 现实是很多时候特征间是有互相联系的，所以朴素贝叶斯会牺牲一定的准确性


##### 实现

![17136053539311713605353909.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136053539311713605353909.png)

需要利用极大似然估计进行估计先验概率

极大似然估计：通过若干次实验，观察其结果，利用实验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。


##### 贝叶斯估计

极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 0 的情况，影响后验概率的计算结果，贝叶斯估计可以解决

![17136076672461713607667114.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136076672461713607667114.png)

在极大似然估计增加一个 λ 因子，等价于在随机变量各个取值的频数上赋予一个正数，λ 为 0 时即为元素的极大似然估计，为 1 时称为拉普拉斯平滑

#### 朴素贝叶斯的三种常见模型

- 多项式模型：当特征值为离散时，常常使用多项式模型
- 伯努利模型：适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 1 和 0（以文本分类为例，某个单词在文档中出现过，则其特征值为 1，否则为 0）
- 高斯模型：当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多先验概率为0，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的
![17137134075591713713407509.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17137134075591713713407509.png)


### 分类模型评估

- 准确率
- 查准率
- 召回率
- F1 值
- ROC 曲线

#### 准确率

预测正确的结果占的比重

在二分类问题中，我们常常会定义正类和负类，就可以给出实际类别（行名）和预测类别（列名）的混淆矩阵
将结果拆分成 TP 、TN 、FP 、FN 四种情况
所以有准确率公式： (TP + TN) / (TP + FP + TN + FN)
- TP：预期正确，并且预测为正类
- TN：预测正确，并且预测为负
- FP: 失败，预测为正类（实际为负）
- FN: 失败，预测为负（实际为正）

![17138006977071713800697307.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138006977071713800697307.png)

#### 查准率

又称精确率，即正确分类的正例个数占分类为正例的实例个数的比例

预测了 n 个数据为正，那么准确的正占据所有正的多少

P = TP / (TP + FP)

你认为的该类样本，有多少猜对了（猜的准确率如何）

#### 召回率 Recall

查全率，即正确分类的正例个数占实际正例个数的比例

预测了 n 个数据为正，占据所有正的数据的多少

R = TP / (TP + FN)

该类样本有多少被找出来（召回了多少）

#### F1 值

查准率和召回率的加权平均数，精确率和召回率的综合评价指标，对衡量数据更有利

F1 = 2 * P * R / (P + R)

![17138012117111713801211356.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138012117111713801211356.png)

#### ROC 曲线

部分分类模型中（如：逻辑回归），通常会设定一个阈值，并规定大于该阈值为正类，小于则为负类。所以，当我们减小阀值时，将会有更多的样本被划分到正类。这样会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类

ROC 曲线的目的在用形象化该变化过程，从而评价一个分类器好坏

![17138013287101713801328301.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138013287101713801328301.png)

- 当 FPR=0，TPR=0 时，意味着将每一个实例都预测为负例。
- 当 FPR=1，TPR=1 时，意味着将每一个实例都预测为正例。
- 当 FPR=0，TPR=1 时，意味着为最优分类器点。

优秀分类器对应的 ROC 曲线应该尽量靠近左上角。当曲线越接近于 45 度对角线，则分类器效果越差

**AUC**

全称为 Area Under Curve,曲线下面积，ROC 曲线下面积
作用：将ROC的曲线视觉好坏，转成数值

- AUC = 1：完美分类器。
- 0.5 < AUC < 1：分类器优于随机猜测。
- AUC = 0.5：分类器和随机猜测的结果接近。
- AUC < 0.5：分类器比随机猜测的结果还差。

### 支持向量机

SVM（Support Vector Machine）

在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，不仅可以应用于线性分布数据，还可以用于非线性分布数据
相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法

#### 线性SVM

1.先找到数据分类分割线
2.然后在分割线两侧在设立两个平行的虚线，两条虚线与分割线的距离一致，这个距离成为“间隔”
3.使得分割线和虚线之间的间隔最大化，即两个虚线间隔最大（不进行错分的情况下）
4.最终实现正类虚线外的就是正类，负类虚线外的是负类
5.正好位于两条虚线上方的样本点就被我们称为“支持向量”，这也就是支持向量机的名字来源

支持向量机的目标：找到最大的分类间隔所对应的分割线

##### 函数间隔

对于线性SVM,决策函数定义为：f(x) = wT * x + b
假设分类结果为 1 和 -1，即结果 y = 1 / -1
那么就有：y * f(x) > 0,h = y * f(x) 就是函数间隔的定义
此时对于每个点 h，到分割平面都会有一个距离，
我们定义 h 取最小值时就是函数间隔，即平面与某个点的间隔值 h(min)

从公式可知，当 (w,b) 平面等比例放大时，f(x) 值会放大，但是平面并没有变，但是函数间隔也跟着变大了，所以函数间隔其实是一个动态值，单纯的函数间隔意义不大

##### 几何间隔



定义一个点 x ，其到超平面上的垂直映射点为 x0 ，两点之间的距离为 h，w 表示超平面的法向量

![17138034117261713803411045.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138034117261713803411045.png)

![17138034227261713803421966.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138034227261713803421966.png)

平面上点到线的距离，拓展到高维即为点到平面的距离，公式: d = (Ax + by + c) / (A^2 + B^2)开根
> 将上述公式转化成向量来写：d = (wT * x + b) / ||w||
> w = [A,B]T 是直线的法向量，x = [x,y]T 是点的坐标向量， b = c,wT * x 是向量的点积，||w|| 是向量 w 的范数即 (A^2 + B^2)开根
> 可见 几何间隔公式其实等价于 点到面的距离公式

公式：函数间隔除以 ||w|| 得到的就是几何间隔，而函数间隔的本质其实可以理解为 |f(x)| 

##### 拉格朗日对偶性

二次规划被用来找到几何间隔最优的决策边界

![17138888645531713888864504.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138888645531713888864504.png)

将从最大值转为求最小值，同时变为二次函数，此时可利用二次规划来求解

在满足 条件 的情况下，拉格朗日对偶性通过给每一个约束条件加上一个拉格朗日乘子 λ，并将约束条件融合到目标函数中

#### 非线性SVM

对于线性不可分的数据集，可以通过支持向量机去完成分类
> 需要增加一个技巧把线性不可分数据转换为线性可分数据之后，再完成分类
> 此技巧称为：核技巧
> 实现数据转换的函数称之为「核函数」

本质：将低维数据映射到高维空间中，使得数据集在高维空间能被线性可分

##### 核函数

核函数有很多种，但大多需要大量计算，目前有总结出几种计算量较小的常用核函数

![17138901103071713890109520.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138901103071713890109520.png)

- 线性核函数: k(xi,xj) = xi * xj
- 多项式核函数：k(xi,xj) = (xi * xj)^d, d > 1
- 高斯径向基：......
- sigmoid: ......

分别对应：linear, scikit-learn 中 SVC 方法的 poly, rbf, sigmoid

核函数也可以进行线性组合使用


#### 多分类支持向量机

支持向量机最初是为二分类问题设计的，当我们面对多分类问题时，其实同样可以使用支持向量机解决。而解决的方法就是通过组合多个二分类器来实现多分类器的构造

- 一对多法：即训练时依次把某个类别的样本归为一类，剩余的样本归为另一类，这样 k 个类别的样本就构造出了 k 个支持向量机。
- 一对一法：即在任意两类样本之间构造一个支持向量机，因此 k 个类别的样本就需要设计 k(k - 1) / 2 个支持向量机

#### 总结

SVM 是一种表现非常不错的方法，尤其是对于非线性分类问题。而且最大的劣势在于计算效率，随着数据集的增大，计算时间陡增。所以，一般我们会在小数据集下应用 SVM，而大数据集基本不予考虑

### 决策树

决策树是一种特殊的树形结构，一般由节点和有向边组成。
- 节点表示特征、属性或者一个类
- 有向边包含判断条件

决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树

例如：
![17139732085771713973208527.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139732085771713973208527.png)

#### 算法原理

- 数据准备 → 通过数据清洗和数据处理，将数据整理为没有缺省值的向量。
- 寻找最佳特征 → 遍历每个特征的每一种划分方式，找到最好的划分特征。
- 生成分支 → 划分成两个或多个节点。
- 生成决策树 → 对分裂后的节点分别继续执行 2-3 步，直到每个节点只有一种类别。
- 决策分类 → 根据训练决策树模型，将预测数据进行分类。

#### 信息增益（ID3）

**信息熵**

度量样本纯度最常用的一种指标

公式：![17139733548731713973354170.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139733548731713973354170.png)
- D表示样本集合
- p(k) 表示第 k 类样本所占的比例
- Ent(D) 越小则 D 的纯度越高

计算示例：![17139735548751713973554575.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139735548751713973554575.png)
- 所有结果概率 * log 的和

**信息增益**

是建立在信息熵的基础上

公式：![17139737308761713973730789.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139737308761713973730789.png)

信息增益越大，使用特征划分出来的集合纯度越高。

![17139739108791713973910831.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139739108791713973910831.png)

#### 信息增益率（C4.5）

信息增益的不足：当信息增益作为标准时，易偏向于取值较多的特征，为了避免这种偏好给预测结果带来的不好影响，可以使用增益率来选择最优划分

![17139740368971713974036871.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139740368971713974036871.png)

#### 连续值处理

对于离散值可以每个值作为一个分类，但是当数据是连续值时，这样分类就太多了，采用二分法对连续值进行处理

对于连续的属性 X 假设共出现了 n 个不同的取值，将这些取值从小到大排序 { x1, x2, x3 ......}，其中找一点作为划分点 t ，则将数据划分为两类，大于 t 的为一类，小于 t 的为另一类。而 t 的取值通常为相邻两点的平均数 t = (xi + xi+1) / 2

![17139742428821713974242782.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139742428821713974242782.png)

#### 预剪枝和后剪枝

决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多

对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝

##### 预剪枝

预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点

##### 后剪枝

在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。

#### 实现

1.数据清洗：
- 选取合适的特征（有效，少量减少计算量）
- 对数据进行简单分类：对于数值，可以简化分为几类
- 对数据特征进行替换：分类 -> 0、1、2、3
2.划分训练、测试集
3.训练模型


### CART 决策树

分类与回归树（classification and regression tree, CART）同样也是应用广泛的决策树学习算法，CART 算法是按照特征划分，由树的生成和树的剪枝构成，既可以进行分类又可以用于回归，按照作用将其分为决策树和回归树

![17139759908951713975990600.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139759908951713975990600.png)

### 超参数选择方法

#### 网格搜索

预先制定好各参数的有限个候选取值，然后通过排列组合的方式来传入这些参数，最终通过 K 折交叉验证的方法来确定表现最好的参数
- K 折交叉验证：是交叉验证中的一种常见方法，其通过将数据集均分成 K 个子集，并依次将其中的 K-1 个子集作为训练集，剩下的 1 个子集用作测试集。在 K 折交叉验证的过程中，每个子集均会被验证一次

**步骤**

- 首先将数据集均分为 K 个子集。
- 依次选取其中的 K-1 个子集作为训练集，剩下的 1 个子集用作测试集进行实验。
- 计算每次验证结果的平均值作为最终结果。

K 折交叉验证让每一条数据都有均等的几率被用于训练和验证，在一定程度上能提升模型的泛化能力

#### 随机搜索

网格搜索很直观，也很方便，但是最大的问题在于随着候选参数增多，搜索需要的时间迅速增加

随机搜索，顾名思义就是经验 + 运气的碰撞。我们依据经验制定一个参数范围，然后在范围内随机选取参数测试，并返回最佳结果
> 本质上还是类似的思路和方式，只是定了区间后，随机选取数据进行尝试

### 装袋和提升集成学习方法

每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，由于数据的不确定性，单独应用个别分类器可能会出现分类准确率低的问题。为了应对这样的情况，集成学习被提出，其可以利用多个弱分类器结合的方式提高分类准确率

#### 集成学习

通过构建多个分类器并综合使用来完成学习任务，同时也被称为多分类器系统。其最大的特点就是结合各个弱分类器的长处

集成分类
- 同质集成：在一个集成学习中，「个体学习器」是同一类型，如 「决策树集成」 所有个体学习器都为决策树
- 异质集成：在一个集成学习中，「个体学习器」为不同类型，如一个集成学习中可以包含决策树模型也可以包含支持向量机模型

集成方式：
- 并行式，当个体学习器之间不存在强依赖关系时，可同时生成并行化方法，其中代表算法为装袋（Bagging）算法。
- 串行式，当个体学习器之间存在强依赖关系时，必须串行生成序列化方法，其中代表算法为提升（Boosting）算法

![17140605014791714060500573.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17140605014791714060500573.png)

#### 结合策略

- 平均法: 在数值型输出中，最常用的结合策略为平均法
  - 简单平均法：取每一个「个体学习器」学习后的平均值
  - 加权平均法：其中 w(i) 是每一个「个体学习器」 h(i) 的权重，通常为 w(i) > 0 
- 投票法: 对于分类输出而言，平均法效果不太好,最常用的结合策略为投票法
  - 多数投票法：即在「个体学习器」分类完成后，通过投票选出分类最多的标签作为此次分类的结果。
  - 加权投票法：同加权平均法类似，w(i) 是每一个「个体学习器」 h(i) 的权重，通常为 w(i) > 0
- 学习法：平均法和投票法比较简单，但是可能学习误差较大
  - 代表方法：stacking 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，即把训练集弱学习器的学习结果作为输入，重新训练一个学习器来得到最终结果
    - 首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果

#### 装袋算法 Bagging

1.数据处理：将数据根据实际情况进行清洗整理。
2.随机采样：从样本中随机选出 m 个样本作为一个子样本集。有放回的重复 T 次，得到 T 个子样本集。
3.个体训练：设定 T 个个体学习器，将每一个子样本集放入对应个体学习器进行训练。
4.分类决策：用投票法集成进行分类决策。

##### Bagging Tre(决策树)

决策树是一个十分「完美」的训练器，但特别容易出现过拟合的情况，最终导致预测准确率低的问题

在装袋算法中，决策树常常被用作弱分类器

Bagging Tree 算法：是应用子数据集中的所有特征构建一棵完整的树，最终通过投票的方式进行预测

##### 随机森林 Random Forest

随机森林：「随机抽样 + 决策树森林」

将一个大的数据集使用自助采样法进行处理，即从原样本数据集中随机抽取多个子样本集，并基于每一个子样本集生成相应的决策树

构建出由许多小决策树组形成的决策树「森林」。最后，实验通过投票法选择决策树最多的预测结果作为最终的输出

**改进点**
1.对于普通的决策树，会在 N 个样本的所有特征中选择一个最优划分特征，但是随机森林首先会从所有特征中随机选择部分特征，再从该部分特征中选择一个最优划分特征。这样进一步增强了模型的泛化能力。
2.在决定部分特征个数时，通过交叉验证的方式来获取一个合适的值。

**随机森林算法流程**

1.从样本集中有放回随机采样选出 n 个样本。
2.从所有特征中随机选择 k 个特征，对选出的样本利用这些特征建立决策树。
3.重复以上两步 m 次，即生成 m 棵决策树，形成随机森林。
4.对于新数据，经过每棵树决策，最后投票确认分到哪一类。


#### 提升算法 Boosting

当「个体学习器」之间存在较强的依赖时，采用装袋的算法便有些不合适，此时最好的方法就是使用串行集成方式：提升（Boosting）

作用：将弱学习器提升为强学习器

方式：从初始训练集训练出一个「个体学习器」，再根据个体学习器的表现对训练样本分布进行调整，使得在个体学习器中判断错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个「个体学习器」。如此重复进行，直至个体学习器数目达到事先指定的值 T，最终将这 T 个「个体学习器」输出的值进行加权结合得到最终的输出值

##### Adaboost

利用前一轮迭代弱学习器的误差率来更新训练集的权重

自适应增强：上一个「个体学习器」中被错误分类的样本的权值会增大，正确分类的样本的权值会减小，并再次用来训练下一个基本分类器；在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器

**流程**

1.数据准备：通过数据清理和数据整理的方式得到符合规范的数据。
2.初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1 / N。
3.弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。
4.更改权重：如果某个样本点被准确地分类，降低其权值；若被分类错误，那么提高其权值。然后，权值更新过的样本集被用于训练下一个分类器。
  - 使在下轮时训练样本集更注重于难以识别的样本
5.强分类器组合：重复 3，4 步骤，直至训练结束，加大分类误差率小的弱分类器的权重（这里的权重和样本权重不一样），使其在最终的分类函数中起着较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用，最终输出结果

##### 梯度提升树 GBDT

梯度提升树（Gradient Boosting Decison Tree，GBDT）

采用前向分布算法，且弱学习器限定了只能使用CART树模型

**流程**

1.数据准备：通过数据清理和数据整理的方式得到符合规范的数据。
2.初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1 / N
3.弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。
4.CART 树拟合：计算每一个子样本的梯度值，通过梯度值和子样本拟合一棵 CART 树
5.更新强学习器：在拟合好的 CART 树中通过损失函数计算出最佳的拟合值，更新先前组成的强学习器。
6.强分类器组合：重复 3，4，5 步骤，直至训练结束，得到一个强分类器，最终输出结果。

### 模型选择

同时使用多种模型，并且使用 k 折数据法，进行快速验证选取哪一个模型


## 无监督学习

面对无标签数据常常使用的一类机器学习方法
常用于聚类，其他：降维、图分析、关联规则分析

数据聚类：把一堆数据按照它们特征的相似度分为多个子类（向量分布？）
- 聚类后可用于自动打标签


### 聚类

通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）

**划分过程**：首先由用户确定划分子集的个数 𝑘，然后随机选定 𝑘 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 𝑘 个子集，即将数据划分为 𝑘 类。

**评估标准就是**：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。

#### 方式

##### K-Means

**步骤**
- 随机初始化 K 个（代表拟聚类簇个数）中心点
- 每一个样本按照距离自身最近的中心点进行聚类，等效于通过两中心点连线的中垂线划分区域
- 移动中心点到个簇的质心位置，并将此质心作为新的中心点
- 反复迭代，直至中心点的变化满足收敛条件（变化很小或几乎不变化）


**SSE**

类似于回归算法通过减小目标函数（如：损失函数）的值拟合数据集一样，聚类算法通常也是优化一个目标函数，从而提高聚类的质量


常常使用误差的平方和 SSE（Sum of squared errors）作为度量聚类效果的标准，当 SSE 越小表示聚类效果越好


![17149131450481714913144012.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17149131450481714913144012.png)


**k值选取**

- 肘部法则：计算多个k值，看sse变化，畸变程度最大的点称之为「肘部」，可以认为是最佳 k 值

**缺点**

随着数据量的增长，分类数目增多时，会出现一个较大子集有多个中心点，而其他多个较小子集公用一个中心点的问题。即算法陷入局部最优解而不是达到全局最优解的问题

原因：一部分中心点在初始化时离的太近

解决方案：
- 在同一数据集上运行多次 K-Means 算法聚类，选取 SSE 最小的那次作为最终的聚类结果
	- 数据集较大时，比较耗时
- k-means++

##### K-Means++

在初始化中心点上做了改进

**步骤**

- 在数据集中随机选择一个样本点作为第一个初始化的聚类中心。
- 计算样本中的非中心点与最近中心点之间的距离 𝐷(𝑥) 并保存于一个数组里，将数组中的这些距离加起来得到 𝑠𝑢𝑚(𝐷(𝑥)) 。
- 取一个落在 𝑠𝑢𝑚(𝐷(𝑥)) 范围中的随机值 𝑅 ，重复计算 𝑅=𝑅−𝐷(𝑥) 直至得到 𝑅≤0 ，选取此时的点作为下一个中心点。
- 重复 2,3 步骤，直到 𝐾 个聚类中心都被确定。
- 对 𝐾 个初始化的聚类中心，利用 K-Means 算法计算最终的聚类中心

R的作用：直接选取距离最远的点作为初始点的方法，会容易受到数据集中离群点的干扰，随机值 𝑅 的方法避免数据集中所包含的离群点对算法思想中要选择相距最远的中心点的目标干扰

**总结**
无法完全避免随机选择中心点带来的不稳定性，所以偶尔也会得到不太好的结果。但是 K-Means++ 算法得到不太好的聚类的概率远小于 K-Means 算法


##### Mini-Batch K-Means

为了减少计算量而开发的算法

在每一次迭代过程中，从数据集中随机抽取一部分数据形成小批量数据集，用该部分数据集进行距离计算和中心点的更新

### 层次聚类

k means 问题：需要手动指定 K 值，需要多次测试找到最优 K 值

#### 自底向上层次聚类法（Agglomerative Clustering）

**步骤/原理**

对于数据集 D，D = (x1,x2,.....xn) ：
1.将数据集中每个样本标记为 1 类，即 D 初始时包含的类别（Class）为 C = (c1,c2....cn) 。
2.计算并找出 C 中距离最近的 2 个类别，合并为 1 类。
3.依次合并直到最后仅剩下一个列表，即建立起一颗完整的层次树。




**距离计算**
- 单连接：根据两种类别之间最近的元素间距离作为两类别之间的距离
- 全连接：根据两种类别之间最远的元素间距离作为两类别之间的距离
- 平均连接：依次计算两种类别之间两两元素间距离，并最终求得平均值作为两类别之间的距离。
- 中心连接：平均连接虽然看起来更加合理，但是两两元素间的距离计算量往往非常庞大。有时候，也可以使用中心连接计算方法。即先计算类别中心，再以中心连线作为两类别之间的距离

「单连接」和「全连接」都相对极端，容易受到噪声点和分布不均匀数据造成的干扰



#### 自顶向下层次聚类法


比自底向上复杂

自顶向下层次聚类法在实施过程中常常遇到一个问题，那就是如果两个样本在上一步聚类中被划分成不同的类别，那么即使这两个点距离非常近，后面也不会被放到一类中。
在实际应用中，自顶向下层次聚类法没有自底而上的层次聚类法常用


**利用 k means 进行分割**
1.把数据集 D 归为单个类别 C 作为顶层。
2.使用 K-Means 算法把 C 划分成 2 个子类别，构成子层；
3.递归使用 K-Means 算法，分别对划分的子类，继续进行 2 类 划分子层直到每个点都是单独分类或特定条件结束

**利用平均距离进行分割**
1.把数据集 D 归为单个类别 C 作为顶层。
2.从类别 C 中取出点 d，使得 d 满足到 C 中其他点的平均距离最远，构成类别 N。
3.继续从类别 C 中取出点 d1， 使得 d1 满足到 C 中其他点的平均距离与到 N 中点的平均距离之间的差值最大，并将点放入 N。
4.重复步骤 3，直到差值为负数。(直到点 d1 远离 N 类，接近 C 类)
5.再从子类中重复步骤 2，3，4 直到全部点单独成类，即完成分割。


##### BIRCH 聚类算法

Balanced Iterative Reducing and Clustering using Hierarchies，直译过来就是「使用层次方法的平衡迭代规约和聚类」，该算法由时任 IBM 工程师 Tian Zhang 于 1996 年发明。
优点：效率高，可用于大型数据集的快速聚类
原理：基于训练样本建立了 CF 聚类特征树。CF 聚类特征树对应的输出就是若干个 CF 节点，每个节点里的样本点就是一个聚类的类别

BIRCH 算法在建立 CF 特征树时只存储原始数据的特征信息，并不需要存储原始数据信息，内存开销上更优，计算高效。
BIRCH 算法只需要遍历一遍原始数据，而 Agglomerative 算法在每次迭代都需要遍历一遍数据，再次突出 BIRCH 的高效性。
BIRCH 属于在线学习算法，并支持对流数据的聚类，开始聚类时并不需要知道所有的数据。

CF 聚类特征：对特征数据进行运算并且以元组的形式记录，定义类别（簇）的信息，并有效地对数据进行压缩
CF = (N,LS,SS)
N: 表示该 CF 中拥有的样本点的数量；  
LS: 表示该 CF 中拥有的样本点各特征维度的和向量；  所有点各维度间各自相加（结果是向量）
SS: 表示该 CF 中拥有的样本点各特征维度的平方和；  所有点维度值平方总和（结果是值）
CF 拥有可进行加法运算

CF Tree 聚类特征树
枝平衡因子 β、叶平衡因子 λ 和空间阈值 t
非叶节点包含多个子节点，不少于 β 个


### 密度聚类

通过评估样本的紧密程度来划分对应的类别，理论上可以找出任何形状的聚类并有效避免噪声的干扰,也无需像划分聚类那样提前声明需要聚集的类别数量

层次聚类和划分聚类共有的缺点：无法很好地接近非凸（non-convex）数据的聚类问题

#### DBSCAN 密度聚类算法

density-based spatial clustering of applications with noise,具有噪声的基于密度的聚类方法,由 Martin Ester 等在 1996 年提出

可以在有噪音的数据中发现形状与大小各异的类别

**步骤**

 一般假定样本的类别可以通过样本分布的紧密程度决定，于是先发现密度较高的点，然后把相近的高密度点逐步连成一片，进而找到不同的类别（簇）

1.首先，DBSCAN 会以每个数据点为圆心，以 eps（σ-邻域） 为半径画圆。
2.然后，DBSCAN 会计算相应圆中有多少个其他点，并以该数目作为圆心数据点的密度值。
3.接下来，我们需要确定密度阈值 MinPts，并分别将小于或大于该密度阈值的数据点（包含自己）称作低密度或高密度点（核心点）。
4.如果，此时有一个高密度的点在另一个高密度的点的圆圈范围内，我们就把这两个点串联起来。
5.之后，如果有低密度的点也在高密度的点的圆圈范围内，也将其连接到最近的高密度点上，并称之为边界点。

最后，所有能连到一起的数据点就形成了一个聚类类别

**概念**

- σ-邻域：N(xj) = distance(xi,xj) <= σ-邻域
	- 点两两间的距离小于距离 σ 的，划分为子样本集的数量
- 核心对象：如果某个样本集包含了 >= MinPts 个样本，则该样本集被称为核心对象。
- 密度直达：xi 位于 xj 的邻域中,xj 是核心对象，则 xi 由 xj 密度直达
- 密度可达：密度直达具有传播性，例如有样本 p1、p2......pn,分别 pn+1 由 pn 密度直达，那 p1 和 pn 成为密度可达
- 密度相连：由核心对象密度直达的任意两个点，它们的关系成为密度相连

**对比**

缺点：参数敏感性，eps 敏感

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240520000415.png)

#### HDBSCAN 密度聚类算法

由 [Campello, Moulavi, and Sander](http://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14) 三人于 2013 年提出,Density-Based Clustering Based on Hierarchical Density Estimates

基于层次密度估计的密度聚类方法

解决 DBSCAN 参数敏感的缺点

**核心**

1. 生成原始聚簇。 此阶段和 DBSCAN 算法的第一阶段相似，确定核心点，生成原始聚簇并有效地识别噪声点。
    
2. 合并原始聚簇。 在此阶段使用层次聚类的思想对原始聚簇进行合并，降低了聚类结果对输入参数的敏感性。由于该方法不需要对每个对象进行测试和判断，同时也降低了时间复杂度。


**实现**

涉及到图论


核心距离 𝑐𝑜𝑟𝑒𝑘(𝑥) : 当前点到其第 k 近的点直接的距离，一般用欧式距离。

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240520001401.png)


互达距离 𝑑mreach−𝑘(𝑎,𝑏) : 从核心点到相邻核心点直接的距离，一般用欧式距离。

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240520001450.png)


**应用**

- 筛选合适聚类的点
- 反向操作：筛选异常点

### 谱聚类（Spectral Clustering）  
  
于 2006 年由 Ulrike von Luxburg 公布在论文 A Tutorial on Spectral Clustering 上。  

很多时候会被用于图像分割

1. 适用于各类形状的数据分布聚类。
2. 计算速度较快，尤其是在大数据集上明显优于其他算法。
3. 避免了 K-Means 会将少数离群点划为一类的现象

缺点：对参数是比较敏感的
- 最后使用的 K-Means 聚类，所以要提前指定聚类数量
- 使用 K-近邻生成邻接矩阵时还需要指定最近邻样本数量
- 不适合于大规模数据（计算矩阵的特征值和特征向量会非常耗时）

#### 概念
  
**无向图**: 把平面/空间中的数据点通过直线连接起来的图形就是无向图  
  
**拉普拉斯矩阵（Laplacian Matrix）**

也称为基尔霍夫矩阵，是无向图的一种矩阵表示形式  
表达式：L(n x n) = D - A  
D 为图的度矩阵，A 为图的邻接矩阵  
  
**度矩阵**  

- 无向图中，顶点 vi 的度 d(vi) = N(i)𝑑(𝑣𝑖) = 𝑁(𝑖)（即与顶点相连的边的数目）
- 有向图中，顶点 vi 的度分为顶点 vi 的出度和入度，即从顶点vi出去的有向边的数量和进入顶点vi的有向边的数量。

对于有连接的点 i、j，有权重  w(i,j) > 0  
对于无连接的点 i、j，w(i,j) = 0  
对于任意一点 i, 它的度 d(i) = Σw(i,j),即它关联的所有（权重 / 点的数量？）的和  

度矩阵是一个对角矩阵，主角线上的值由点的度组成，多少个点就有多少个对角线点  

对角矩阵：只有主对角线有值，其他位置为 0  


**邻接矩阵**

邻接矩阵表示顶点间关系，任意两点之间的权重值 𝑤(𝑖,𝑗) 组成的矩阵

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521001111.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521001338.png)


构造邻接矩阵的三种方式
- σ-邻近法
	- 设置一个阈值 𝜖，再求解任意两点 𝑥𝑖 和 𝑥𝑗 间的欧式距离 𝑠(𝑖,𝑗) 来度量相似性。然后，根据 𝑠(𝑖,𝑗) 和 𝜖 的大小关系
- 全连接法：通过选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和 Sigmoid 核函数
	- 也是通过距离得到 w(i,j)，但是获得距离的函数是其他函数
- K-近邻法


#### 无向图切图

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521002034.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521003211.png)


对于数据，可以切分为没有交集的 n 个集合

对于集合 A 、B 他们之间的切图权重为 W(A,B) = ∑w(i,j)

扩展到 k 个子图集合有：
cut(A1,A2....Ak) = (1/2) \* ∑W(Ai,\_A)  \_A 是 Ai 的补集
- W(Ai,\_A)  表示 Ai 和 任一集合的权重
- ∑W(Ai,\_A) 就是 Ai 和剩下其他所有集合的权重和
- 计算时，其实会计算 W(A,B) 又会计算W(B,A)，所以最终要除 2

cut(A1,A2....Ak) 的值是子图间的权重和，即不同子图之间的连接程度
- 值很高，意味着有很多边或权重较大的边连接不同的子图。这样的情况表明这些子图之间联系紧密，即相似性较高
- 值很低，意味着只有少量的边或权重较小的边连接不同的子图。这样的情况表明这些子图之间联系较弱，即相似性较低

所以：值越小则代表子图的差异性越大
> 极端场景下，少量或者一个点被划分为一个子图，此时会影响数据的有效性

- RatioCut：在切图的定义下增加 节点个数作为分母，稀释大小
- Ncut：Ratiocut 的分母 |𝐴𝑖| 换成 𝑎𝑠𝑠𝑜𝑐(𝐴𝑖)，以数据点的权重和作为分母，从根源上进行正则
	- 一般来说 Ncut 切图优于 RatioCut 切图


#### 步骤


1. 根据数据构造无向图 𝐺，图中的每一个节点对应一个数据点，将相似的点连接起来，并且边的权重用于表示数据之间的相似度。
2. 计算图的邻接矩阵 𝐴 和度矩阵 𝐷，并求解对应的拉普拉斯矩阵 𝐿。
3. 求出 𝐿 的前 𝑘 个由小到大排列的特征值{𝜆}𝑖=1𝑘以及对应的特征向量{𝑣}𝑖=1𝑘。
4. 把 𝑘 个特征向量排列在一起组成一个 𝑁×𝑘 的矩阵，将其中每一行看作 𝑘 维空间中的一个向量，并使用 K-Means 算法进行聚类，并得到最终的聚类类别。

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521001925.png)


较小特征值及其特征向量提供了有关图的低频信息，反映了图的整体结构或大尺度结构。这些信息在图聚类中尤其重要，因为它们可以揭示图中社区或簇的结构

###  亲和传播聚类

Affinity Propagation，基于数据点进行消息传递的理念设计的，不需要提前确定聚类的数量，即 K 值。但由于 Affinity Propagation 运行效率较低，不太适合于大数据集聚类


### Mean Shift

均值漂移聚类，找出最密集的区域，同样也是一个迭代过程。
在聚类过程中，首先算出初始中心点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件。

Mean Shift 在图像分割，视频跟踪等领域也有较好的应用


### 聚类方式选取

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521010458.png)


### 关联规则

一种在大型数据库中发现变量之间的有趣关系的方法

#### 概念

- 项集：集合S 属于项Item的集合  
- k-项集：包含 k 个项的项集  
- 集合和项集，根据划分层次，会有不一样的划分  
	- 例如：  
		- 购物车可以理解为项的集合I，其中某几个物品形成的集合则为项集  
		- 如果超市一天的账单为集合I，则单个购物车的下单内容可以作为项集    
- 频繁项集：表明一个项集出现很频繁  

对于X、Y不相交的两个项集，X -> Y，通过支持度和置信度可以衡量关联强度  
- 支持度：项集X、Y在集合I中出现的概率，可以设定一定支持阈值，可以认为超过的项集为频繁项集，即“有用”  
- 置信度：在X出现的情况下，Y出现的可能性  


提升度、杠杆率和确信度
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240523005205.png)

#### 任务

- **发现频繁项集**：发现满足最小支持度阈值的所有项集，也就是所说的「频繁项集」。
- **计算关联规则**：从发现的频繁项集中提取所有高置信度的规则，作为关联规则挖掘结果。

#### 疑问
只有样本足够大，才会比较准确，同时项集怎么指定也是问题    
实操：一般计算时怎么划分项集？遍历一遍？  
通过判断大集合可以省略小集合的运算？



### Apriori 算法

一般通过格结构来枚举可能的项集  
一般n项数据可以产生2^n -1的候选项集

于 1994 年在论文 [Fast algorithms for mining association rules in large databases](https://dl.acm.org/citation.cfm?id=672836) 中首次出现，它也是关联规则挖掘中最常用的方法

- **定理 1**：如果一个项集是频繁项集，那么其所有的子集也一定是频繁项集。
- **定理 2**：如果一个项集是非频繁项集，那么其所有的超集也一定是非频繁项集。





#### 步骤

首先，令 𝐹𝑘 表示频繁 𝑘 项集的集合，𝐶𝑘 表示候选 𝑘 项集的集合：

- 算法一次扫描整个数据集，计算每个项的支持度，并得到频繁 1 项集的集合 𝐹1。
    
- 然后，算法使用频繁 𝑘−1 项集产生候选 𝑘 项集，可以使用 𝐹𝑘−1×𝐹𝑘−1 或 𝐹𝑘−1×𝐹1 方法。
    
- 算法再次扫描数据集对候选项集完成支持度计数，并使用子集函数确定在 𝐶𝑘 中的全部候选 𝑘 项集。
    
- 计算候选项集的支持度，并删去支持度小于预先设定阈值的候选项集。
    
- 当没有频繁项集产生，即 𝐹𝑘=∅ 时，算法结束。
    

产生完频繁项集，就可以从给定的频繁项集中提取关联规则。一般情况下，每个频繁 𝑘 项集能够产生 2𝑘−2 个关联规则。产生出来的规则需要基于置信度剪枝，以满足提前设定的置信度阈值。

这里，我们又要给出一条关于置信度的定理。假设项集 𝑌 可以被划分为两个非空子集 𝑋 和 𝑌−𝑋，那么：

- 如果规则 𝑋→𝑌−𝑋 不满足置信度阈值，则规则 𝑋′→𝑌−𝑋′ 一定不满足置信度阈值。其中 𝑋′ 是 𝑋 子集。
    

Apriori 算法使用逐层方法来产生关联规则，其中每层对应于规则后件中的项数。初始时，提取规则后半部分只含一个项的所有高置信度规则；然后，使用这些规则来产生新的候选规则。


### 时间序列数据建模分析

年份转换为序号，从而引入回归分析的手段得到未来的预测结果

  

时间序列数据特点:

- 时间序列数据依赖于时间，但不一定是时间的严格函数。

- 时间序列数据每时刻上的值具有一定的随机性，不可能完全准确地用历史值去预测。

- 时间序列数据前后时刻（但不一定是相邻时刻）的数值往往具有相关性。

- 时间序列往往会呈现出某种趋势性或出现周期性变化的现象。


  

数据类型

- 按研究对象分类：一元时间序列和多元时间序列。

- 按时间参数分类：离散时间序列和连续时间序列。

- 按统计特性分类：平稳时间序列和非平稳时间序列。

- 按分布规律分类：高斯型时间序列和非高斯型时间序列。

  

#### 描述性时序分析

确定型时序分析，通过直观的数据比较或绘图观测，寻找序列中蕴含的发展规律
缺点：需要数据呈现出一定的规律性

#### 统计时序分析

利用数理统计学相关的原理和方法来分析时间序列

- 频域分析：假设任何一种无趋势的时间序列都可以分解成若干不同频率的周期波动。
- 早期的频域分析方法借助傅里叶分析从频率的角度揭示时间序列的规律。后来，其借助了傅里叶变换，用正弦、余弦项之和来逼近某个函数。再到极大熵谱估计理论的引入，频域分析进入了现代谱分析阶段。由于谱分析依赖于强数学背景且不利于直观解释，导致该方法具有很大的局限性
- 时域分析：时域分析方法应用要广泛很多。原理主要是参照事件发展过程中的惯性，从而通过惯性用统计来描述就是时间序列值之间存在的相关关系，而这种相关关系通常具有某种统计规律。时域分析的目的是，通过寻找出时间序列值之间相关关系的统计规律，并拟合出适当的数学模型来描述这种规律，进而利用这个拟合模型预测序列未来的走势
- 时域分析方法的产生最早可以追溯到 1927 年出现的自回归 AR 模型。不久之后，英国数学家、天文学家 Walker 爵士在分析印度大气规律时使用了移动平均 MA 模型和自回归移动平均 ARMA 模型。这些模型奠定了时间序列时域分析方法的基础
- ARMA 模型也被得以广泛应用。再到后来，美国统计学家 Box 和英国统计学家 Jenkins 系统地阐述了对求和自回归移动平均 ARIMA 模型的识别、估计、检验及预测的原理及方法。这些知识现在被称为经典时间序列分析方法，是时域分析方法的核心内容。
- ARMA 模型通常被用于平稳时间序列分析过程
- ARIMA 模型则广泛应用于非平稳序列随机分析过程

  
**平稳时间序列检验**

从概率统计的角度来定义平稳时间序列。一般来讲，有两种定义
- 严平稳时间序列和宽平稳时间序列：严平稳要求序列所有的统计性质都不会随着时间的推移而发生变化
- 宽平稳：认为只要保证序列 二阶矩 平稳，就代表序列稳定

严平稳比宽平稳的条件严格。严平稳是对序列联合分布的要求，以保证序列所有的统计特征都相同。


关于序列平稳性的检验，一般有两种方法：
- 图检验
- 假设检验

  
图检验是根据时序图和自相关图显示的特征作出判断，因其操作简便而运用广泛。简单来讲，如果一张时序图呈现出明显的增长和下降趋势，那么就一定不平稳。

**自相关图**

用于确认数据是否平稳曲线

自相关（Autocorrelation），又称之为序列相关，是一个统计学上的概念。
相关性其实就是变量之间的关系强度

在时间序列中，当我们使用以前的时间步长来计算时间序列观测的相关性时。由于时间序列的相关性与之前的相同系列的值进行了计算，就被称之为自相关。

自相关函数 ACF 用来度量时间序列中延迟为 k 时，相距 k 个时间间隔（延迟期）的序列值之间的相关性，所形成的图称之为自相关图。


**区分平稳序列**

平稳序列通常具有短期相关性。该性质用自相关系数来描述就是随着延迟 𝑘 的增加，平稳序列的自相关系数会很快地衰减向零

- 图中显示的序列自相关系数一直较小，在 0 附近震荡波动


**纯随机性检验**


纯随机性检验：判断一个平稳序列是否随机呢

序列平稳，那么就可以应用 ARMA 等成熟的建模方法完成分析
如果随机性太强，纯随机序列是没有任何分析价值的


原理：一般会涉及到两个统计量
- Q 统计量
- LB 统计量（Ljung-Box）
	- LB 统计量是 Q 统计量的修正，所以业界通常所称的 Q 统计量也就是 LB 统计量

 LB 统计量的 P 值小于 `0.05`，我们则认为该序列为非随机序列，否则就为随机序列

#### ARMA 介绍及建模

全称是自回归移动平均模型，它是目前最常用的拟合平稳序列的模型
可以被细分为 
- AR 自回归模型
- MA 移动平均模型
- ARMA

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240527191503.png)


ARMA 模型一般记作：𝐴𝑅𝑀𝐴(𝑝,𝑞)，即为 𝑝 阶 AR 和 𝑞 阶 MA 模型的组合。Python 中，我们可以利用 `statsmodels` 统计计算库中的 `tsa.ARMA` 类完成 ARMA 建模和预测

计算时，需要确定：p 、q 的取值，有以下三种方法
- AIC（Akaike Information Criterion ）
- BIC（Bayesian Information Criterion ） 
- HQIC（Hannan-Quinn Criterion ）

ARMA 建模步骤：获取序列 → 通过平稳性检验 → 通过纯随机性检验 → 估计 𝑝 和 𝑞 参数 → ARMA 建模 → 模型评估

对于不平稳的序列，如果能通过一些方法处理成平稳就可以用 ARMA 进行推导
> 例如：差分

#### ARIMA 介绍及建模

适合于对非平稳序列进行建模分析，I 就代表差分，多了一个参数，那就是使非平稳序列成为平稳序列所做的差分阶数 𝑑。所以，ARIMA 模型通常记作：𝐴𝑅𝐼𝑀𝐴(𝑝,𝑑,𝑞)。


#### 时间序列数据分析流程

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240527192750.png)

## 机器学习工程：模型部署和推理

### 自动化机器学习

Automated machine learning，AutoML，可以看作为一种基于人工智能的解决方案，以应对不断增长的机器学习应用场景的需要

较早的想法见于 ACM 论文 [Auto-WEKA](https://dl.acm.org/citation.cfm?id=2487629)

将机器学习应用于现实问题的端到端流程自动化的过程

原因：典型机器学习方法门槛太高
>在典型的机器学习流程中，开发者必须学会数据预处理，特征工程，特征提取和特征选择方法，使数据集适合机器学习。在这些数据预处理步骤之后，开发者必须选择合适的算法，并完成超参数及优化方法的选择

作用：开发者只需要提供数据，例如不同类别的图片。接下来，算法的选择，算法的训练，参数的调优，模型的部署等一系列过程都可以交给 AutoML 组件来完成

#### 方向

- 自动化特征工程：Automated Feature Enginnering，简称 Auto FE
	- 包含特征选择，特征提取，元学习，以及检测和处理不均衡数据或缺失数据等操作
	- [特征选择](https://en.wikipedia.org/wiki/Feature_selection)（英语：Feature selection）也被称为变量选择、属性选择或变量子集选择
		- 为了构建模型而选择相关特征子集的过程
		- 可以被看作是数据降低维度的步骤，初始的资料集合被降到更容易管理的族群（特征）以便于学习，同时保持描述原始资料集的精准性与完整性。这其中包含像主成分分析、独立成分分析等方法
	- [元学习](https://en.wikipedia.org/wiki/Meta_learning_(computer_science))主要是解决学习如何学习的问题
		- 构建一个用于学习经验的子系统，模仿人类学习的方式
		- 相关论文： [An introduction to Meta-learning](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a)  [研究论文](https://github.com/floodsung/Meta-Learning-Papers)
- 自动化模型选择：Automated Model Selection，简称 AMS
	- 依据数据特征来选择最为合适的机器学习算法模型
	- 传统的机器学习中，模型的选择一般由机器学习专家根据经验，以及交叉验证的结果来对比决定
- 超参数自动优化：Hyperparameter Optimization，简称 HPO
	- 论文：[基于贝叶斯优化 Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)，[基于进化算法 Evolutionary Algorithms](https://en.wikipedia.org/wiki/Evolutionary_algorithm)，[基于 Lipschitz Functions](https://arxiv.org/pdf/1703.02628.pdf)，[基于本地搜索 Local Search](https://arxiv.org/pdf/1401.3492.pdf)，[基于随机搜索 Random Search](https://en.wikipedia.org/wiki/Random_search)，[基于粒子群优化算法 Particle Swarm Optimization](http://www.sciencedirect.com/science/article/pii/S0957417407003752)，[基于元学习 Meta Learning](http://openproceedings.org/2019/conf/edbt/EDBT19_paper_235.pdf)，[基于迁移学习 Transfer Learning](https://pdfs.semanticscholar.org/75f2/6734972ebaffc6b43d45abd3048ef75f15a5.pdf)
- 神经结构搜索： Neural Architecture Search，简称 NAS
	- 搭建人工神经网络
	- 传统方法中，我们大量应用随机搜索 Random Search 或者网格搜索 Grid Search 方法来进行调参，但这些参数的变动会需要大量的算力需求并且效率极低
	- 论文：[基于进化算法 Evolutionary Algorithms](https://arxiv.org/abs/1902.06827)，[基于元学习 Meta Learning](https://arxiv.org/pdf/1606.01885.pdf)，[基于迁移学习 Transfer Learning](https://arxiv.org/abs/1707.07012)，[基于本地搜索 Local Search](https://arxiv.org/pdf/1711.04528.pdf)，[基于强化学习 Reinforcement Learning](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf)，[基于 Network Morphism](https://arxiv.org/abs/1806.10282)，[基于 Continuous Optimization](https://arxiv.org/abs/1806.09055)


#### 学习路径

- 开源框架：例如 Auto-Keras，auto-sklearn 等开源工具本地完成。
- 商业服务：例如 Google Cloud，Microsoft Azure 等云服务商工具在云端完成

开源框架的自定义程度高，方便集成，但需要较强的本地算力支持。商业服务对本地环境要求不高，有完善的开发文档和技术支持
[开源框架和商业服务列表](https://github.com/hibayesian/awesome-automl-papers#projects)

## 数据处理手段

### 差分运算

一种从序列中提取确定性信息的方法，也是一种非常基础的数学分析手段

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240527192120.png)


p 阶 k 步差分，看不同次数下看数据稳定性改善效果

一般在差分时阶数不宜过大。原因在于差分其实是对信息提取加工的过程，每次差分都会带来信息损失，过度差分会导致有效信息损失而降低精度。一般情况下，线性变化通过 1 次差分即可平稳，非线性趋势 2，3 次差分也能变得平稳，一般差分次数不超过 2 次


### 正则化方法

- 增加总数作为分母


### min-max 归一化

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240523001412.png)


```python
moons = (moons - np.min(moons)) / (np.max(moons) - np.min(moons))
# 不管维度，拍平数据，找出最大、最小值
```

### 零值化处理

去除均值对变换的影响，减去均值后数据的信息量没有变化，即数据的区分度（方差）是不变的

### PCA 主成分分析

降低数据的维数，通过保留数据集中的主要成分来简化数据集
数学原理：通过对协方差矩阵进行特征分解，从而得出主成分（特征向量）与对应的权值（特征值）。然后剔除那些较小特征值（较小权值）对应的特征，从而达到降低数据维数的目的。

作用
1.方便将数据用于低维空间可视化。聚类过程中的可视化是很有必要的。
2.高维度数据集往往就意味着计算资源的大量消耗。通过对数据进行降维，我们就能在不较大影响结果的同时，减少模型学习时间。

一般情况下，我们不会拿到数据就进行 PCA 处理，只有当算法不尽如人意、训练时间太长、需要可视化等情形才考虑使用 PCA。其主要原因是，PCA 被看作是对数据的有损压缩，会造成数据集原始特征丢失。

#### 计算

**向量基**
基（basis）也称为基底，其是用于描述、刻画向量空间的基本工具。向量空间的基是它的一个特殊的子集，基的元素称为基向量。向量空间中任意一个元素，都可以唯一地表示成基向量的线性组合，使用基底可以便利地描述向量空间
可认为是坐标系的单位向量(并且模长为 1？)


**投影降维**

通过向新基进行映射（投影） ==> （新基矩阵） * （向量） = （向量映射）
通过增加、减少基，即可实现降维、升维


**基的寻找的理论**

要紧降维、升维，首先要选取合适的基

方差：是用来度量单个随机变量的离散程度。也就是说，最大方差给出了数据最重要的信息。
一个维度中的方差，可以看作是该维度中每个元素与其均值的差平方和的均值，公式如下：

降维：尽量留得更多，最重要的信息

总结：找到一个基，通过这个基映射原来的二维数据点，得到一个方差最大的结果，从而完成降维的目的

**寻找基的方法**

如果按方差最大一个基一个基的寻找，每次都寻找最合适的基，最终合并进行降维，那么可能会出现局部最优（其中有些基是重合的）

特点：基之间应该是没有相关性的
在二维空间中两个向量垂直，则线性无关，那么在高维空间中，向量之间相互正交，则线性无关

协方差：一般用来刻画两个随机变量的相似程度
基于方差公式，改为两个变量
方差 == 特殊情况的协方差：从原来衡量两个相同变量的误差，到现在变成了衡量两个不同变量的误差
两个变量之间的协方差等于 0 即可以选择到无关的基

协方差矩阵
为了解决协方差只能处理两个变量的问题，引入协方差矩阵
由向量自身的方差与向量之间的协方差构成。这个定义延伸到更多向量时同样受用。
选择一组基，使得原始向量通过该基映射到新的维度，同时需要满足使得该向量的方差最大，向量之间的协方差为 0 ，也就是矩阵对角化。


通过计算所有的特征的协方差矩阵，看哪些特征的特征值值比较大，即为权重比较大的特征



### 独热编码

One-Hot编码，又称为一位有效编码，采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效
  - 性别特征：["男","女"] 男 => 10  女 => 01
  - 祖国特征：["中国"，"美国，"法国"] 中国 => 100  美国 => 010  法国 => 001

### 规范化

目的：将特征数据的分布调整成标准正太分布，也叫高斯分布，即使得数据的均值维0，方差为1

具体步骤
![17138004047041713800404465.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138004047041713800404465.png)

- 统一尺度
- 避免数值不稳定和提高精度
- 改善算法收敛速度：更便于计算


# 数学

y = wT * x 
> w 转成向量表示是列向量，x也是列向量， wT * x 指的不是普通乘法，而是内积
> wT 则是行向量
> 列项列和行向量的内积（矩阵内积） = 数值
> y1 = w1 * x1
> y2 = w2 * x2
> 矩阵表示 y = [w1,w2] * [x1,x2]T