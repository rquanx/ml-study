机器学习 Machine Learning 是人工智能的一个分支，其核心构成为机器学习算法，并通过从数据中获取经验来改善自身的性能。

[动手实战人工智能 AI By Doing](https://aibydoing.com/intro)

# 概念

**人工智能**：最先出现，并且涵盖最广的
**机器学习**：为了实现人工智能的手段
**深度学习**：机器学习下的一种方式

## 机器学习

大致包含四大类
- 监督学习
- 无监督学习
- 半监督学习
- 强化学习

## 监督学习

监督学习是基于示例输入-输出数据对，在输入和输出数据之间建立数学函数的机器学习任务，而该数学函数来源于对有标签训练数据集的学习过程。
- 输入：训练数据集中的特征变量
- 输出：标签
- 数学函数：机器学习预测模型

监督学习的特点是：训练数据集有标签

应用：
- 分类：动物的种类判断、植物的种类判断，表现为标签
- 回归：股票价格预测，房价预测，洪水水位线预测，表现为数值

### 线性回归

通过找到一条直线去拟合数据点的分布趋势的过程，就是线性回归的过程

找到最适合的那一条直线，是线性回归中需要解决的目标问题

#### 一元线性回归

表达式：y = w0 + w1 * x
```py
def f(x: list, w0: float, w1: float):
    """一元一次函数表达式"""
    y = w0 + w1 * x
    return y
```

#### 平方损失函数

**残差（损失）**

真实值和预测值之间的偏离程度

每个数据点的损失：
![17124282270991712428226182.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282270991712428226182.png)


损失总和：
![17124282611011712428261010.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282611011712428261010.png)

一般使用残差的平方和来表示所有样本点的误差，即平方损失函数
![17124282840981712428283141.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282840981712428283141.png)

为什么要用平方和：能保证损失始终是累加的正数，而不会存在正负残差抵消的问题

使平方和的值最小，就能得到拟合的w0、w1

```py
def square_loss(x: np.ndarray, y: np.ndarray, w0: float, w1: float):
    """平方损失函数"""
    loss = sum(np.square(y - (w0 + w1 * x)))
    return loss
```


#### 最小二乘法代数求解

用于求解线性回归拟合参数的一种常用方法
- 二乘: 代表平方
- 平方: 平方损失函数
- 最小二乘: 最小平方，使平方损失函数最小

为了求得参数，分别对未知数进行求导：
![17124286380981712428637687.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124286380981712428637687.png)

再令导数值为0
![17124287260981712428725502.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124287260981712428725502.png)

```py
def least_squares_algebraic(x: np.ndarray, y: np.ndarray):
    """最小二乘法代数求解"""
    n = x.shape[0]
    w1 = (n * sum(x * y) - sum(x) * sum(y)) / (n * sum(x * x) - sum(x) * sum(x))
    w0 = (sum(x * x) * sum(y) - sum(x) * sum(x * y)) / (
        n * sum(x * x) - sum(x) * sum(x)
    )
    return w0, w1
```

**为什么要令导数为0**

要使平方损失函数得到最小值，由于平方损失函数是二次函数，且 >= 0,即是开口向上的抛物线，此时导数为0处即为极小值

**为什么要偏导**

二元函数求极值（二元函数是三维的图像）
多元函数的极值求法

多元函数的极值点（想象在山峰最高或者山谷最低），此时在临界点处不管往哪个方向都会是导数为0（高度不会再变化），所以偏导值为0，是多元函数极值的一个条件
对于一元线性回归，一阶偏导为0，即为最低点


#### 最小二乘法矩阵求解

一元线性函数表示为矩阵
![17125138433131712513842390.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17125138433131712513842390.png)

y = XW
- X 是输入数据的矩阵（经过补充矩阵维度）[1,x1]
- W 是参数 w0、w1的矩阵 [w0,w1]T

[1,x1] * [w0,w1]T = w0 + w1 * x

以矩阵形式表示损失函数
![17125139543111712513953369.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17125139543111712513953369.png)
- y 为实际值的集合（数组）
- XW = yi 为拟合的值
- XW 是 10 行 1 列矩阵
- (y - XW)T： 10 行 1 列矩阵转置，为 1行，10列
- (y - XW)T * (y - XW)：自己的转置 * 自己 即为求自己的平方和
  - 1 x 10 矩阵 * 10 * 1 矩阵 = 1 * 1即为总和

通过分配率，进一步计算
![17126027113111712602711252.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126027113111712602711252.png)
![17126027856401712602784724.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126027856401712602784724.png)
- yT * XW： 1 行 10 列矩阵 * 10 行 1 列矩阵 = 一个数
- (XW)T * y: 同上 === 一个数，且之与 yT * XW 一样
- 所以合并

对矩阵进行求偏导（一次性对所有参数进行求导）
![17126052286381712605228529.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126052286381712605228529.png)
- yT * y 是一个常数对于 W 的偏导数为0
- −2(XW)Ty 的偏导数是 −2XT * y 因为 (XW) T * y 相对于 W 的导数是 XT * y
- (XW)T * XW: 设 XW = Q => QT * Q,应用链式法则......，很复杂

![17126052786371712605277830.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126052786371712605277830.png)

```py
def least_squares_matrix(x: np.matrix, y: np.matrix):
    """最小二乘法矩阵求解"""
    w = (x.T * x).I * x.T * y
    return w
```
#### 训练

训练一个机器学习预测模型时，我们通常会将数据集划分为 70% 和 30%
- 70% 的部分被称之为训练集，用于模型训练： 用于从训练集中找到最佳拟合参数的值
- 30% 被称为测试集：对比预测的目标值与真实目标值之间的差异，评估模型的预测性能

- 提取出训练数据集（作为变量x的数据）：训练特征
- 提取出数据结果（期望输出的数据）：训练目标
- 提取出测试数据集
- 提取出测试数据结果
- 模型训练（选择模型、数据处理、传入） ==> 得到拟合参数
- 输入测试集进行预测校验
  - 平均绝对误差（MAE）
  - 均方误差（MSE）

预测结果不对的原因
- 数据集是否精挑细选
- 模型选择是否合理：对于一些复杂的数据，无法通过线性方程来拟合

##### 平均绝对误差（MAE）

绝对误差的平均值：测试集预期结果 - 预测结果的差的绝对值的总和平均值

![17126864706391712686469843.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126864706391712686469843.png)

```py
def mae_solver(y_true: np.ndarray, y_pred: np.ndarray):
    """MAE 求解"""
    n = len(y_true)
    mae = sum(np.abs(y_true - y_pred)) / n
    return mae
```

##### 均方误差（MSE）

表示误差的平方的期望值：测试集预期结果 - 预测结果的差的绝对值的平方总和平均值，平方后数值会更明显？

![17126866576371712686657585.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126866576371712686657585.png)

```py
def mse_solver(y_true: np.ndarray, y_pred: np.ndarray):
    """mse 求解"""
    n = len(y_true)
    mse = sum(np.square(y_true - y_pred)) / n
    return mse
```

##### 平均绝对百分比误差 MAPE

MAPE 是一个百分比值，比其他统计量更容易理解,如果 MAPE 为 5，则表示预测结果较真实结果平均偏离 5%

![17128502516381712850250700.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128502516381712850250700.png)

```py
def mape_slover(y_true: np.ndarray, y_pred: np.ndarray):
    """mape 求解"""
    n = len(y_true)
    return np.sum(np.abs((y_true - y_pred) / y_true)) / n * 100 
```

### 多项式回归（非线性回归）

多项式：多个未知数的
多元：多次方


#### 多项式回归相当于线性回归的特殊形式

对于: w0 + w1 * x + w2 * x^2,进行替代 x = x1, x^2 = x2
等式就变成： w0 + w1 * x1 + w2 * x2，即多元线性

一元高次多项式  -> 多元一次多项式

一般变量数据都是有多个，所以 x1 可以是列向量，x1、x2 可以构成特征矩阵来计算
![17128556746381712855673846.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128556746381712855673846.png)

根据least_squares_matrix，X代入构造的特征矩阵进行计算

**计算过程**

一元线性函数可表示为：![17128543938941712854393865.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128543938941712854393865.png)
- y = XW

一元二次 => 二元线性回归：![17128545686381712854568382.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128545686381712854568382.png)
- 也让 y = XW
- X = [1,x1,x2] => [1,x,x^2]
- W = [w0,w1,w2]T

接上线性矩阵计算......

#### 一元高阶多项式

![17128518155691712851815547.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128518155691712851815547.png)


#### 练习

一个题目，进行线性回归、二次多项式回归......

不知道选择哪一种模型，可以直接跑多次，看误差数据变化（mae、mse......）,当误差图像稳定，取第一个开始稳定的点，就是相对好的点（防止过拟合）

### 其他

#### 普通最小二乘法的局限性

二范数：欧几里得范数或L2范数,向量的平方和再开根
|X|2 = sqrt(x1^2 + x2^2......)
![17130834936921713083493655.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130834936921713083493655.png)

参数解：
![17130837166371713083716258.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130837166371713083716258.png)

参数解成立的条件就是 |XT * X|不能为 0。而当变量之间的相关性较强（多重共线性），或者 m 大于 n 时，式中的 X 不是满秩（rank(A) ≠ dim(x)）矩阵。从而使得 |XT * X| 的结果趋近于 0，造成拟合参数的数值不稳定性增加，这也就是普通最小二乘法的局限。
> 满秩: 矩阵秩等于行数，称为行满秩,矩阵秩等于列数，称为列满秩,既是行满秩又是列满秩则为n阶矩阵即n阶方阵
> k阶子式：从矩阵中任取k行k列，即为矩阵的k阶子式（从矩阵中拆除方阵）
> 矩阵求秩：经过初等变化后（最简矩阵），能从矩阵中拆出来的最大k阶子式(子式不为0)，那么秩就是K
> 初等变换：行交换、列交换、某一行 * k（k ≠ 0）、某一列 * k、某一行加到另一行上、某一列加到另一列上（分别是行变换、列变换各3种）
> 最简矩阵：一般指的是行最简形矩阵
>   最简行矩阵：非零行的第一个非零元素全是1，且非零行的第一个元素1所在的列的其余元素全是0的矩阵（每一行的第一个数是1，且所在的列其余为0）

![17130852817571713085281730.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130852817571713085281730.png)
> |XT * X| 是矩阵的行列式值
> |XT * X| 趋近于0： 意味着矩阵 X 的列向量之间存在极高的线性相关性
>   - 奇异矩阵：没有解
>   - 多重共性：当模型的解释变量高度相关时，即一个变量可以通过其他变量的线性组合来近似表达，对于输入数据中的小波动或噪声非常敏感，从而造成最小二乘法求解的系数会不稳定
> 行列式(determinant): 一个数字，表示| X |
>   - 对于二阶矩阵，所围平面的面积、对于三阶矩阵，所围成的六面体的体积、对于n维矩阵，它就是n维立体的体积


**无法处理的场景**
 
数据集的列（特征）数量 > 数据量（行数量），即 X 不是列满秩。

数据集列（特征）数据之间存在较强的线性相关性，即模型容易出现过拟合。

##### 希尔伯特矩阵 OLS 线性拟合

希尔伯特矩阵是一种系数都是单位分数的方块矩阵: H(i,j) = 1 / (i + j - 1)
![17130857436371713085743607.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130857436371713085743607.png)

希尔伯特矩阵每列数据之间存在较强的线性相关性,满足 XT * X 趋近于 0，可用来验证局限性

##### 皮尔逊相关系数（Pearson Correlation Coefficient）

通常用于度量两个变量 x 和 y 之间的线性相关程度，其值介于 -1 与 1 之间。其中，数值越趋近于 1 表示正相关程度越高，趋近于 0 表示线性相关度越低，趋近于 -1 则表示负相关程度越高

##### 岭回归（Ridge Regression）

为了解决普通最小二乘法局限的方法，改良的最小二乘法

通过向损失函数中添加 L2 正则项（2-范数）有效防止模型出现过拟合

损失函数：
![17131036466371713103645733.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036466371713103645733.png)

向量表示：
![17131036666371713103666108.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036666371713103666108.png)

参数解：
![17131036976371713103697332.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036976371713103697332.png)
> 通过给 XT * X 增加一个单位矩阵，从而使得矩阵变成满秩，完善普通最小二乘法的不足



##### LASSO

LASSO 回归同样是通过添加正则项来改进普通最小二乘法，不过这里添加的是 L1 正则项

![17131052146381713105214376.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131052146381713105214376.png)

##### 正则项

正则化（Regularization）是一种常用的技术，它通过在目标函数中增加一个 惩罚项 来控制模型的复杂度，从而防止过拟合问题的出现

正则化项通常添加在模型的损失函数（目标函数）中

方式：
- L1正则化(LASSO)：系数向量w的L1范数，即绝对值之和
  - 使部分系数变为0，从而实现特征选择和降维
- L2正则化(岭回归)：系数向量w的L2范数的平方，即平方和开根号
  - 使系数向量w的每个分量都尽可能小，从而防止过拟合问题的出现

正则化系数 λ 可以通过交叉验证等方法来确定，通常取值范围为 0到1 之间的实数，数值越大，正则化项的惩罚力度越强，模型越倾向于选择较小的系数

### 回归模型评价与检验

在一元线性回归模型中，一般需要：拟合优度检验、变量的显著性检验及参数的区间估计
> 一元线性回归中的 T 检验和 F 检验一致

多元线性回归模型中有：拟合优度检验、变量的显著性检验（T 检验）、方程的显著性检验（F 检验）及参数的置信区间

#### 可解释和不可解释

在回归分析中，可解释和不可解释部分指的是数据变异性（即变化）的两个来源：

可解释部分（Explained Variation）：这是由回归模型捕捉的变异性。它反映了因变量（响应变量）的变化中可以通过模型中的自变量（解释变量）来解释的部分。
在线性回归中，可解释部分通常通过计算拟合值和因变量平均值的差来得到。
这些差的平方和就是总变异中可以被模型解释的部分。它显示了模型如何有效地利用自变量来预测因变量。

不可解释部分（Unexplained Variation）：这是回归模型无法捕捉的变异性，通常被认为是随机误差或噪声。
这些变异性可能是由数据中未观测到的因素、测量误差、或模型不完全贴合真实关系等原因造成的。
在线性回归中，不可解释部分由残差表示，即实际观测值与模型预测值之间的差异。

总变异（Total Variation）是可解释和不可解释部分的总和。在确定模型的好坏时，我们希望可解释部分尽可能大，不可解释部分尽可能小。一般而言，如果模型具有高度的解释能力，可解释变异会占总变异的较大比例，这通常通过 R^2 统计量（决定系数）来衡量，R^2 值越接近1，表示模型的解释能力越强。

#### 一元线性回归的拟合优度检验

一般会使用判定系数 R^2 作为度量拟合优度的指标

离差：y(数据) - y(数据平均值)

![17131070566361713107056275.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131070566361713107056275.png)
> 往离差中 - 拟合值 + 拟合值 得到, 再分别拆分为e(i) 和 Y(拟合)
>   - e(i)：实际观测值与样本回归拟合值之差，也就是「残差」
>       - e(i) 为0时即拟合值等于观察值，此时拟合最佳，并且离差与残差无关，误差来自于回归线本身数据，所以不能解释
>   - Y(拟合)：样本回归拟合值与观测均值之差

![17131086431091713108643080.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131086431091713108643080.png)
- TSS：离差平方,总体平方和(Total Sum of Squares)
- ESS: Y(拟合)平方,回归平方和(Explained Sum of Squares)
- RSS: e(i)平方,残差平方和(Residual Sum of Squares)

![17131087216371713108721198.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131087216371713108721198.png)
> [TSS 和 ESS、RSS的关系 TSS = ESS + RSS](https://aibydoing.com/notebooks/chapter01-08-lab-evaluation-and-validation-of-regression-models)

当 TSS 不变，实际观测点离样本回归拟合线越近，则 ESS 在 TSS 中占的比重越大。
因此，我们定义拟合优度等于回归平方和 ESS 与 y 的总离差 TSS 的比值。
![17131091926401713109192021.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131091926401713109192021.png)
当 RSS 越小时，R^2 就越趋近于 1，那么代表模型的解释力越强
- 一般有效值在 0 ~ 1 间，值越大，模型拟合度越好
- scikit-learn 提供的 API 有可能计算出 R^2 值为负数的情况,这时候 TSS = RSS + ESS 会失效，具体 R^2 值取多少需要视情况而定，不同问题不一样


#### 变量显著性检验

回归分析中判断解释变量 x 是否是被解释变量 y 的一个显著性的影响因素,利用了数学的假设检验知识

假设检验的原理：通过事先对总体参数或总体分布形式作出假设。然后，利用样本信息来判断原假设是否合理。也就是说，通过判断样本信息与原假设是否有显著差异，从而决定是否接受或否定原假设

细节见代码


### 逻辑回归

Logistic Regression,逻辑斯蒂回归,是一种分类方法

#### 线性可分和不可分

二维平面内，如果只使用一条直接就可以将样本分开，则称为线性可分，否则为线性不可分
三维空间则是一个平面去分

#### 线性回归分类

通过拟合一条直线去预测更多的连续值

二分类问题：只有两种类别，也可以称之为：0 - 1 分类问题

#### Sigmoid 分布函数

![17131914946371713191493684.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131914946371713191493684.png)

```py
def sigmoid(z):
    sigmoid = 1 / (1 + np.exp(-z))
    return sigmoid
```

图像呈现出完美的 S 型（Sigmoid 的含义）。它的取值仅介于 0 和 1 之间，且关于 x = 0 轴中心对称。同时当 x 越大时，y 越接近于 1，而 x 越小时，y 越接近于 0

- 输出范围：Sigmoid函数将任意实数映射到(0,1)区间内，非常适合表示概率。在二分类问题中，可以将这个值解释为属于某一类的概率。
- 形状特性：Sigmoid函数形状为"S"型，当输入接近0时，输出变化敏感，而输入值很大或很小的时候，输出趋于平稳，这使得它在区分两个类别时表现良好。
- 梯度特性：在学习过程中，Sigmoid函数的导数（梯度）表达简单，便于在算法中使用梯度下降法进行优化

数学定义: 如果一组连续随机变量符合 Sigmoid 函数样本分布，就称作为逻辑分布。

结合数学定义和特性：把线性函数拟合的结果使用 Sigmoid 函数压缩到 (0,1) 之间。如果线性函数的 y  值越大，也就代表概率越接近于 1，反之接近于 0

![17131920702691713192070243.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131920702691713192070243.png)

对于二分类，值要么是 0 要么是 1，所以概率分布可以这样设定
![17131922076371713192207398.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131922076371713192207398.png)


似然函数：在统计学中用来表示一个模型参数值下观察到的数据概率。
总概率：当多个事件独立发生时，这些事件同时发生的总概率是每个事件发生概率的乘积
> 在逻辑回归中，每个样本产生观察结果的概率是独立的，所以通过所有样本的概率乘积来构造似然函数
将似然函数转化为对数似然函数，从累乘转为累加
![17131923926391713192392307.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131923926391713192392307.png)

#### 对数损失函数

对数似然函数衡量了事件发生的总概率
根据最大似然估计原理，只需要通过对 L(w) 求最大值，即得到 w 的估计值。而在机器学习问题中，我们需要一个损失函数，并通过求其最小值来进行参数优化
> 最大似然估计（MLE）原理基于这样的想法：给定观测数据，在所有可能的参数值中，最有可能产生这些数据的参数值是最优的。
> 实际上，MLE是寻找一组参数，使得观测数据出现的概率（即似然）最大。如果参数值使得已知数据的似然最大，那么在统计意义上，这些参数就是最符合数据的。因此，最大化似然函数可以视为是在寻找最能“解释”观测数据的参数值，这些值通常被认为是这个模型在给定数据下的最优参数估计

除以m，获得平均损失，乘以 -1，将求最大值，转化为求最小值
![17131929726401713192972563.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131929726401713192972563.png)

```py
def loss(h, y):
    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    return loss
```

**为什么不用线性回归的平方损失函数**

我们要求损失最小的最优模型，只有凸函数能求得全局最小值，非凸函数一般只能得到局部最优解

#### 梯度下降法

梯度下降法用处：求取极小值
梯度：一个向量，表示某一函数在该点处的方向的导数沿着该方向取得最大值，即沿着该方向变化最快，变化率最大
- 对于一元函数：是某点的导数
- 对于多元函数：是某点的偏导组成的向量

梯度下降：即沿着下降方向（梯度的反方向）去寻找损失函数的极小值

对对数损失函数进行求导
![17131935956361713193594680.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131935956361713193594680.png)
![17131936226381713193621952.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131936226381713193621952.png)
![17131936386371713193638429.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131936386371713193638429.png)

将导数 * 常量a，即得到了每次梯度下降的步长，a 通常成为 学习率

每次权重的变化为 w = w - a * 导数

```py
def gradient(X, h, y):
    gradient = np.dot(X.T, (h - y)) / y.shape[0]
    return gradient
```

线性回归方法之所以使用普通最小二乘法来求解，是因为我们可以很方便地求出损失函数的最小值。但是，机器学习中的很多问题，往往会面对非常复杂的损失函数，这些损失函数一般无法直接求得最小值，只能使用迭代方法来求取局部或全局极小值

### K 近邻算法

在解决分类问题的过程中，K 近邻算法（简称：KNN）是一种简单而且实用的方法

#### 最近邻算法

最近邻算法（Nearest Neighbor，简称：NN）：针对未知类别数据 x，在训练集中找到与 x 最相似的训练样本 ，用 y 的样本对应的类别作为未知类别数据 x 的类别，从而达到分类的效果

![17132816910641713281691013.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132816910641713281691013.png)

#### K 近邻算法

K 近邻（K-Nearest Neighbors，简称：KNN）算法是最近邻（NN）算法的一个推广
> NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好

![17132818178801713281817150.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132818178801713281817150.png)

##### 步骤

1.数据准备：通过数据清洗，数据处理，将每条数据整理成向量。
2.计算距离：计算测试数据与训练数据之间的距离。
3.寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。
4.决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。

![17132818678781713281867547.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132818678781713281867547.png)

##### 距离度量

在计算两个样本间的相似度时，可以通过计算样本之间特征值的距离进行表示。

常用的两个距离：曼哈顿距离 和 欧式距离

![17132820608781713282060575.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132820608781713282060575.png)
> 直线距离和轴线距离？

**曼哈顿距离**

又称马氏距离，是计算距离最简单的方式之一

![17132821208781713282120695.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132821208781713282120695.png)
> 将两个数据 X 和 Y 中每一个对应特征值之间差值的绝对值，再求和，便得到曼哈顿距离。

```py
def d_man(x, y):
    d = np.sum(np.abs(x - y))
    return d
```

**欧式距离**

源自 N 维欧氏空间中两点之间的距离公式

![17132822468781713282246297.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132822468781713282246297.png)
> 将两个数据 X 和 Y 中的每一个对应特征值之间差值的平方，再求和，最后开平方

```py
def d_euc(x, y):
    d = np.sqrt(np.sum(np.square(x - y)))
    return d
```

##### 决策规则

根据数据特征对决策规则进行选取,从而通过 K 个邻居来判断测试样本的最终类别

- 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。
- 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。

##### KNN实现

```py
def d_euc(x, y):
    d = np.sqrt(np.sum(np.square(x - y)))
    return d
  
def majority_voting(class_count):
    # 多数表决函数
    sorted_class_count = sorted(
        class_count.items(), key=operator.itemgetter(1), reverse=True
    )
    return sorted_class_count

def knn_classify(test_data, train_data, labels, k):
    # KNN 方法完整实现
    distances = np.array([])  # 创建一个空的数组用于存放距离

    for each_data in train_data:  # 使用欧式距离计算数据相似度
        d = d_euc(test_data, each_data)
        distances = np.append(distances, d)

    sorted_distance_index = distances.argsort()  # 获取按距离从小到大排序后的索引
    sorted_distance = np.sort(distances)
    r = (sorted_distance[k] + sorted_distance[k - 1]) / 2  # 计算

    class_count = {}
    for i in range(k):  # 多数表决
        vote_label = labels[sorted_distance_index[i]]
        class_count[vote_label] = class_count.get(vote_label, 0) + 1

    final_label = majority_voting(class_count)
    return final_label, r
```

knn算法中，取的k值不同会极大的影响分类结果

#### KD 树算法

如果目前所在点比目前最佳点更靠近输入点，则将其变为目前最佳点。
检查另一边子树有没有更近的点，如果有则从该节点往下找

### K 近邻回归

- 数据准备：通过数据清洗，数据处理，将每条数据整理成向量。

- 计算距离：计算测试数据与训练数据之间的距离。

- 寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。

- 决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。


**「决策分类」是决定未知样本类别的关键步骤**

- 分类问题：根据 K 个邻居的类别，多数表决得到未知样本的类别。

- 回归问题：根据 K 个邻居的目标值，计算平均值得到未知样本的预测值。

###  朴素贝叶斯

朴素贝叶斯是以概率论作为基础的算法
- 实现简单
- 分类效率很高

#### 前置知识

##### 条件概率

![17136023711471713602371072.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136023711471713602371072.png)

条件概率就是指事件 A 在另外一个事件 B 已经发生条件下的概率

P(AB) = P(A) * P(B) // 同时发生A 和 B 的概率
P(AB) / P(B) = P(A|B) // 已发生b后a的发生概率

##### 贝叶斯定理

![17136026412091713602640971.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136026412091713602640971.png)

P(A|B) = P(AB) / P(B)
P(B|A) = P(AB) / P(A)

##### 先验概率

根据以往经验和分析得到的概率，例如抛硬币，根据经验就是0.5

##### 后验概率

事件发生后求的反向条件概率；即基于先验概率通过贝叶斯公式求得的反向条件概率,即反推

#### 朴素贝叶斯原理

将贝叶斯原理以及条件独立结合而成的算法

![17136028122111713602811839.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136028122111713602811839.png)

P(类别|特征) = P(特征|类别) * P(类别) / P(特征)


朴素贝叶斯中的「朴素」，即条件独立：将各个特征割裂开，认定特征之间相互独立
> 现实是很多时候特征间是有互相联系的，所以朴素贝叶斯会牺牲一定的准确性


##### 实现

![17136053539311713605353909.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136053539311713605353909.png)

需要利用极大似然估计进行估计先验概率

极大似然估计：通过若干次实验，观察其结果，利用实验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。


##### 贝叶斯估计

极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 0 的情况，影响后验概率的计算结果，贝叶斯估计可以解决

![17136076672461713607667114.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136076672461713607667114.png)

在极大似然估计增加一个 λ 因子，等价于在随机变量各个取值的频数上赋予一个正数，λ 为 0 时即为元素的极大似然估计，为 1 时称为拉普拉斯平滑

#### 朴素贝叶斯的三种常见模型

- 多项式模型：当特征值为离散时，常常使用多项式模型
- 伯努利模型：适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 1 和 0（以文本分类为例，某个单词在文档中出现过，则其特征值为 1，否则为 0）
- 高斯模型：当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多先验概率为0，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的
![17137134075591713713407509.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17137134075591713713407509.png)


### 分类模型评估

- 准确率
- 查准率
- 召回率
- F1 值
- ROC 曲线

#### 准确率

预测正确的结果占的比重

在二分类问题中，我们常常会定义正类和负类，就可以给出实际类别（行名）和预测类别（列名）的混淆矩阵
将结果拆分成 TP 、TN 、FP 、FN 四种情况
所以有准确率公式： (TP + TN) / (TP + FP + TN + FN)
- TP：预期正确，并且预测为正类
- TN：预测正确，并且预测为负
- FP: 失败，预测为正类（实际为负）
- FN: 失败，预测为负（实际为正）

![17138006977071713800697307.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138006977071713800697307.png)

#### 查准率

又称精确率，即正确分类的正例个数占分类为正例的实例个数的比例

预测了 n 个数据为正，那么准确的正占据所有正的多少

P = TP / (TP + FP)

你认为的该类样本，有多少猜对了（猜的准确率如何）

#### 召回率 Recall

查全率，即正确分类的正例个数占实际正例个数的比例

预测了 n 个数据为正，占据所有正的数据的多少

R = TP / (TP + FN)

该类样本有多少被找出来（召回了多少）

#### F1 值

查准率和召回率的加权平均数，精确率和召回率的综合评价指标，对衡量数据更有利

F1 = 2 * P * R / (P + R)

![17138012117111713801211356.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138012117111713801211356.png)

#### ROC 曲线

部分分类模型中（如：逻辑回归），通常会设定一个阈值，并规定大于该阈值为正类，小于则为负类。所以，当我们减小阀值时，将会有更多的样本被划分到正类。这样会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类

ROC 曲线的目的在用形象化该变化过程，从而评价一个分类器好坏

![17138013287101713801328301.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138013287101713801328301.png)

- 当 FPR=0，TPR=0 时，意味着将每一个实例都预测为负例。
- 当 FPR=1，TPR=1 时，意味着将每一个实例都预测为正例。
- 当 FPR=0，TPR=1 时，意味着为最优分类器点。

优秀分类器对应的 ROC 曲线应该尽量靠近左上角。当曲线越接近于 45 度对角线，则分类器效果越差

**AUC**

全称为 Area Under Curve,曲线下面积，ROC 曲线下面积
作用：将ROC的曲线视觉好坏，转成数值

- AUC = 1：完美分类器。
- 0.5 < AUC < 1：分类器优于随机猜测。
- AUC = 0.5：分类器和随机猜测的结果接近。
- AUC < 0.5：分类器比随机猜测的结果还差。

### 支持向量机

SVM（Support Vector Machine）

在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，不仅可以应用于线性分布数据，还可以用于非线性分布数据
相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法

#### 线性SVM

1.先找到数据分类分割线
2.然后在分割线两侧在设立两个平行的虚线，两条虚线与分割线的距离一致，这个距离成为“间隔”
3.使得分割线和虚线之间的间隔最大化，即两个虚线间隔最大（不进行错分的情况下）
4.最终实现正类虚线外的就是正类，负类虚线外的是负类
5.正好位于两条虚线上方的样本点就被我们称为“支持向量”，这也就是支持向量机的名字来源

支持向量机的目标：找到最大的分类间隔所对应的分割线

##### 函数间隔

对于线性SVM,决策函数定义为：f(x) = wT * x + b
假设分类结果为 1 和 -1，即结果 y = 1 / -1
那么就有：y * f(x) > 0,h = y * f(x) 就是函数间隔的定义
此时对于每个点 h，到分割平面都会有一个距离，
我们定义 h 取最小值时就是函数间隔，即平面与某个点的间隔值 h(min)

从公式可知，当 (w,b) 平面等比例放大时，f(x) 值会放大，但是平面并没有变，但是函数间隔也跟着变大了，所以函数间隔其实是一个动态值，单纯的函数间隔意义不大

##### 几何间隔



定义一个点 x ，其到超平面上的垂直映射点为 x0 ，两点之间的距离为 h，w 表示超平面的法向量

![17138034117261713803411045.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138034117261713803411045.png)

![17138034227261713803421966.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138034227261713803421966.png)

平面上点到线的距离，拓展到高维即为点到平面的距离，公式: d = (Ax + by + c) / (A^2 + B^2)开根
> 将上述公式转化成向量来写：d = (wT * x + b) / ||w||
> w = [A,B]T 是直线的法向量，x = [x,y]T 是点的坐标向量， b = c,wT * x 是向量的点积，||w|| 是向量 w 的范数即 (A^2 + B^2)开根
> 可见 几何间隔公式其实等价于 点到面的距离公式

公式：函数间隔除以 ||w|| 得到的就是几何间隔，而函数间隔的本质其实可以理解为 |f(x)| 

##### 拉格朗日对偶性

二次规划被用来找到几何间隔最优的决策边界

![17138888645531713888864504.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138888645531713888864504.png)

将从最大值转为求最小值，同时变为二次函数，此时可利用二次规划来求解

在满足 条件 的情况下，拉格朗日对偶性通过给每一个约束条件加上一个拉格朗日乘子 λ，并将约束条件融合到目标函数中

#### 非线性SVM

对于线性不可分的数据集，可以通过支持向量机去完成分类
> 需要增加一个技巧把线性不可分数据转换为线性可分数据之后，再完成分类
> 此技巧称为：核技巧
> 实现数据转换的函数称之为「核函数」

本质：将低维数据映射到高维空间中，使得数据集在高维空间能被线性可分

##### 核函数

核函数有很多种，但大多需要大量计算，目前有总结出几种计算量较小的常用核函数

![17138901103071713890109520.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138901103071713890109520.png)

- 线性核函数: k(xi,xj) = xi * xj
- 多项式核函数：k(xi,xj) = (xi * xj)^d, d > 1
- 高斯径向基：......
- sigmoid: ......

分别对应：linear, scikit-learn 中 SVC 方法的 poly, rbf, sigmoid

核函数也可以进行线性组合使用


#### 多分类支持向量机

支持向量机最初是为二分类问题设计的，当我们面对多分类问题时，其实同样可以使用支持向量机解决。而解决的方法就是通过组合多个二分类器来实现多分类器的构造

- 一对多法：即训练时依次把某个类别的样本归为一类，剩余的样本归为另一类，这样 k 个类别的样本就构造出了 k 个支持向量机。
- 一对一法：即在任意两类样本之间构造一个支持向量机，因此 k 个类别的样本就需要设计 k(k - 1) / 2 个支持向量机

#### 总结

SVM 是一种表现非常不错的方法，尤其是对于非线性分类问题。而且最大的劣势在于计算效率，随着数据集的增大，计算时间陡增。所以，一般我们会在小数据集下应用 SVM，而大数据集基本不予考虑

### 决策树

决策树是一种特殊的树形结构，一般由节点和有向边组成。
- 节点表示特征、属性或者一个类
- 有向边包含判断条件

决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树

例如：
![17139732085771713973208527.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139732085771713973208527.png)

#### 算法原理

- 数据准备 → 通过数据清洗和数据处理，将数据整理为没有缺省值的向量。
- 寻找最佳特征 → 遍历每个特征的每一种划分方式，找到最好的划分特征。
- 生成分支 → 划分成两个或多个节点。
- 生成决策树 → 对分裂后的节点分别继续执行 2-3 步，直到每个节点只有一种类别。
- 决策分类 → 根据训练决策树模型，将预测数据进行分类。

#### 信息增益（ID3）

**信息熵**

度量样本纯度最常用的一种指标

公式：![17139733548731713973354170.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139733548731713973354170.png)
- D表示样本集合
- p(k) 表示第 k 类样本所占的比例
- Ent(D) 越小则 D 的纯度越高

计算示例：![17139735548751713973554575.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139735548751713973554575.png)
- 所有结果概率 * log 的和

**信息增益**

是建立在信息熵的基础上

公式：![17139737308761713973730789.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139737308761713973730789.png)

信息增益越大，使用特征划分出来的集合纯度越高。

![17139739108791713973910831.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139739108791713973910831.png)

#### 信息增益率（C4.5）

信息增益的不足：当信息增益作为标准时，易偏向于取值较多的特征，为了避免这种偏好给预测结果带来的不好影响，可以使用增益率来选择最优划分

![17139740368971713974036871.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139740368971713974036871.png)

#### 连续值处理

对于离散值可以每个值作为一个分类，但是当数据是连续值时，这样分类就太多了，采用二分法对连续值进行处理

对于连续的属性 X 假设共出现了 n 个不同的取值，将这些取值从小到大排序 { x1, x2, x3 ......}，其中找一点作为划分点 t ，则将数据划分为两类，大于 t 的为一类，小于 t 的为另一类。而 t 的取值通常为相邻两点的平均数 t = (xi + xi+1) / 2

![17139742428821713974242782.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139742428821713974242782.png)

#### 预剪枝和后剪枝

决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多

对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝

##### 预剪枝

预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点

##### 后剪枝

在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。

#### 实现

1.数据清洗：
- 选取合适的特征（有效，少量减少计算量）
- 对数据进行简单分类：对于数值，可以简化分为几类
- 对数据特征进行替换：分类 -> 0、1、2、3
2.划分训练、测试集
3.训练模型


### CART 决策树

分类与回归树（classification and regression tree, CART）同样也是应用广泛的决策树学习算法，CART 算法是按照特征划分，由树的生成和树的剪枝构成，既可以进行分类又可以用于回归，按照作用将其分为决策树和回归树

![17139759908951713975990600.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139759908951713975990600.png)

### 超参数选择方法

#### 网格搜索

预先制定好各参数的有限个候选取值，然后通过排列组合的方式来传入这些参数，最终通过 K 折交叉验证的方法来确定表现最好的参数
- K 折交叉验证：是交叉验证中的一种常见方法，其通过将数据集均分成 K 个子集，并依次将其中的 K-1 个子集作为训练集，剩下的 1 个子集用作测试集。在 K 折交叉验证的过程中，每个子集均会被验证一次

**步骤**

- 首先将数据集均分为 K 个子集。
- 依次选取其中的 K-1 个子集作为训练集，剩下的 1 个子集用作测试集进行实验。
- 计算每次验证结果的平均值作为最终结果。

K 折交叉验证让每一条数据都有均等的几率被用于训练和验证，在一定程度上能提升模型的泛化能力

#### 随机搜索

网格搜索很直观，也很方便，但是最大的问题在于随着候选参数增多，搜索需要的时间迅速增加

随机搜索，顾名思义就是经验 + 运气的碰撞。我们依据经验制定一个参数范围，然后在范围内随机选取参数测试，并返回最佳结果
> 本质上还是类似的思路和方式，只是定了区间后，随机选取数据进行尝试

### 装袋和提升集成学习方法

每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，由于数据的不确定性，单独应用个别分类器可能会出现分类准确率低的问题。为了应对这样的情况，集成学习被提出，其可以利用多个弱分类器结合的方式提高分类准确率

#### 集成学习

通过构建多个分类器并综合使用来完成学习任务，同时也被称为多分类器系统。其最大的特点就是结合各个弱分类器的长处

集成分类
- 同质集成：在一个集成学习中，「个体学习器」是同一类型，如 「决策树集成」 所有个体学习器都为决策树
- 异质集成：在一个集成学习中，「个体学习器」为不同类型，如一个集成学习中可以包含决策树模型也可以包含支持向量机模型

集成方式：
- 并行式，当个体学习器之间不存在强依赖关系时，可同时生成并行化方法，其中代表算法为装袋（Bagging）算法。
- 串行式，当个体学习器之间存在强依赖关系时，必须串行生成序列化方法，其中代表算法为提升（Boosting）算法

![17140605014791714060500573.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17140605014791714060500573.png)

#### 结合策略

- 平均法: 在数值型输出中，最常用的结合策略为平均法
  - 简单平均法：取每一个「个体学习器」学习后的平均值
  - 加权平均法：其中 w(i) 是每一个「个体学习器」 h(i) 的权重，通常为 w(i) > 0 
- 投票法: 对于分类输出而言，平均法效果不太好,最常用的结合策略为投票法
  - 多数投票法：即在「个体学习器」分类完成后，通过投票选出分类最多的标签作为此次分类的结果。
  - 加权投票法：同加权平均法类似，w(i) 是每一个「个体学习器」 h(i) 的权重，通常为 w(i) > 0
- 学习法：平均法和投票法比较简单，但是可能学习误差较大
  - 代表方法：stacking 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，即把训练集弱学习器的学习结果作为输入，重新训练一个学习器来得到最终结果
    - 首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果

#### 装袋算法 Bagging

1.数据处理：将数据根据实际情况进行清洗整理。
2.随机采样：从样本中随机选出 m 个样本作为一个子样本集。有放回的重复 T 次，得到 T 个子样本集。
3.个体训练：设定 T 个个体学习器，将每一个子样本集放入对应个体学习器进行训练。
4.分类决策：用投票法集成进行分类决策。

##### Bagging Tre(决策树)

决策树是一个十分「完美」的训练器，但特别容易出现过拟合的情况，最终导致预测准确率低的问题

在装袋算法中，决策树常常被用作弱分类器

Bagging Tree 算法：是应用子数据集中的所有特征构建一棵完整的树，最终通过投票的方式进行预测

##### 随机森林 Random Forest

随机森林：「随机抽样 + 决策树森林」

将一个大的数据集使用自助采样法进行处理，即从原样本数据集中随机抽取多个子样本集，并基于每一个子样本集生成相应的决策树

构建出由许多小决策树组形成的决策树「森林」。最后，实验通过投票法选择决策树最多的预测结果作为最终的输出

**改进点**
1.对于普通的决策树，会在 N 个样本的所有特征中选择一个最优划分特征，但是随机森林首先会从所有特征中随机选择部分特征，再从该部分特征中选择一个最优划分特征。这样进一步增强了模型的泛化能力。
2.在决定部分特征个数时，通过交叉验证的方式来获取一个合适的值。

**随机森林算法流程**

1.从样本集中有放回随机采样选出 n 个样本。
2.从所有特征中随机选择 k 个特征，对选出的样本利用这些特征建立决策树。
3.重复以上两步 m 次，即生成 m 棵决策树，形成随机森林。
4.对于新数据，经过每棵树决策，最后投票确认分到哪一类。


#### 提升算法 Boosting

当「个体学习器」之间存在较强的依赖时，采用装袋的算法便有些不合适，此时最好的方法就是使用串行集成方式：提升（Boosting）

作用：将弱学习器提升为强学习器

方式：从初始训练集训练出一个「个体学习器」，再根据个体学习器的表现对训练样本分布进行调整，使得在个体学习器中判断错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个「个体学习器」。如此重复进行，直至个体学习器数目达到事先指定的值 T，最终将这 T 个「个体学习器」输出的值进行加权结合得到最终的输出值

##### Adaboost

利用前一轮迭代弱学习器的误差率来更新训练集的权重

自适应增强：上一个「个体学习器」中被错误分类的样本的权值会增大，正确分类的样本的权值会减小，并再次用来训练下一个基本分类器；在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器

**流程**

1.数据准备：通过数据清理和数据整理的方式得到符合规范的数据。
2.初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1 / N。
3.弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。
4.更改权重：如果某个样本点被准确地分类，降低其权值；若被分类错误，那么提高其权值。然后，权值更新过的样本集被用于训练下一个分类器。
  - 使在下轮时训练样本集更注重于难以识别的样本
5.强分类器组合：重复 3，4 步骤，直至训练结束，加大分类误差率小的弱分类器的权重（这里的权重和样本权重不一样），使其在最终的分类函数中起着较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用，最终输出结果

##### 梯度提升树 GBDT

梯度提升树（Gradient Boosting Decison Tree，GBDT）

采用前向分布算法，且弱学习器限定了只能使用CART树模型

**流程**

1.数据准备：通过数据清理和数据整理的方式得到符合规范的数据。
2.初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1 / N
3.弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。
4.CART 树拟合：计算每一个子样本的梯度值，通过梯度值和子样本拟合一棵 CART 树
5.更新强学习器：在拟合好的 CART 树中通过损失函数计算出最佳的拟合值，更新先前组成的强学习器。
6.强分类器组合：重复 3，4，5 步骤，直至训练结束，得到一个强分类器，最终输出结果。

### 模型选择

同时使用多种模型，并且使用 k 折数据法，进行快速验证选取哪一个模型

## 数据处理手段

### 独热编码

One-Hot编码，又称为一位有效编码，采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效
  - 性别特征：["男","女"] 男 => 10  女 => 01
  - 祖国特征：["中国"，"美国，"法国"] 中国 => 100  美国 => 010  法国 => 001

### 规范化

目的：将特征数据的分布调整成标准正太分布，也叫高斯分布，即使得数据的均值维0，方差为1

具体步骤
![17138004047041713800404465.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138004047041713800404465.png)

- 统一尺度
- 避免数值不稳定和提高精度
- 改善算法收敛速度：更便于计算

## 无监督学习

面对无标签数据常常使用的一类机器学习方法
常用于聚类，其他：降维、图分析、关联规则分析

数据聚类：把一堆数据按照它们特征的相似度分为多个子类（向量分布？）
- 聚类后可用于自动打标签


### 聚类

通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）

**划分过程**：首先由用户确定划分子集的个数 𝑘，然后随机选定 𝑘 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 𝑘 个子集，即将数据划分为 𝑘 类。

**评估标准就是**：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。

#### 方式

##### K-Means

**步骤**
- 随机初始化 K 个（代表拟聚类簇个数）中心点
- 每一个样本按照距离自身最近的中心点进行聚类，等效于通过两中心点连线的中垂线划分区域
- 移动中心点到个簇的质心位置，并将此质心作为新的中心点
- 反复迭代，直至中心点的变化满足收敛条件（变化很小或几乎不变化）


**SSE**

类似于回归算法通过减小目标函数（如：损失函数）的值拟合数据集一样，聚类算法通常也是优化一个目标函数，从而提高聚类的质量


常常使用误差的平方和 SSE（Sum of squared errors）作为度量聚类效果的标准，当 SSE 越小表示聚类效果越好


![17149131450481714913144012.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17149131450481714913144012.png)


**k值选取**

- 肘部法则：计算多个k值，看sse变化，畸变程度最大的点称之为「肘部」，可以认为是最佳 k 值

**缺点**

随着数据量的增长，分类数目增多时，会出现一个较大子集有多个中心点，而其他多个较小子集公用一个中心点的问题。即算法陷入局部最优解而不是达到全局最优解的问题

解决方案：
- 在同一数据集上运行多次 K-Means 算法聚类，选取 SSE 最小的那次作为最终的聚类结果
	- 数据集较大时，比较耗时
- k-means++


原因：一部分中心点在初始化时离的太近

##### K-Means++

在初始化中心点上做了改进

**步骤**

- 在数据集中随机选择一个样本点作为第一个初始化的聚类中心。
- 计算样本中的非中心点与最近中心点之间的距离 𝐷(𝑥) 并保存于一个数组里，将数组中的这些距离加起来得到 𝑠𝑢𝑚(𝐷(𝑥)) 。
- 取一个落在 𝑠𝑢𝑚(𝐷(𝑥)) 范围中的随机值 𝑅 ，重复计算 𝑅=𝑅−𝐷(𝑥) 直至得到 𝑅≤0 ，选取此时的点作为下一个中心点。
- 重复 2,3 步骤，直到 𝐾 个聚类中心都被确定。
- 对 𝐾 个初始化的聚类中心，利用 K-Means 算法计算最终的聚类中心






# 数学

y = wT * x 
> w 转成向量表示是列向量，x也是列向量， wT * x 指的不是普通乘法，而是内积
> wT 则是行向量
> 列项列和行向量的内积（矩阵内积） = 数值
> y1 = w1 * x1
> y2 = w2 * x2
> 矩阵表示 y = [w1,w2] * [x1,x2]T