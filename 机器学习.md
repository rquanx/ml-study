机器学习 Machine Learning 是人工智能的一个分支，其核心构成为机器学习算法，并通过从数据中获取经验来改善自身的性能。

[动手实战人工智能 AI By Doing](https://aibydoing.com/intro)

# 概念

**人工智能**：最先出现，并且涵盖最广的
**机器学习**：为了实现人工智能的手段
**深度学习**：机器学习下的一种方式

## 机器学习

大致包含四大类
- 监督学习
- 无监督学习
- 半监督学习
- 强化学习

## 监督学习

监督学习是基于示例输入-输出数据对，在输入和输出数据之间建立数学函数的机器学习任务，而该数学函数来源于对有标签训练数据集的学习过程。
- 输入：训练数据集中的特征变量
- 输出：标签
- 数学函数：机器学习预测模型

监督学习的特点是：训练数据集有标签

应用：
- 分类：动物的种类判断、植物的种类判断，表现为标签
- 回归：股票价格预测，房价预测，洪水水位线预测，表现为数值

### 线性回归

通过找到一条直线去拟合数据点的分布趋势的过程，就是线性回归的过程

找到最适合的那一条直线，是线性回归中需要解决的目标问题

#### 一元线性回归

表达式：y = w0 + w1 * x
```py
def f(x: list, w0: float, w1: float):
    """一元一次函数表达式"""
    y = w0 + w1 * x
    return y
```

#### 平方损失函数

**残差（损失）**

真实值和预测值之间的偏离程度

每个数据点的损失：
![17124282270991712428226182.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282270991712428226182.png)


损失总和：
![17124282611011712428261010.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282611011712428261010.png)

一般使用残差的平方和来表示所有样本点的误差，即平方损失函数
![17124282840981712428283141.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124282840981712428283141.png)

为什么要用平方和：能保证损失始终是累加的正数，而不会存在正负残差抵消的问题

使平方和的值最小，就能得到拟合的w0、w1

```py
def square_loss(x: np.ndarray, y: np.ndarray, w0: float, w1: float):
    """平方损失函数"""
    loss = sum(np.square(y - (w0 + w1 * x)))
    return loss
```


#### 最小二乘法代数求解

用于求解线性回归拟合参数的一种常用方法
- 二乘: 代表平方
- 平方: 平方损失函数
- 最小二乘: 最小平方，使平方损失函数最小

为了求得参数，分别对未知数进行求导：
![17124286380981712428637687.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124286380981712428637687.png)

再令导数值为0
![17124287260981712428725502.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17124287260981712428725502.png)

```py
def least_squares_algebraic(x: np.ndarray, y: np.ndarray):
    """最小二乘法代数求解"""
    n = x.shape[0]
    w1 = (n * sum(x * y) - sum(x) * sum(y)) / (n * sum(x * x) - sum(x) * sum(x))
    w0 = (sum(x * x) * sum(y) - sum(x) * sum(x * y)) / (
        n * sum(x * x) - sum(x) * sum(x)
    )
    return w0, w1
```

**为什么要令导数为0**

要使平方损失函数得到最小值，由于平方损失函数是二次函数，且 >= 0,即是开口向上的抛物线，此时导数为0处即为极小值

**为什么要偏导**

二元函数求极值（二元函数是三维的图像）
多元函数的极值求法

多元函数的极值点（想象在山峰最高或者山谷最低），此时在临界点处不管往哪个方向都会是导数为0（高度不会再变化），所以偏导值为0，是多元函数极值的一个条件
对于一元线性回归，一阶偏导为0，即为最低点


#### 最小二乘法矩阵求解

一元线性函数表示为矩阵
![17125138433131712513842390.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17125138433131712513842390.png)

y = XW
- X 是输入数据的矩阵（经过补充矩阵维度）[1,x1]
- W 是参数 w0、w1的矩阵 [w0,w1]T

[1,x1] * [w0,w1]T = w0 + w1 * x

以矩阵形式表示损失函数
![17125139543111712513953369.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17125139543111712513953369.png)
- y 为实际值的集合（数组）
- XW = yi 为拟合的值
- XW 是 10 行 1 列矩阵
- (y - XW)T： 10 行 1 列矩阵转置，为 1行，10列
- (y - XW)T * (y - XW)：自己的转置 * 自己 即为求自己的平方和
  - 1 x 10 矩阵 * 10 * 1 矩阵 = 1 * 1即为总和

通过分配率，进一步计算
![17126027113111712602711252.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126027113111712602711252.png)
![17126027856401712602784724.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126027856401712602784724.png)
- yT * XW： 1 行 10 列矩阵 * 10 行 1 列矩阵 = 一个数
- (XW)T * y: 同上 === 一个数，且之与 yT * XW 一样
- 所以合并

对矩阵进行求偏导（一次性对所有参数进行求导）
![17126052286381712605228529.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126052286381712605228529.png)
- yT * y 是一个常数对于 W 的偏导数为0
- −2(XW)Ty 的偏导数是 −2XT * y 因为 (XW) T * y 相对于 W 的导数是 XT * y
- (XW)T * XW: 设 XW = Q => QT * Q,应用链式法则......，很复杂

![17126052786371712605277830.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126052786371712605277830.png)

```py
def least_squares_matrix(x: np.matrix, y: np.matrix):
    """最小二乘法矩阵求解"""
    w = (x.T * x).I * x.T * y
    return w
```
#### 训练

训练一个机器学习预测模型时，我们通常会将数据集划分为 70% 和 30%
- 70% 的部分被称之为训练集，用于模型训练： 用于从训练集中找到最佳拟合参数的值
- 30% 被称为测试集：对比预测的目标值与真实目标值之间的差异，评估模型的预测性能

- 提取出训练数据集（作为变量x的数据）：训练特征
- 提取出数据结果（期望输出的数据）：训练目标
- 提取出测试数据集
- 提取出测试数据结果
- 模型训练（选择模型、数据处理、传入） ==> 得到拟合参数
- 输入测试集进行预测校验
  - 平均绝对误差（MAE）
  - 均方误差（MSE）

预测结果不对的原因
- 数据集是否精挑细选
- 模型选择是否合理：对于一些复杂的数据，无法通过线性方程来拟合

##### 平均绝对误差（MAE）

绝对误差的平均值：测试集预期结果 - 预测结果的差的绝对值的总和平均值

![17126864706391712686469843.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126864706391712686469843.png)

```py
def mae_solver(y_true: np.ndarray, y_pred: np.ndarray):
    """MAE 求解"""
    n = len(y_true)
    mae = sum(np.abs(y_true - y_pred)) / n
    return mae
```

##### 均方误差（MSE）

表示误差的平方的期望值：测试集预期结果 - 预测结果的差的绝对值的平方总和平均值，平方后数值会更明显？

![17126866576371712686657585.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17126866576371712686657585.png)

```py
def mse_solver(y_true: np.ndarray, y_pred: np.ndarray):
    """mse 求解"""
    n = len(y_true)
    mse = sum(np.square(y_true - y_pred)) / n
    return mse
```

##### 平均绝对百分比误差 MAPE

MAPE 是一个百分比值，比其他统计量更容易理解,如果 MAPE 为 5，则表示预测结果较真实结果平均偏离 5%

![17128502516381712850250700.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128502516381712850250700.png)

```py
def mape_slover(y_true: np.ndarray, y_pred: np.ndarray):
    """mape 求解"""
    n = len(y_true)
    return np.sum(np.abs((y_true - y_pred) / y_true)) / n * 100 
```

### 多项式回归（非线性回归）

多项式：多个未知数的
多元：多次方


#### 多项式回归相当于线性回归的特殊形式

对于: w0 + w1 * x + w2 * x^2,进行替代 x = x1, x^2 = x2
等式就变成： w0 + w1 * x1 + w2 * x2，即多元线性

一元高次多项式  -> 多元一次多项式

一般变量数据都是有多个，所以 x1 可以是列向量，x1、x2 可以构成特征矩阵来计算
![17128556746381712855673846.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128556746381712855673846.png)

根据least_squares_matrix，X代入构造的特征矩阵进行计算

**计算过程**

一元线性函数可表示为：![17128543938941712854393865.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128543938941712854393865.png)
- y = XW

一元二次 => 二元线性回归：![17128545686381712854568382.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128545686381712854568382.png)
- 也让 y = XW
- X = [1,x1,x2] => [1,x,x^2]
- W = [w0,w1,w2]T

接上线性矩阵计算......

#### 一元高阶多项式

![17128518155691712851815547.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17128518155691712851815547.png)


#### 练习

一个题目，进行线性回归、二次多项式回归......

不知道选择哪一种模型，可以直接跑多次，看误差数据变化（mae、mse......）,当误差图像稳定，取第一个开始稳定的点，就是相对好的点（防止过拟合）

### 其他

#### 普通最小二乘法的局限性

二范数：欧几里得范数或L2范数,向量的平方和再开根
|X|2 = sqrt(x1^2 + x2^2......)
![17130834936921713083493655.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130834936921713083493655.png)

参数解：
![17130837166371713083716258.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130837166371713083716258.png)

参数解成立的条件就是 |XT * X|不能为 0。而当变量之间的相关性较强（多重共线性），或者 m 大于 n 时，式中的 X 不是满秩（rank(A) ≠ dim(x)）矩阵。从而使得 |XT * X| 的结果趋近于 0，造成拟合参数的数值不稳定性增加，这也就是普通最小二乘法的局限。
> 满秩: 矩阵秩等于行数，称为行满秩,矩阵秩等于列数，称为列满秩,既是行满秩又是列满秩则为n阶矩阵即n阶方阵
> k阶子式：从矩阵中任取k行k列，即为矩阵的k阶子式（从矩阵中拆除方阵）
> 矩阵求秩：经过初等变化后（最简矩阵），能从矩阵中拆出来的最大k阶子式(子式不为0)，那么秩就是K
> 初等变换：行交换、列交换、某一行 * k（k ≠ 0）、某一列 * k、某一行加到另一行上、某一列加到另一列上（分别是行变换、列变换各3种）
> 最简矩阵：一般指的是行最简形矩阵
>   最简行矩阵：非零行的第一个非零元素全是1，且非零行的第一个元素1所在的列的其余元素全是0的矩阵（每一行的第一个数是1，且所在的列其余为0）

![17130852817571713085281730.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130852817571713085281730.png)
> |XT * X| 是矩阵的行列式值
> |XT * X| 趋近于0： 意味着矩阵 X 的列向量之间存在极高的线性相关性
>   - 奇异矩阵：没有解
>   - 多重共性：当模型的解释变量高度相关时，即一个变量可以通过其他变量的线性组合来近似表达，对于输入数据中的小波动或噪声非常敏感，从而造成最小二乘法求解的系数会不稳定
> 行列式(determinant): 一个数字，表示| X |
>   - 对于二阶矩阵，所围平面的面积、对于三阶矩阵，所围成的六面体的体积、对于n维矩阵，它就是n维立体的体积


**无法处理的场景**
 
数据集的列（特征）数量 > 数据量（行数量），即 X 不是列满秩。

数据集列（特征）数据之间存在较强的线性相关性，即模型容易出现过拟合。

##### 希尔伯特矩阵 OLS 线性拟合

希尔伯特矩阵是一种系数都是单位分数的方块矩阵: H(i,j) = 1 / (i + j - 1)
![17130857436371713085743607.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17130857436371713085743607.png)

希尔伯特矩阵每列数据之间存在较强的线性相关性,满足 XT * X 趋近于 0，可用来验证局限性

##### 皮尔逊相关系数（Pearson Correlation Coefficient）

通常用于度量两个变量 x 和 y 之间的线性相关程度，其值介于 -1 与 1 之间。其中，数值越趋近于 1 表示正相关程度越高，趋近于 0 表示线性相关度越低，趋近于 -1 则表示负相关程度越高

##### 岭回归（Ridge Regression）

为了解决普通最小二乘法局限的方法，改良的最小二乘法

通过向损失函数中添加 L2 正则项（2-范数）有效防止模型出现过拟合

损失函数：
![17131036466371713103645733.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036466371713103645733.png)

向量表示：
![17131036666371713103666108.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036666371713103666108.png)

参数解：
![17131036976371713103697332.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131036976371713103697332.png)
> 通过给 XT * X 增加一个单位矩阵，从而使得矩阵变成满秩，完善普通最小二乘法的不足



##### LASSO

LASSO 回归同样是通过添加正则项来改进普通最小二乘法，不过这里添加的是 L1 正则项

![17131052146381713105214376.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131052146381713105214376.png)

##### 正则项

正则化（Regularization）是一种常用的技术，它通过在目标函数中增加一个 惩罚项 来控制模型的复杂度，从而防止过拟合问题的出现

正则化项通常添加在模型的损失函数（目标函数）中

方式：
- L1正则化(LASSO)：系数向量w的L1范数，即绝对值之和
  - 使部分系数变为0，从而实现特征选择和降维
- L2正则化(岭回归)：系数向量w的L2范数的平方，即平方和开根号
  - 使系数向量w的每个分量都尽可能小，从而防止过拟合问题的出现

正则化系数 λ 可以通过交叉验证等方法来确定，通常取值范围为 0到1 之间的实数，数值越大，正则化项的惩罚力度越强，模型越倾向于选择较小的系数

### 回归模型评价与检验

在一元线性回归模型中，一般需要：拟合优度检验、变量的显著性检验及参数的区间估计
> 一元线性回归中的 T 检验和 F 检验一致

多元线性回归模型中有：拟合优度检验、变量的显著性检验（T 检验）、方程的显著性检验（F 检验）及参数的置信区间

#### 可解释和不可解释

在回归分析中，可解释和不可解释部分指的是数据变异性（即变化）的两个来源：

可解释部分（Explained Variation）：这是由回归模型捕捉的变异性。它反映了因变量（响应变量）的变化中可以通过模型中的自变量（解释变量）来解释的部分。
在线性回归中，可解释部分通常通过计算拟合值和因变量平均值的差来得到。
这些差的平方和就是总变异中可以被模型解释的部分。它显示了模型如何有效地利用自变量来预测因变量。

不可解释部分（Unexplained Variation）：这是回归模型无法捕捉的变异性，通常被认为是随机误差或噪声。
这些变异性可能是由数据中未观测到的因素、测量误差、或模型不完全贴合真实关系等原因造成的。
在线性回归中，不可解释部分由残差表示，即实际观测值与模型预测值之间的差异。

总变异（Total Variation）是可解释和不可解释部分的总和。在确定模型的好坏时，我们希望可解释部分尽可能大，不可解释部分尽可能小。一般而言，如果模型具有高度的解释能力，可解释变异会占总变异的较大比例，这通常通过 R^2 统计量（决定系数）来衡量，R^2 值越接近1，表示模型的解释能力越强。

#### 一元线性回归的拟合优度检验

一般会使用判定系数 R^2 作为度量拟合优度的指标

离差：y(数据) - y(数据平均值)

![17131070566361713107056275.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131070566361713107056275.png)
> 往离差中 - 拟合值 + 拟合值 得到, 再分别拆分为e(i) 和 Y(拟合)
>   - e(i)：实际观测值与样本回归拟合值之差，也就是「残差」
>       - e(i) 为0时即拟合值等于观察值，此时拟合最佳，并且离差与残差无关，误差来自于回归线本身数据，所以不能解释
>   - Y(拟合)：样本回归拟合值与观测均值之差

![17131086431091713108643080.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131086431091713108643080.png)
- TSS：离差平方,总体平方和(Total Sum of Squares)
- ESS: Y(拟合)平方,回归平方和(Explained Sum of Squares)
- RSS: e(i)平方,残差平方和(Residual Sum of Squares)

![17131087216371713108721198.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131087216371713108721198.png)
> [TSS 和 ESS、RSS的关系 TSS = ESS + RSS](https://aibydoing.com/notebooks/chapter01-08-lab-evaluation-and-validation-of-regression-models)

当 TSS 不变，实际观测点离样本回归拟合线越近，则 ESS 在 TSS 中占的比重越大。
因此，我们定义拟合优度等于回归平方和 ESS 与 y 的总离差 TSS 的比值。
![17131091926401713109192021.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131091926401713109192021.png)
当 RSS 越小时，R^2 就越趋近于 1，那么代表模型的解释力越强
- 一般有效值在 0 ~ 1 间，值越大，模型拟合度越好
- scikit-learn 提供的 API 有可能计算出 R^2 值为负数的情况,这时候 TSS = RSS + ESS 会失效，具体 R^2 值取多少需要视情况而定，不同问题不一样


#### 变量显著性检验

回归分析中判断解释变量 x 是否是被解释变量 y 的一个显著性的影响因素,利用了数学的假设检验知识

假设检验的原理：通过事先对总体参数或总体分布形式作出假设。然后，利用样本信息来判断原假设是否合理。也就是说，通过判断样本信息与原假设是否有显著差异，从而决定是否接受或否定原假设

细节见代码


### 逻辑回归

Logistic Regression,逻辑斯蒂回归,是一种分类方法

#### 线性可分和不可分

二维平面内，如果只使用一条直接就可以将样本分开，则称为线性可分，否则为线性不可分
三维空间则是一个平面去分

#### 线性回归分类

通过拟合一条直线去预测更多的连续值

二分类问题：只有两种类别，也可以称之为：0 - 1 分类问题

#### Sigmoid 分布函数

![17131914946371713191493684.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131914946371713191493684.png)

```py
def sigmoid(z):
    sigmoid = 1 / (1 + np.exp(-z))
    return sigmoid
```

图像呈现出完美的 S 型（Sigmoid 的含义）。它的取值仅介于 0 和 1 之间，且关于 x = 0 轴中心对称。同时当 x 越大时，y 越接近于 1，而 x 越小时，y 越接近于 0

- 输出范围：Sigmoid函数将任意实数映射到(0,1)区间内，非常适合表示概率。在二分类问题中，可以将这个值解释为属于某一类的概率。
- 形状特性：Sigmoid函数形状为"S"型，当输入接近0时，输出变化敏感，而输入值很大或很小的时候，输出趋于平稳，这使得它在区分两个类别时表现良好。
- 梯度特性：在学习过程中，Sigmoid函数的导数（梯度）表达简单，便于在算法中使用梯度下降法进行优化

数学定义: 如果一组连续随机变量符合 Sigmoid 函数样本分布，就称作为逻辑分布。

结合数学定义和特性：把线性函数拟合的结果使用 Sigmoid 函数压缩到 (0,1) 之间。如果线性函数的 y  值越大，也就代表概率越接近于 1，反之接近于 0

![17131920702691713192070243.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131920702691713192070243.png)

对于二分类，值要么是 0 要么是 1，所以概率分布可以这样设定
![17131922076371713192207398.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131922076371713192207398.png)


似然函数：在统计学中用来表示一个模型参数值下观察到的数据概率。
总概率：当多个事件独立发生时，这些事件同时发生的总概率是每个事件发生概率的乘积
> 在逻辑回归中，每个样本产生观察结果的概率是独立的，所以通过所有样本的概率乘积来构造似然函数
将似然函数转化为对数似然函数，从累乘转为累加
![17131923926391713192392307.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131923926391713192392307.png)

#### 对数损失函数

对数似然函数衡量了事件发生的总概率
根据最大似然估计原理，只需要通过对 L(w) 求最大值，即得到 w 的估计值。而在机器学习问题中，我们需要一个损失函数，并通过求其最小值来进行参数优化
> 最大似然估计（MLE）原理基于这样的想法：给定观测数据，在所有可能的参数值中，最有可能产生这些数据的参数值是最优的。
> 实际上，MLE是寻找一组参数，使得观测数据出现的概率（即似然）最大。如果参数值使得已知数据的似然最大，那么在统计意义上，这些参数就是最符合数据的。因此，最大化似然函数可以视为是在寻找最能“解释”观测数据的参数值，这些值通常被认为是这个模型在给定数据下的最优参数估计

除以m，获得平均损失，乘以 -1，将求最大值，转化为求最小值
![17131929726401713192972563.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131929726401713192972563.png)

```py
def loss(h, y):
    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    return loss
```

**为什么不用线性回归的平方损失函数**

我们要求损失最小的最优模型，只有凸函数能求得全局最小值，非凸函数一般只能得到局部最优解

#### 梯度下降法

梯度下降法用处：求取极小值
梯度：一个向量，表示某一函数在该点处的方向的导数沿着该方向取得最大值，即沿着该方向变化最快，变化率最大
- 对于一元函数：是某点的导数
- 对于多元函数：是某点的偏导组成的向量

梯度下降：即沿着下降方向（梯度的反方向）去寻找损失函数的极小值

对对数损失函数进行求导
![17131935956361713193594680.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131935956361713193594680.png)
![17131936226381713193621952.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131936226381713193621952.png)
![17131936386371713193638429.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17131936386371713193638429.png)

将导数 * 常量a，即得到了每次梯度下降的步长，a 通常成为 学习率

每次权重的变化为 w = w - a * 导数

```py
def gradient(X, h, y):
    gradient = np.dot(X.T, (h - y)) / y.shape[0]
    return gradient
```

线性回归方法之所以使用普通最小二乘法来求解，是因为我们可以很方便地求出损失函数的最小值。但是，机器学习中的很多问题，往往会面对非常复杂的损失函数，这些损失函数一般无法直接求得最小值，只能使用迭代方法来求取局部或全局极小值

### K 近邻算法

在解决分类问题的过程中，K 近邻算法（简称：KNN）是一种简单而且实用的方法

#### 最近邻算法

最近邻算法（Nearest Neighbor，简称：NN）：针对未知类别数据 x，在训练集中找到与 x 最相似的训练样本 ，用 y 的样本对应的类别作为未知类别数据 x 的类别，从而达到分类的效果

![17132816910641713281691013.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132816910641713281691013.png)

#### K 近邻算法

K 近邻（K-Nearest Neighbors，简称：KNN）算法是最近邻（NN）算法的一个推广
> NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好

![17132818178801713281817150.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132818178801713281817150.png)

##### 步骤

1.数据准备：通过数据清洗，数据处理，将每条数据整理成向量。
2.计算距离：计算测试数据与训练数据之间的距离。
3.寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。
4.决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。

![17132818678781713281867547.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132818678781713281867547.png)

##### 距离度量

在计算两个样本间的相似度时，可以通过计算样本之间特征值的距离进行表示。

常用的两个距离：曼哈顿距离 和 欧式距离

![17132820608781713282060575.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132820608781713282060575.png)
> 直线距离和轴线距离？

**曼哈顿距离**

又称马氏距离，是计算距离最简单的方式之一

![17132821208781713282120695.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132821208781713282120695.png)
> 将两个数据 X 和 Y 中每一个对应特征值之间差值的绝对值，再求和，便得到曼哈顿距离。

```py
def d_man(x, y):
    d = np.sum(np.abs(x - y))
    return d
```

**欧式距离**

源自 N 维欧氏空间中两点之间的距离公式

![17132822468781713282246297.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17132822468781713282246297.png)
> 将两个数据 X 和 Y 中的每一个对应特征值之间差值的平方，再求和，最后开平方

```py
def d_euc(x, y):
    d = np.sqrt(np.sum(np.square(x - y)))
    return d
```

##### 决策规则

根据数据特征对决策规则进行选取,从而通过 K 个邻居来判断测试样本的最终类别

- 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。
- 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。

##### KNN实现

```py
def d_euc(x, y):
    d = np.sqrt(np.sum(np.square(x - y)))
    return d
  
def majority_voting(class_count):
    # 多数表决函数
    sorted_class_count = sorted(
        class_count.items(), key=operator.itemgetter(1), reverse=True
    )
    return sorted_class_count

def knn_classify(test_data, train_data, labels, k):
    # KNN 方法完整实现
    distances = np.array([])  # 创建一个空的数组用于存放距离

    for each_data in train_data:  # 使用欧式距离计算数据相似度
        d = d_euc(test_data, each_data)
        distances = np.append(distances, d)

    sorted_distance_index = distances.argsort()  # 获取按距离从小到大排序后的索引
    sorted_distance = np.sort(distances)
    r = (sorted_distance[k] + sorted_distance[k - 1]) / 2  # 计算

    class_count = {}
    for i in range(k):  # 多数表决
        vote_label = labels[sorted_distance_index[i]]
        class_count[vote_label] = class_count.get(vote_label, 0) + 1

    final_label = majority_voting(class_count)
    return final_label, r
```

knn算法中，取的k值不同会极大的影响分类结果

#### KD 树算法

如果目前所在点比目前最佳点更靠近输入点，则将其变为目前最佳点。
检查另一边子树有没有更近的点，如果有则从该节点往下找

### K 近邻回归

- 数据准备：通过数据清洗，数据处理，将每条数据整理成向量。

- 计算距离：计算测试数据与训练数据之间的距离。

- 寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。

- 决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。


**「决策分类」是决定未知样本类别的关键步骤**

- 分类问题：根据 K 个邻居的类别，多数表决得到未知样本的类别。

- 回归问题：根据 K 个邻居的目标值，计算平均值得到未知样本的预测值。

###  朴素贝叶斯

朴素贝叶斯是以概率论作为基础的算法
- 实现简单
- 分类效率很高

#### 前置知识

##### 条件概率

![17136023711471713602371072.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136023711471713602371072.png)

条件概率就是指事件 A 在另外一个事件 B 已经发生条件下的概率

P(AB) = P(A) * P(B) // 同时发生A 和 B 的概率
P(AB) / P(B) = P(A|B) // 已发生b后a的发生概率

##### 贝叶斯定理

![17136026412091713602640971.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136026412091713602640971.png)

P(A|B) = P(AB) / P(B)
P(B|A) = P(AB) / P(A)

##### 先验概率

根据以往经验和分析得到的概率，例如抛硬币，根据经验就是0.5

##### 后验概率

事件发生后求的反向条件概率；即基于先验概率通过贝叶斯公式求得的反向条件概率,即反推

#### 朴素贝叶斯原理

将贝叶斯原理以及条件独立结合而成的算法

![17136028122111713602811839.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136028122111713602811839.png)

P(类别|特征) = P(特征|类别) * P(类别) / P(特征)


朴素贝叶斯中的「朴素」，即条件独立：将各个特征割裂开，认定特征之间相互独立
> 现实是很多时候特征间是有互相联系的，所以朴素贝叶斯会牺牲一定的准确性


##### 实现

![17136053539311713605353909.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136053539311713605353909.png)

需要利用极大似然估计进行估计先验概率

极大似然估计：通过若干次实验，观察其结果，利用实验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。


##### 贝叶斯估计

极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 0 的情况，影响后验概率的计算结果，贝叶斯估计可以解决

![17136076672461713607667114.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17136076672461713607667114.png)

在极大似然估计增加一个 λ 因子，等价于在随机变量各个取值的频数上赋予一个正数，λ 为 0 时即为元素的极大似然估计，为 1 时称为拉普拉斯平滑

#### 朴素贝叶斯的三种常见模型

- 多项式模型：当特征值为离散时，常常使用多项式模型
- 伯努利模型：适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 1 和 0（以文本分类为例，某个单词在文档中出现过，则其特征值为 1，否则为 0）
- 高斯模型：当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多先验概率为0，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的
![17137134075591713713407509.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17137134075591713713407509.png)


### 分类模型评估

- 准确率
- 查准率
- 召回率
- F1 值
- ROC 曲线

#### 准确率

预测正确的结果占的比重

在二分类问题中，我们常常会定义正类和负类，就可以给出实际类别（行名）和预测类别（列名）的混淆矩阵
将结果拆分成 TP 、TN 、FP 、FN 四种情况
所以有准确率公式： (TP + TN) / (TP + FP + TN + FN)
- TP：预期正确，并且预测为正类
- TN：预测正确，并且预测为负
- FP: 失败，预测为正类（实际为负）
- FN: 失败，预测为负（实际为正）

![17138006977071713800697307.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138006977071713800697307.png)

#### 查准率

又称精确率，即正确分类的正例个数占分类为正例的实例个数的比例

预测了 n 个数据为正，那么准确的正占据所有正的多少

P = TP / (TP + FP)

你认为的该类样本，有多少猜对了（猜的准确率如何）

#### 召回率 Recall

查全率，即正确分类的正例个数占实际正例个数的比例

预测了 n 个数据为正，占据所有正的数据的多少

R = TP / (TP + FN)

该类样本有多少被找出来（召回了多少）

#### F1 值

查准率和召回率的加权平均数，精确率和召回率的综合评价指标，对衡量数据更有利

F1 = 2 * P * R / (P + R)

![17138012117111713801211356.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138012117111713801211356.png)

#### ROC 曲线

部分分类模型中（如：逻辑回归），通常会设定一个阈值，并规定大于该阈值为正类，小于则为负类。所以，当我们减小阀值时，将会有更多的样本被划分到正类。这样会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类

ROC 曲线的目的在用形象化该变化过程，从而评价一个分类器好坏

![17138013287101713801328301.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138013287101713801328301.png)

- 当 FPR=0，TPR=0 时，意味着将每一个实例都预测为负例。
- 当 FPR=1，TPR=1 时，意味着将每一个实例都预测为正例。
- 当 FPR=0，TPR=1 时，意味着为最优分类器点。

优秀分类器对应的 ROC 曲线应该尽量靠近左上角。当曲线越接近于 45 度对角线，则分类器效果越差

**AUC**

全称为 Area Under Curve,曲线下面积，ROC 曲线下面积
作用：将ROC的曲线视觉好坏，转成数值

- AUC = 1：完美分类器。
- 0.5 < AUC < 1：分类器优于随机猜测。
- AUC = 0.5：分类器和随机猜测的结果接近。
- AUC < 0.5：分类器比随机猜测的结果还差。

### 支持向量机

SVM（Support Vector Machine）

在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，不仅可以应用于线性分布数据，还可以用于非线性分布数据
相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法

#### 线性SVM

1.先找到数据分类分割线
2.然后在分割线两侧在设立两个平行的虚线，两条虚线与分割线的距离一致，这个距离成为“间隔”
3.使得分割线和虚线之间的间隔最大化，即两个虚线间隔最大（不进行错分的情况下）
4.最终实现正类虚线外的就是正类，负类虚线外的是负类
5.正好位于两条虚线上方的样本点就被我们称为“支持向量”，这也就是支持向量机的名字来源

支持向量机的目标：找到最大的分类间隔所对应的分割线

##### 函数间隔

对于线性SVM,决策函数定义为：f(x) = wT * x + b
假设分类结果为 1 和 -1，即结果 y = 1 / -1
那么就有：y * f(x) > 0,h = y * f(x) 就是函数间隔的定义
此时对于每个点 h，到分割平面都会有一个距离，
我们定义 h 取最小值时就是函数间隔，即平面与某个点的间隔值 h(min)

从公式可知，当 (w,b) 平面等比例放大时，f(x) 值会放大，但是平面并没有变，但是函数间隔也跟着变大了，所以函数间隔其实是一个动态值，单纯的函数间隔意义不大

##### 几何间隔



定义一个点 x ，其到超平面上的垂直映射点为 x0 ，两点之间的距离为 h，w 表示超平面的法向量

![17138034117261713803411045.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138034117261713803411045.png)

![17138034227261713803421966.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138034227261713803421966.png)

平面上点到线的距离，拓展到高维即为点到平面的距离，公式: d = (Ax + by + c) / (A^2 + B^2)开根
> 将上述公式转化成向量来写：d = (wT * x + b) / ||w||
> w = [A,B]T 是直线的法向量，x = [x,y]T 是点的坐标向量， b = c,wT * x 是向量的点积，||w|| 是向量 w 的范数即 (A^2 + B^2)开根
> 可见 几何间隔公式其实等价于 点到面的距离公式

公式：函数间隔除以 ||w|| 得到的就是几何间隔，而函数间隔的本质其实可以理解为 |f(x)| 

##### 拉格朗日对偶性

二次规划被用来找到几何间隔最优的决策边界

![17138888645531713888864504.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138888645531713888864504.png)

将从最大值转为求最小值，同时变为二次函数，此时可利用二次规划来求解

在满足 条件 的情况下，拉格朗日对偶性通过给每一个约束条件加上一个拉格朗日乘子 λ，并将约束条件融合到目标函数中

#### 非线性SVM

对于线性不可分的数据集，可以通过支持向量机去完成分类
> 需要增加一个技巧把线性不可分数据转换为线性可分数据之后，再完成分类
> 此技巧称为：核技巧
> 实现数据转换的函数称之为「核函数」

本质：将低维数据映射到高维空间中，使得数据集在高维空间能被线性可分

##### 核函数

核函数有很多种，但大多需要大量计算，目前有总结出几种计算量较小的常用核函数

![17138901103071713890109520.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138901103071713890109520.png)

- 线性核函数: k(xi,xj) = xi * xj
- 多项式核函数：k(xi,xj) = (xi * xj)^d, d > 1
- 高斯径向基：......
- sigmoid: ......

分别对应：linear, scikit-learn 中 SVC 方法的 poly, rbf, sigmoid

核函数也可以进行线性组合使用


#### 多分类支持向量机

支持向量机最初是为二分类问题设计的，当我们面对多分类问题时，其实同样可以使用支持向量机解决。而解决的方法就是通过组合多个二分类器来实现多分类器的构造

- 一对多法：即训练时依次把某个类别的样本归为一类，剩余的样本归为另一类，这样 k 个类别的样本就构造出了 k 个支持向量机。
- 一对一法：即在任意两类样本之间构造一个支持向量机，因此 k 个类别的样本就需要设计 k(k - 1) / 2 个支持向量机

#### 总结

SVM 是一种表现非常不错的方法，尤其是对于非线性分类问题。而且最大的劣势在于计算效率，随着数据集的增大，计算时间陡增。所以，一般我们会在小数据集下应用 SVM，而大数据集基本不予考虑

### 决策树

决策树是一种特殊的树形结构，一般由节点和有向边组成。
- 节点表示特征、属性或者一个类
- 有向边包含判断条件

决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树

例如：
![17139732085771713973208527.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139732085771713973208527.png)

#### 算法原理

- 数据准备 → 通过数据清洗和数据处理，将数据整理为没有缺省值的向量。
- 寻找最佳特征 → 遍历每个特征的每一种划分方式，找到最好的划分特征。
- 生成分支 → 划分成两个或多个节点。
- 生成决策树 → 对分裂后的节点分别继续执行 2-3 步，直到每个节点只有一种类别。
- 决策分类 → 根据训练决策树模型，将预测数据进行分类。

#### 信息增益（ID3）

**信息熵**

度量样本纯度最常用的一种指标

公式：![17139733548731713973354170.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139733548731713973354170.png)
- D表示样本集合
- p(k) 表示第 k 类样本所占的比例
- Ent(D) 越小则 D 的纯度越高

计算示例：![17139735548751713973554575.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139735548751713973554575.png)
- 所有结果概率 * log 的和

**信息增益**

是建立在信息熵的基础上

公式：![17139737308761713973730789.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139737308761713973730789.png)

信息增益越大，使用特征划分出来的集合纯度越高。

![17139739108791713973910831.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139739108791713973910831.png)

#### 信息增益率（C4.5）

信息增益的不足：当信息增益作为标准时，易偏向于取值较多的特征，为了避免这种偏好给预测结果带来的不好影响，可以使用增益率来选择最优划分

![17139740368971713974036871.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139740368971713974036871.png)

#### 连续值处理

对于离散值可以每个值作为一个分类，但是当数据是连续值时，这样分类就太多了，采用二分法对连续值进行处理

对于连续的属性 X 假设共出现了 n 个不同的取值，将这些取值从小到大排序 { x1, x2, x3 ......}，其中找一点作为划分点 t ，则将数据划分为两类，大于 t 的为一类，小于 t 的为另一类。而 t 的取值通常为相邻两点的平均数 t = (xi + xi+1) / 2

![17139742428821713974242782.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139742428821713974242782.png)

#### 预剪枝和后剪枝

决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多

对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝

##### 预剪枝

预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点

##### 后剪枝

在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。

#### 实现

1.数据清洗：
- 选取合适的特征（有效，少量减少计算量）
- 对数据进行简单分类：对于数值，可以简化分为几类
- 对数据特征进行替换：分类 -> 0、1、2、3
2.划分训练、测试集
3.训练模型


### CART 决策树

分类与回归树（classification and regression tree, CART）同样也是应用广泛的决策树学习算法，CART 算法是按照特征划分，由树的生成和树的剪枝构成，既可以进行分类又可以用于回归，按照作用将其分为决策树和回归树

![17139759908951713975990600.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17139759908951713975990600.png)

### 超参数选择方法

#### 网格搜索

预先制定好各参数的有限个候选取值，然后通过排列组合的方式来传入这些参数，最终通过 K 折交叉验证的方法来确定表现最好的参数
- K 折交叉验证：是交叉验证中的一种常见方法，其通过将数据集均分成 K 个子集，并依次将其中的 K-1 个子集作为训练集，剩下的 1 个子集用作测试集。在 K 折交叉验证的过程中，每个子集均会被验证一次

**步骤**

- 首先将数据集均分为 K 个子集。
- 依次选取其中的 K-1 个子集作为训练集，剩下的 1 个子集用作测试集进行实验。
- 计算每次验证结果的平均值作为最终结果。

K 折交叉验证让每一条数据都有均等的几率被用于训练和验证，在一定程度上能提升模型的泛化能力

#### 随机搜索

网格搜索很直观，也很方便，但是最大的问题在于随着候选参数增多，搜索需要的时间迅速增加

随机搜索，顾名思义就是经验 + 运气的碰撞。我们依据经验制定一个参数范围，然后在范围内随机选取参数测试，并返回最佳结果
> 本质上还是类似的思路和方式，只是定了区间后，随机选取数据进行尝试

### 装袋和提升集成学习方法

每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，由于数据的不确定性，单独应用个别分类器可能会出现分类准确率低的问题。为了应对这样的情况，集成学习被提出，其可以利用多个弱分类器结合的方式提高分类准确率

#### 集成学习

通过构建多个分类器并综合使用来完成学习任务，同时也被称为多分类器系统。其最大的特点就是结合各个弱分类器的长处

集成分类
- 同质集成：在一个集成学习中，「个体学习器」是同一类型，如 「决策树集成」 所有个体学习器都为决策树
- 异质集成：在一个集成学习中，「个体学习器」为不同类型，如一个集成学习中可以包含决策树模型也可以包含支持向量机模型

集成方式：
- 并行式，当个体学习器之间不存在强依赖关系时，可同时生成并行化方法，其中代表算法为装袋（Bagging）算法。
- 串行式，当个体学习器之间存在强依赖关系时，必须串行生成序列化方法，其中代表算法为提升（Boosting）算法

![17140605014791714060500573.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17140605014791714060500573.png)

#### 结合策略

- 平均法: 在数值型输出中，最常用的结合策略为平均法
  - 简单平均法：取每一个「个体学习器」学习后的平均值
  - 加权平均法：其中 w(i) 是每一个「个体学习器」 h(i) 的权重，通常为 w(i) > 0 
- 投票法: 对于分类输出而言，平均法效果不太好,最常用的结合策略为投票法
  - 多数投票法：即在「个体学习器」分类完成后，通过投票选出分类最多的标签作为此次分类的结果。
  - 加权投票法：同加权平均法类似，w(i) 是每一个「个体学习器」 h(i) 的权重，通常为 w(i) > 0
- 学习法：平均法和投票法比较简单，但是可能学习误差较大
  - 代表方法：stacking 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，即把训练集弱学习器的学习结果作为输入，重新训练一个学习器来得到最终结果
    - 首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果

#### 装袋算法 Bagging

1.数据处理：将数据根据实际情况进行清洗整理。
2.随机采样：从样本中随机选出 m 个样本作为一个子样本集。有放回的重复 T 次，得到 T 个子样本集。
3.个体训练：设定 T 个个体学习器，将每一个子样本集放入对应个体学习器进行训练。
4.分类决策：用投票法集成进行分类决策。

##### Bagging Tre(决策树)

决策树是一个十分「完美」的训练器，但特别容易出现过拟合的情况，最终导致预测准确率低的问题

在装袋算法中，决策树常常被用作弱分类器

Bagging Tree 算法：是应用子数据集中的所有特征构建一棵完整的树，最终通过投票的方式进行预测

##### 随机森林 Random Forest

随机森林：「随机抽样 + 决策树森林」

将一个大的数据集使用自助采样法进行处理，即从原样本数据集中随机抽取多个子样本集，并基于每一个子样本集生成相应的决策树

构建出由许多小决策树组形成的决策树「森林」。最后，实验通过投票法选择决策树最多的预测结果作为最终的输出

**改进点**
1.对于普通的决策树，会在 N 个样本的所有特征中选择一个最优划分特征，但是随机森林首先会从所有特征中随机选择部分特征，再从该部分特征中选择一个最优划分特征。这样进一步增强了模型的泛化能力。
2.在决定部分特征个数时，通过交叉验证的方式来获取一个合适的值。

**随机森林算法流程**

1.从样本集中有放回随机采样选出 n 个样本。
2.从所有特征中随机选择 k 个特征，对选出的样本利用这些特征建立决策树。
3.重复以上两步 m 次，即生成 m 棵决策树，形成随机森林。
4.对于新数据，经过每棵树决策，最后投票确认分到哪一类。


#### 提升算法 Boosting

当「个体学习器」之间存在较强的依赖时，采用装袋的算法便有些不合适，此时最好的方法就是使用串行集成方式：提升（Boosting）

作用：将弱学习器提升为强学习器

方式：从初始训练集训练出一个「个体学习器」，再根据个体学习器的表现对训练样本分布进行调整，使得在个体学习器中判断错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个「个体学习器」。如此重复进行，直至个体学习器数目达到事先指定的值 T，最终将这 T 个「个体学习器」输出的值进行加权结合得到最终的输出值

##### Adaboost

利用前一轮迭代弱学习器的误差率来更新训练集的权重

自适应增强：上一个「个体学习器」中被错误分类的样本的权值会增大，正确分类的样本的权值会减小，并再次用来训练下一个基本分类器；在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器

**流程**

1.数据准备：通过数据清理和数据整理的方式得到符合规范的数据。
2.初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1 / N。
3.弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。
4.更改权重：如果某个样本点被准确地分类，降低其权值；若被分类错误，那么提高其权值。然后，权值更新过的样本集被用于训练下一个分类器。
  - 使在下轮时训练样本集更注重于难以识别的样本
5.强分类器组合：重复 3，4 步骤，直至训练结束，加大分类误差率小的弱分类器的权重（这里的权重和样本权重不一样），使其在最终的分类函数中起着较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用，最终输出结果

##### 梯度提升树 GBDT

梯度提升树（Gradient Boosting Decison Tree，GBDT）

采用前向分布算法，且弱学习器限定了只能使用CART树模型

**流程**

1.数据准备：通过数据清理和数据整理的方式得到符合规范的数据。
2.初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1 / N
3.弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。
4.CART 树拟合：计算每一个子样本的梯度值，通过梯度值和子样本拟合一棵 CART 树
5.更新强学习器：在拟合好的 CART 树中通过损失函数计算出最佳的拟合值，更新先前组成的强学习器。
6.强分类器组合：重复 3，4，5 步骤，直至训练结束，得到一个强分类器，最终输出结果。

### 模型选择

同时使用多种模型，并且使用 k 折数据法，进行快速验证选取哪一个模型


## 无监督学习

面对无标签数据常常使用的一类机器学习方法
常用于聚类，其他：降维、图分析、关联规则分析

数据聚类：把一堆数据按照它们特征的相似度分为多个子类（向量分布？）
- 聚类后可用于自动打标签


### 聚类

通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）

**划分过程**：首先由用户确定划分子集的个数 𝑘，然后随机选定 𝑘 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 𝑘 个子集，即将数据划分为 𝑘 类。

**评估标准就是**：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。

#### 方式

##### K-Means

**步骤**
- 随机初始化 K 个（代表拟聚类簇个数）中心点
- 每一个样本按照距离自身最近的中心点进行聚类，等效于通过两中心点连线的中垂线划分区域
- 移动中心点到个簇的质心位置，并将此质心作为新的中心点
- 反复迭代，直至中心点的变化满足收敛条件（变化很小或几乎不变化）


**SSE**

类似于回归算法通过减小目标函数（如：损失函数）的值拟合数据集一样，聚类算法通常也是优化一个目标函数，从而提高聚类的质量


常常使用误差的平方和 SSE（Sum of squared errors）作为度量聚类效果的标准，当 SSE 越小表示聚类效果越好


![17149131450481714913144012.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17149131450481714913144012.png)


**k值选取**

- 肘部法则：计算多个k值，看sse变化，畸变程度最大的点称之为「肘部」，可以认为是最佳 k 值

**缺点**

随着数据量的增长，分类数目增多时，会出现一个较大子集有多个中心点，而其他多个较小子集公用一个中心点的问题。即算法陷入局部最优解而不是达到全局最优解的问题

原因：一部分中心点在初始化时离的太近

解决方案：
- 在同一数据集上运行多次 K-Means 算法聚类，选取 SSE 最小的那次作为最终的聚类结果
	- 数据集较大时，比较耗时
- k-means++

##### K-Means++

在初始化中心点上做了改进

**步骤**

- 在数据集中随机选择一个样本点作为第一个初始化的聚类中心。
- 计算样本中的非中心点与最近中心点之间的距离 𝐷(𝑥) 并保存于一个数组里，将数组中的这些距离加起来得到 𝑠𝑢𝑚(𝐷(𝑥)) 。
- 取一个落在 𝑠𝑢𝑚(𝐷(𝑥)) 范围中的随机值 𝑅 ，重复计算 𝑅=𝑅−𝐷(𝑥) 直至得到 𝑅≤0 ，选取此时的点作为下一个中心点。
- 重复 2,3 步骤，直到 𝐾 个聚类中心都被确定。
- 对 𝐾 个初始化的聚类中心，利用 K-Means 算法计算最终的聚类中心

R的作用：直接选取距离最远的点作为初始点的方法，会容易受到数据集中离群点的干扰，随机值 𝑅 的方法避免数据集中所包含的离群点对算法思想中要选择相距最远的中心点的目标干扰

**总结**
无法完全避免随机选择中心点带来的不稳定性，所以偶尔也会得到不太好的结果。但是 K-Means++ 算法得到不太好的聚类的概率远小于 K-Means 算法


##### Mini-Batch K-Means

为了减少计算量而开发的算法

在每一次迭代过程中，从数据集中随机抽取一部分数据形成小批量数据集，用该部分数据集进行距离计算和中心点的更新

### 层次聚类

k means 问题：需要手动指定 K 值，需要多次测试找到最优 K 值

#### 自底向上层次聚类法（Agglomerative Clustering）

**步骤/原理**

对于数据集 D，D = (x1,x2,.....xn) ：
1.将数据集中每个样本标记为 1 类，即 D 初始时包含的类别（Class）为 C = (c1,c2....cn) 。
2.计算并找出 C 中距离最近的 2 个类别，合并为 1 类。
3.依次合并直到最后仅剩下一个列表，即建立起一颗完整的层次树。




**距离计算**
- 单连接：根据两种类别之间最近的元素间距离作为两类别之间的距离
- 全连接：根据两种类别之间最远的元素间距离作为两类别之间的距离
- 平均连接：依次计算两种类别之间两两元素间距离，并最终求得平均值作为两类别之间的距离。
- 中心连接：平均连接虽然看起来更加合理，但是两两元素间的距离计算量往往非常庞大。有时候，也可以使用中心连接计算方法。即先计算类别中心，再以中心连线作为两类别之间的距离

「单连接」和「全连接」都相对极端，容易受到噪声点和分布不均匀数据造成的干扰



#### 自顶向下层次聚类法


比自底向上复杂

自顶向下层次聚类法在实施过程中常常遇到一个问题，那就是如果两个样本在上一步聚类中被划分成不同的类别，那么即使这两个点距离非常近，后面也不会被放到一类中。
在实际应用中，自顶向下层次聚类法没有自底而上的层次聚类法常用


**利用 k means 进行分割**
1.把数据集 D 归为单个类别 C 作为顶层。
2.使用 K-Means 算法把 C 划分成 2 个子类别，构成子层；
3.递归使用 K-Means 算法，分别对划分的子类，继续进行 2 类 划分子层直到每个点都是单独分类或特定条件结束

**利用平均距离进行分割**
1.把数据集 D 归为单个类别 C 作为顶层。
2.从类别 C 中取出点 d，使得 d 满足到 C 中其他点的平均距离最远，构成类别 N。
3.继续从类别 C 中取出点 d1， 使得 d1 满足到 C 中其他点的平均距离与到 N 中点的平均距离之间的差值最大，并将点放入 N。
4.重复步骤 3，直到差值为负数。(直到点 d1 远离 N 类，接近 C 类)
5.再从子类中重复步骤 2，3，4 直到全部点单独成类，即完成分割。


##### BIRCH 聚类算法

Balanced Iterative Reducing and Clustering using Hierarchies，直译过来就是「使用层次方法的平衡迭代规约和聚类」，该算法由时任 IBM 工程师 Tian Zhang 于 1996 年发明。
优点：效率高，可用于大型数据集的快速聚类
原理：基于训练样本建立了 CF 聚类特征树。CF 聚类特征树对应的输出就是若干个 CF 节点，每个节点里的样本点就是一个聚类的类别

BIRCH 算法在建立 CF 特征树时只存储原始数据的特征信息，并不需要存储原始数据信息，内存开销上更优，计算高效。
BIRCH 算法只需要遍历一遍原始数据，而 Agglomerative 算法在每次迭代都需要遍历一遍数据，再次突出 BIRCH 的高效性。
BIRCH 属于在线学习算法，并支持对流数据的聚类，开始聚类时并不需要知道所有的数据。

CF 聚类特征：对特征数据进行运算并且以元组的形式记录，定义类别（簇）的信息，并有效地对数据进行压缩
CF = (N,LS,SS)
N: 表示该 CF 中拥有的样本点的数量；  
LS: 表示该 CF 中拥有的样本点各特征维度的和向量；  所有点各维度间各自相加（结果是向量）
SS: 表示该 CF 中拥有的样本点各特征维度的平方和；  所有点维度值平方总和（结果是值）
CF 拥有可进行加法运算

CF Tree 聚类特征树
枝平衡因子 β、叶平衡因子 λ 和空间阈值 t
非叶节点包含多个子节点，不少于 β 个


### 密度聚类

通过评估样本的紧密程度来划分对应的类别，理论上可以找出任何形状的聚类并有效避免噪声的干扰,也无需像划分聚类那样提前声明需要聚集的类别数量

层次聚类和划分聚类共有的缺点：无法很好地接近非凸（non-convex）数据的聚类问题

#### DBSCAN 密度聚类算法

density-based spatial clustering of applications with noise,具有噪声的基于密度的聚类方法,由 Martin Ester 等在 1996 年提出

可以在有噪音的数据中发现形状与大小各异的类别

**步骤**

 一般假定样本的类别可以通过样本分布的紧密程度决定，于是先发现密度较高的点，然后把相近的高密度点逐步连成一片，进而找到不同的类别（簇）

1.首先，DBSCAN 会以每个数据点为圆心，以 eps（σ-邻域） 为半径画圆。
2.然后，DBSCAN 会计算相应圆中有多少个其他点，并以该数目作为圆心数据点的密度值。
3.接下来，我们需要确定密度阈值 MinPts，并分别将小于或大于该密度阈值的数据点（包含自己）称作低密度或高密度点（核心点）。
4.如果，此时有一个高密度的点在另一个高密度的点的圆圈范围内，我们就把这两个点串联起来。
5.之后，如果有低密度的点也在高密度的点的圆圈范围内，也将其连接到最近的高密度点上，并称之为边界点。

最后，所有能连到一起的数据点就形成了一个聚类类别

**概念**

- σ-邻域：N(xj) = distance(xi,xj) <= σ-邻域
	- 点两两间的距离小于距离 σ 的，划分为子样本集的数量
- 核心对象：如果某个样本集包含了 >= MinPts 个样本，则该样本集被称为核心对象。
- 密度直达：xi 位于 xj 的邻域中,xj 是核心对象，则 xi 由 xj 密度直达
- 密度可达：密度直达具有传播性，例如有样本 p1、p2......pn,分别 pn+1 由 pn 密度直达，那 p1 和 pn 成为密度可达
- 密度相连：由核心对象密度直达的任意两个点，它们的关系成为密度相连

**对比**

缺点：参数敏感性，eps 敏感

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240520000415.png)

#### HDBSCAN 密度聚类算法

由 [Campello, Moulavi, and Sander](http://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14) 三人于 2013 年提出,Density-Based Clustering Based on Hierarchical Density Estimates

基于层次密度估计的密度聚类方法

解决 DBSCAN 参数敏感的缺点

**核心**

1. 生成原始聚簇。 此阶段和 DBSCAN 算法的第一阶段相似，确定核心点，生成原始聚簇并有效地识别噪声点。
    
2. 合并原始聚簇。 在此阶段使用层次聚类的思想对原始聚簇进行合并，降低了聚类结果对输入参数的敏感性。由于该方法不需要对每个对象进行测试和判断，同时也降低了时间复杂度。


**实现**

涉及到图论


核心距离 𝑐𝑜𝑟𝑒𝑘(𝑥) : 当前点到其第 k 近的点直接的距离，一般用欧式距离。

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240520001401.png)


互达距离 𝑑mreach−𝑘(𝑎,𝑏) : 从核心点到相邻核心点直接的距离，一般用欧式距离。

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240520001450.png)


**应用**

- 筛选合适聚类的点
- 反向操作：筛选异常点

### 谱聚类（Spectral Clustering）  
  
于 2006 年由 Ulrike von Luxburg 公布在论文 A Tutorial on Spectral Clustering 上。  

很多时候会被用于图像分割

1. 适用于各类形状的数据分布聚类。
2. 计算速度较快，尤其是在大数据集上明显优于其他算法。
3. 避免了 K-Means 会将少数离群点划为一类的现象

缺点：对参数是比较敏感的
- 最后使用的 K-Means 聚类，所以要提前指定聚类数量
- 使用 K-近邻生成邻接矩阵时还需要指定最近邻样本数量
- 不适合于大规模数据（计算矩阵的特征值和特征向量会非常耗时）

#### 概念
  
**无向图**: 把平面/空间中的数据点通过直线连接起来的图形就是无向图  
  
**拉普拉斯矩阵（Laplacian Matrix）**

也称为基尔霍夫矩阵，是无向图的一种矩阵表示形式  
表达式：L(n x n) = D - A  
D 为图的度矩阵，A 为图的邻接矩阵  
  
**度矩阵**  

- 无向图中，顶点 vi 的度 d(vi) = N(i)𝑑(𝑣𝑖) = 𝑁(𝑖)（即与顶点相连的边的数目）
- 有向图中，顶点 vi 的度分为顶点 vi 的出度和入度，即从顶点vi出去的有向边的数量和进入顶点vi的有向边的数量。

对于有连接的点 i、j，有权重  w(i,j) > 0  
对于无连接的点 i、j，w(i,j) = 0  
对于任意一点 i, 它的度 d(i) = Σw(i,j),即它关联的所有（权重 / 点的数量？）的和  

度矩阵是一个对角矩阵，主角线上的值由点的度组成，多少个点就有多少个对角线点  

对角矩阵：只有主对角线有值，其他位置为 0  


**邻接矩阵**

邻接矩阵表示顶点间关系，任意两点之间的权重值 𝑤(𝑖,𝑗) 组成的矩阵

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521001111.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521001338.png)


构造邻接矩阵的三种方式
- σ-邻近法
	- 设置一个阈值 𝜖，再求解任意两点 𝑥𝑖 和 𝑥𝑗 间的欧式距离 𝑠(𝑖,𝑗) 来度量相似性。然后，根据 𝑠(𝑖,𝑗) 和 𝜖 的大小关系
- 全连接法：通过选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和 Sigmoid 核函数
	- 也是通过距离得到 w(i,j)，但是获得距离的函数是其他函数
- K-近邻法


#### 无向图切图

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521002034.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521003211.png)


对于数据，可以切分为没有交集的 n 个集合

对于集合 A 、B 他们之间的切图权重为 W(A,B) = ∑w(i,j)

扩展到 k 个子图集合有：
cut(A1,A2....Ak) = (1/2) \* ∑W(Ai,\_A)  \_A 是 Ai 的补集
- W(Ai,\_A)  表示 Ai 和 任一集合的权重
- ∑W(Ai,\_A) 就是 Ai 和剩下其他所有集合的权重和
- 计算时，其实会计算 W(A,B) 又会计算W(B,A)，所以最终要除 2

cut(A1,A2....Ak) 的值是子图间的权重和，即不同子图之间的连接程度
- 值很高，意味着有很多边或权重较大的边连接不同的子图。这样的情况表明这些子图之间联系紧密，即相似性较高
- 值很低，意味着只有少量的边或权重较小的边连接不同的子图。这样的情况表明这些子图之间联系较弱，即相似性较低

所以：值越小则代表子图的差异性越大
> 极端场景下，少量或者一个点被划分为一个子图，此时会影响数据的有效性

- RatioCut：在切图的定义下增加 节点个数作为分母，稀释大小
- Ncut：Ratiocut 的分母 |𝐴𝑖| 换成 𝑎𝑠𝑠𝑜𝑐(𝐴𝑖)，以数据点的权重和作为分母，从根源上进行正则
	- 一般来说 Ncut 切图优于 RatioCut 切图


#### 步骤


1. 根据数据构造无向图 𝐺，图中的每一个节点对应一个数据点，将相似的点连接起来，并且边的权重用于表示数据之间的相似度。
2. 计算图的邻接矩阵 𝐴 和度矩阵 𝐷，并求解对应的拉普拉斯矩阵 𝐿。
3. 求出 𝐿 的前 𝑘 个由小到大排列的特征值{𝜆}𝑖=1𝑘以及对应的特征向量{𝑣}𝑖=1𝑘。
4. 把 𝑘 个特征向量排列在一起组成一个 𝑁×𝑘 的矩阵，将其中每一行看作 𝑘 维空间中的一个向量，并使用 K-Means 算法进行聚类，并得到最终的聚类类别。

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521001925.png)


较小特征值及其特征向量提供了有关图的低频信息，反映了图的整体结构或大尺度结构。这些信息在图聚类中尤其重要，因为它们可以揭示图中社区或簇的结构

###  亲和传播聚类

Affinity Propagation，基于数据点进行消息传递的理念设计的，不需要提前确定聚类的数量，即 K 值。但由于 Affinity Propagation 运行效率较低，不太适合于大数据集聚类


### Mean Shift

均值漂移聚类，找出最密集的区域，同样也是一个迭代过程。
在聚类过程中，首先算出初始中心点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件。

Mean Shift 在图像分割，视频跟踪等领域也有较好的应用


### 聚类方式选取

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240521010458.png)


### 关联规则

一种在大型数据库中发现变量之间的有趣关系的方法

#### 概念

- 项集：集合S 属于项Item的集合  
- k-项集：包含 k 个项的项集  
- 集合和项集，根据划分层次，会有不一样的划分  
	- 例如：  
		- 购物车可以理解为项的集合I，其中某几个物品形成的集合则为项集  
		- 如果超市一天的账单为集合I，则单个购物车的下单内容可以作为项集    
- 频繁项集：表明一个项集出现很频繁  

对于X、Y不相交的两个项集，X -> Y，通过支持度和置信度可以衡量关联强度  
- 支持度：项集X、Y在集合I中出现的概率，可以设定一定支持阈值，可以认为超过的项集为频繁项集，即“有用”  
- 置信度：在X出现的情况下，Y出现的可能性  


提升度、杠杆率和确信度
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240523005205.png)

#### 任务

- **发现频繁项集**：发现满足最小支持度阈值的所有项集，也就是所说的「频繁项集」。
- **计算关联规则**：从发现的频繁项集中提取所有高置信度的规则，作为关联规则挖掘结果。

#### 疑问
只有样本足够大，才会比较准确，同时项集怎么指定也是问题    
实操：一般计算时怎么划分项集？遍历一遍？  
通过判断大集合可以省略小集合的运算？



### Apriori 算法

一般通过格结构来枚举可能的项集  
一般n项数据可以产生2^n -1的候选项集

于 1994 年在论文 [Fast algorithms for mining association rules in large databases](https://dl.acm.org/citation.cfm?id=672836) 中首次出现，它也是关联规则挖掘中最常用的方法

- **定理 1**：如果一个项集是频繁项集，那么其所有的子集也一定是频繁项集。
- **定理 2**：如果一个项集是非频繁项集，那么其所有的超集也一定是非频繁项集。





#### 步骤

首先，令 𝐹𝑘 表示频繁 𝑘 项集的集合，𝐶𝑘 表示候选 𝑘 项集的集合：

- 算法一次扫描整个数据集，计算每个项的支持度，并得到频繁 1 项集的集合 𝐹1。
    
- 然后，算法使用频繁 𝑘−1 项集产生候选 𝑘 项集，可以使用 𝐹𝑘−1×𝐹𝑘−1 或 𝐹𝑘−1×𝐹1 方法。
    
- 算法再次扫描数据集对候选项集完成支持度计数，并使用子集函数确定在 𝐶𝑘 中的全部候选 𝑘 项集。
    
- 计算候选项集的支持度，并删去支持度小于预先设定阈值的候选项集。
    
- 当没有频繁项集产生，即 𝐹𝑘=∅ 时，算法结束。
    

产生完频繁项集，就可以从给定的频繁项集中提取关联规则。一般情况下，每个频繁 𝑘 项集能够产生 2𝑘−2 个关联规则。产生出来的规则需要基于置信度剪枝，以满足提前设定的置信度阈值。

这里，我们又要给出一条关于置信度的定理。假设项集 𝑌 可以被划分为两个非空子集 𝑋 和 𝑌−𝑋，那么：

- 如果规则 𝑋→𝑌−𝑋 不满足置信度阈值，则规则 𝑋′→𝑌−𝑋′ 一定不满足置信度阈值。其中 𝑋′ 是 𝑋 子集。
    

Apriori 算法使用逐层方法来产生关联规则，其中每层对应于规则后件中的项数。初始时，提取规则后半部分只含一个项的所有高置信度规则；然后，使用这些规则来产生新的候选规则。


### 时间序列数据建模分析

年份转换为序号，从而引入回归分析的手段得到未来的预测结果

  

时间序列数据特点:

- 时间序列数据依赖于时间，但不一定是时间的严格函数。

- 时间序列数据每时刻上的值具有一定的随机性，不可能完全准确地用历史值去预测。

- 时间序列数据前后时刻（但不一定是相邻时刻）的数值往往具有相关性。

- 时间序列往往会呈现出某种趋势性或出现周期性变化的现象。


  

数据类型

- 按研究对象分类：一元时间序列和多元时间序列。

- 按时间参数分类：离散时间序列和连续时间序列。

- 按统计特性分类：平稳时间序列和非平稳时间序列。

- 按分布规律分类：高斯型时间序列和非高斯型时间序列。

  

#### 描述性时序分析

确定型时序分析，通过直观的数据比较或绘图观测，寻找序列中蕴含的发展规律
缺点：需要数据呈现出一定的规律性

#### 统计时序分析

利用数理统计学相关的原理和方法来分析时间序列

- 频域分析：假设任何一种无趋势的时间序列都可以分解成若干不同频率的周期波动。
- 早期的频域分析方法借助傅里叶分析从频率的角度揭示时间序列的规律。后来，其借助了傅里叶变换，用正弦、余弦项之和来逼近某个函数。再到极大熵谱估计理论的引入，频域分析进入了现代谱分析阶段。由于谱分析依赖于强数学背景且不利于直观解释，导致该方法具有很大的局限性
- 时域分析：时域分析方法应用要广泛很多。原理主要是参照事件发展过程中的惯性，从而通过惯性用统计来描述就是时间序列值之间存在的相关关系，而这种相关关系通常具有某种统计规律。时域分析的目的是，通过寻找出时间序列值之间相关关系的统计规律，并拟合出适当的数学模型来描述这种规律，进而利用这个拟合模型预测序列未来的走势
- 时域分析方法的产生最早可以追溯到 1927 年出现的自回归 AR 模型。不久之后，英国数学家、天文学家 Walker 爵士在分析印度大气规律时使用了移动平均 MA 模型和自回归移动平均 ARMA 模型。这些模型奠定了时间序列时域分析方法的基础
- ARMA 模型也被得以广泛应用。再到后来，美国统计学家 Box 和英国统计学家 Jenkins 系统地阐述了对求和自回归移动平均 ARIMA 模型的识别、估计、检验及预测的原理及方法。这些知识现在被称为经典时间序列分析方法，是时域分析方法的核心内容。
- ARMA 模型通常被用于平稳时间序列分析过程
- ARIMA 模型则广泛应用于非平稳序列随机分析过程

  
**平稳时间序列检验**

从概率统计的角度来定义平稳时间序列。一般来讲，有两种定义
- 严平稳时间序列和宽平稳时间序列：严平稳要求序列所有的统计性质都不会随着时间的推移而发生变化
- 宽平稳：认为只要保证序列 二阶矩 平稳，就代表序列稳定

严平稳比宽平稳的条件严格。严平稳是对序列联合分布的要求，以保证序列所有的统计特征都相同。


关于序列平稳性的检验，一般有两种方法：
- 图检验
- 假设检验

  
图检验是根据时序图和自相关图显示的特征作出判断，因其操作简便而运用广泛。简单来讲，如果一张时序图呈现出明显的增长和下降趋势，那么就一定不平稳。

**自相关图**

用于确认数据是否平稳曲线

自相关（Autocorrelation），又称之为序列相关，是一个统计学上的概念。
相关性其实就是变量之间的关系强度

在时间序列中，当我们使用以前的时间步长来计算时间序列观测的相关性时。由于时间序列的相关性与之前的相同系列的值进行了计算，就被称之为自相关。

自相关函数 ACF 用来度量时间序列中延迟为 k 时，相距 k 个时间间隔（延迟期）的序列值之间的相关性，所形成的图称之为自相关图。


**区分平稳序列**

平稳序列通常具有短期相关性。该性质用自相关系数来描述就是随着延迟 𝑘 的增加，平稳序列的自相关系数会很快地衰减向零

- 图中显示的序列自相关系数一直较小，在 0 附近震荡波动


**纯随机性检验**


纯随机性检验：判断一个平稳序列是否随机呢

序列平稳，那么就可以应用 ARMA 等成熟的建模方法完成分析
如果随机性太强，纯随机序列是没有任何分析价值的


原理：一般会涉及到两个统计量
- Q 统计量
- LB 统计量（Ljung-Box）
	- LB 统计量是 Q 统计量的修正，所以业界通常所称的 Q 统计量也就是 LB 统计量

 LB 统计量的 P 值小于 `0.05`，我们则认为该序列为非随机序列，否则就为随机序列

#### ARMA 介绍及建模

全称是自回归移动平均模型，它是目前最常用的拟合平稳序列的模型
可以被细分为 
- AR 自回归模型
- MA 移动平均模型
- ARMA

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240527191503.png)


ARMA 模型一般记作：𝐴𝑅𝑀𝐴(𝑝,𝑞)，即为 𝑝 阶 AR 和 𝑞 阶 MA 模型的组合。Python 中，我们可以利用 `statsmodels` 统计计算库中的 `tsa.ARMA` 类完成 ARMA 建模和预测

计算时，需要确定：p 、q 的取值，有以下三种方法
- AIC（Akaike Information Criterion ）
- BIC（Bayesian Information Criterion ） 
- HQIC（Hannan-Quinn Criterion ）

ARMA 建模步骤：获取序列 → 通过平稳性检验 → 通过纯随机性检验 → 估计 𝑝 和 𝑞 参数 → ARMA 建模 → 模型评估

对于不平稳的序列，如果能通过一些方法处理成平稳就可以用 ARMA 进行推导
> 例如：差分

#### ARIMA 介绍及建模

适合于对非平稳序列进行建模分析，I 就代表差分，多了一个参数，那就是使非平稳序列成为平稳序列所做的差分阶数 𝑑。所以，ARIMA 模型通常记作：𝐴𝑅𝐼𝑀𝐴(𝑝,𝑑,𝑞)。


#### 时间序列数据分析流程

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240527192750.png)

## 机器学习工程：模型部署和推理

### 自动化机器学习

Automated machine learning，AutoML，可以看作为一种基于人工智能的解决方案，以应对不断增长的机器学习应用场景的需要

较早的想法见于 ACM 论文 [Auto-WEKA](https://dl.acm.org/citation.cfm?id=2487629)

将机器学习应用于现实问题的端到端流程自动化的过程

原因：典型机器学习方法门槛太高
>在典型的机器学习流程中，开发者必须学会数据预处理，特征工程，特征提取和特征选择方法，使数据集适合机器学习。在这些数据预处理步骤之后，开发者必须选择合适的算法，并完成超参数及优化方法的选择

作用：开发者只需要提供数据，例如不同类别的图片。接下来，算法的选择，算法的训练，参数的调优，模型的部署等一系列过程都可以交给 AutoML 组件来完成

#### 方向

- 自动化特征工程：Automated Feature Enginnering，简称 Auto FE
	- 包含特征选择，特征提取，元学习，以及检测和处理不均衡数据或缺失数据等操作
	- [特征选择](https://en.wikipedia.org/wiki/Feature_selection)（英语：Feature selection）也被称为变量选择、属性选择或变量子集选择
		- 为了构建模型而选择相关特征子集的过程
		- 可以被看作是数据降低维度的步骤，初始的资料集合被降到更容易管理的族群（特征）以便于学习，同时保持描述原始资料集的精准性与完整性。这其中包含像主成分分析、独立成分分析等方法
	- [元学习](https://en.wikipedia.org/wiki/Meta_learning_(computer_science))主要是解决学习如何学习的问题
		- 构建一个用于学习经验的子系统，模仿人类学习的方式
		- 相关论文： [An introduction to Meta-learning](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a)  [研究论文](https://github.com/floodsung/Meta-Learning-Papers)
- 自动化模型选择：Automated Model Selection，简称 AMS
	- 依据数据特征来选择最为合适的机器学习算法模型
	- 传统的机器学习中，模型的选择一般由机器学习专家根据经验，以及交叉验证的结果来对比决定
- 超参数自动优化：Hyperparameter Optimization，简称 HPO
	- 论文：[基于贝叶斯优化 Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)，[基于进化算法 Evolutionary Algorithms](https://en.wikipedia.org/wiki/Evolutionary_algorithm)，[基于 Lipschitz Functions](https://arxiv.org/pdf/1703.02628.pdf)，[基于本地搜索 Local Search](https://arxiv.org/pdf/1401.3492.pdf)，[基于随机搜索 Random Search](https://en.wikipedia.org/wiki/Random_search)，[基于粒子群优化算法 Particle Swarm Optimization](http://www.sciencedirect.com/science/article/pii/S0957417407003752)，[基于元学习 Meta Learning](http://openproceedings.org/2019/conf/edbt/EDBT19_paper_235.pdf)，[基于迁移学习 Transfer Learning](https://pdfs.semanticscholar.org/75f2/6734972ebaffc6b43d45abd3048ef75f15a5.pdf)
- 神经结构搜索： Neural Architecture Search，简称 NAS
	- 搭建人工神经网络
	- 传统方法中，我们大量应用随机搜索 Random Search 或者网格搜索 Grid Search 方法来进行调参，但这些参数的变动会需要大量的算力需求并且效率极低
	- 论文：[基于进化算法 Evolutionary Algorithms](https://arxiv.org/abs/1902.06827)，[基于元学习 Meta Learning](https://arxiv.org/pdf/1606.01885.pdf)，[基于迁移学习 Transfer Learning](https://arxiv.org/abs/1707.07012)，[基于本地搜索 Local Search](https://arxiv.org/pdf/1711.04528.pdf)，[基于强化学习 Reinforcement Learning](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf)，[基于 Network Morphism](https://arxiv.org/abs/1806.10282)，[基于 Continuous Optimization](https://arxiv.org/abs/1806.09055)


#### 学习路径

- 开源框架：例如 Auto-Keras，auto-sklearn 等开源工具本地完成。
- 商业服务：例如 Google Cloud，Microsoft Azure 等云服务商工具在云端完成

开源框架的自定义程度高，方便集成，但需要较强的本地算力支持。商业服务对本地环境要求不高，有完善的开发文档和技术支持
[开源框架和商业服务列表](https://github.com/hibayesian/awesome-automl-papers#projects)

### auto-sklearn
结构、组成
- Classification：分类问题相关的训练方法。
- Regression：回归问题相关的训练方法。
- Metrics：算法评估方法。
- Extension Interfaces：扩展接口

#### 分类算法

分类问题可以简单概括为：已有了一些数据样本及明确的样本分类。现在从这些样本的特征中总结规律，再用于判断新输入样本到底属于哪一类别

scikit-learn 中能用于分类的算法很多，包括：感知机、人工神经网络、支持向量机、决策树等
auto-sklearn 中分类算法 API 只有一个，那就是 AutoSklearnClassifier

```python
autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=3600, per_run_time_limit=360, initial_configurations_via_metalearning=25, ensemble_size:int=50, ensemble_nbest=50, ensemble_memory_limit=1024, seed=1, ml_memory_limit=3072, include_estimators=None, exclude_estimators=None, include_preprocessors=None, exclude_preprocessors=None, resampling_strategy='holdout', resampling_strategy_arguments=None, tmp_folder=None, output_folder=None, delete_tmp_folder_after_terminate=True, delete_output_folder_after_terminate=True, shared_mode=False, n_jobs: Optional[int] = None, disable_evaluator_output=False, get_smac_object_callback=None, smac_scenario_args=None, logging_config=None, metadata_directory=None)

# time_left_for_this_task：模型搜索时间限制（秒）。通过增加此值，auto-sklearn 有更高的机会找到更好的模型，但耗时更久。

# per_run_time_limit：单次调用模型的时间限制。如果机器学习算法超过该时间限制，则将终止模型拟合。

# initial_configurations_via_metalearning：超参数优化算法从头开始，还是使用元学习方法。

# ensemble_size：从算法库中选择构建集成模型的数量。

# ensemble_memory_limit：整体构建过程的内存限制（MB）。

# ml_memory_limit：单个机器学习算法的内存限制（MB）。

# include_estimators：可手动指定一系列备选算法模型，不设置则使用全部可用算法。

# include_preprocessors：可手动指定一系列备选预处理方法，不设置则使用全部可用预处理方法。

# resampling_strategy：过拟合处理策略，可选交叉验证，或者 holdout，即按照 67:33 比例对训练测试集进行划分。

# tmp_folder：配置和日志文件输出目录。

# output_folder：测试数据预测结果输出目录
```



**使用 scikit-learn 去解决一个机器学习相关的问题时**
- 调用一个机器学习方法构建相应的模型 model，并设置模型参数。
- 使用该机器学习模型提供的 model.fit() 方法训练模型。
- 使用该机器学习模型提供的 model.predict() 方法用于预测。

#### 回归算法

回归问题与分类问题的最大区别（特征）在于，输出变量的类型不同

- 分类问题，输出为有限个离散变量，布尔值或者定类变量。例如上面的手写字符识别分类。
- 回归问题，输出为连续变量，一般为实数，也就是一个确切值。例如预测一个人的年龄。

scikit-learn 中包含的线性模型有最小二乘回归、感知机、逻辑回归、岭回归，贝叶斯回归等，由 sklearn.linear_model 模块导入。
auto-sklearn 中的回归算法和分类算法相似，其只包含一个核心接口 AutoSklearnRegressor。

```python
autosklearn.regression.AutoSklearnRegressor(time_left_for_this_task=3600, per_run_time_limit=360, initial_configurations_via_metalearning=25, ensemble_size: int = 50, ensemble_nbest=50, ensemble_memory_limit=1024, seed=1, ml_memory_limit=3072, include_estimators=None, exclude_estimators=None, include_preprocessors=None, exclude_preprocessors=None, resampling_strategy='holdout', resampling_strategy_arguments=None, tmp_folder=None, output_folder=None, delete_tmp_folder_after_terminate=True, delete_output_folder_after_terminate=True, shared_mode=False, n_jobs: Optional[int] = None, disable_evaluator_output=False, get_smac_object_callback=None, smac_scenario_args=None, logging_config=None, metadata_directory=None)
# AutoSklearnRegressor 的参数设置和 AutoSklearnClassifier 几乎完全一致
```

一部分分类方法实际上可以用于回归问题的解决。所以，AutoSklearnRegressor 对于回归算法的搜索空间中远不止线性回归、多项式回归这些。其还包括像 K 近邻回归，决策树回归等


#### 评估方法

建模预测是机器学习应用的核心，但模型质量的评估必不可少。所以，机器学习中的算法模型都有相对于的评估方法。
分类中，我们常采用预测准确度进行评估，而回归模型的评估不仅有 R^2，还有像平均绝对误差，均方误差等

auto-sklearn 基本上延续了 scikit-learn 中的全部模型评估方法，并放置在 autosklearn.metrics 下

#### 优劣分析


在 scikit-learn 中建模时，你需要从不同模块下导入不同算法，然后针对算法设置超参数，甚至需要使用网格搜索等高阶调参方法。
在 auto-sklearn 中，你只需要导入 AutoSklearnClassifier 或者 AutoSklearnRegressor 即可，有足够的时间，对于这两个 API 甚至都不需要设置参数。

核心的问题是时间。在前面的示例中，为了更快速地得到结果，实验分别设置了最大时间限制。

不设置限制的话，auto-sklearn 执行一次的时间极其漫长。
这还是在 2 个简单的示例数据集上，真实环境中使用的数据往往会复杂很多。

auto-sklearn 
优势在于使用简单，甚至完全不需要了解算法原理。由于其集成了超参数自动优化，数据自动处理，甚至使用起来要方便快捷很多。
劣势在于，需要耗费漫长的搜索时间，几十分钟、几个小时甚至几天。

自动化机器学习并不是万能的，可能出现花费大量时间后找不到理想模型的情况。而在这种情况下，如果由机器学习专家处理，可能会更快地发现数据或者算法的问题。

> 一般性、相对简单的问题可以直接使用  autoML 解决，但是复杂场景还是需要专家来处理

### 模型部署

#### 模型推理

通过训练集构建的神经网络对新输入数据进行预测，就是推理

- 静态推理：集中对批量数据进行推理，并将结果存放在数据表或数据库中。当有需要的时候，再直接通过查询来获得推理结果
- 动态推理：将模型部署到服务器中。当有需要时，通过向服务器发送请求来获得模型返回的预测结果

静态推理适合于对大批量数据进行处理，因为动态推理面对大数据量时非常耗时。但是静态推理无法实时更新，而动态推理的结果是即时计算结果

#### 模型部署

- 进行模型训练
- 通过方式将模型保存成文件
- 运行服务
	- 加载模型
	- 模型进行推理输出结果

### 增量训练

训练过程也有动态和静态

- 静态模型：采用离线训练方式。一般只训练模型一次，然后长时间使用该模型。
- 动态模型：采用在线训练方式。数据会不断进入系统，通过不断地更新系统将这些数据整合到模型中。

不是每一种模型都支持在线（增量）训练，这需要根据模型的自身的特征和所使用机器学习框架来决定

**scikit-learn 支持增量训练的算法**

分类算法
- sklearn.naive_bayes.MultinomialNB
- sklearn.naive_bayes.BernoulliNB
- sklearn.linear_model.Perceptron
- sklearn.linear_model.SGDClassifier
- sklearn.linear_model.PassiveAggressiveClassifier
- sklearn.neural_network.MLPClassifier

回归算法
- sklearn.linear_model.SGDRegressor
- sklearn.linear_model.PassiveAggressiveRegressor
- sklearn.neural_network.MLPRegressor


## 深度学习

以人工神经网络为架构，对数据进行表征学习的算法，属于机器学习下的分支

机器学习 Machine Learning：是人工智能的一个分支，其核心构成为机器学习算法，并通过从数据中获取经验来改善自身的性能

定义（自于计算机科学家 TOM M.Mitchell 于 1997 年出版的《机器学习》专著）：

A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.
  

对于某类任务 T 和性能度量 P ，如果一个计算机程序在 T 上以 P 衡量的性能随着经验 E 而自我完善，那么我们称这个计算机程序在从经验 E 学习。


### 机器学习 & 深度学习 & 人工智能：

最先出现的概念是人工智能，它是于 1956 年由 John McCarthy 提出：当时，人们渴望设计出一种「能够执行人类智能特征任务的机器」

之后，研究人员构思出机器学习的概念，而机器学习的核心是寻求实现人工智能的方法。于是就出现了朴素贝叶斯、决策树学习、人工神经网络等众多机器学习方法。其中，人工神经网络（ANN）是模拟大脑生物结构的一种算法。

后来，就出现了深度学习。深度学习的关键在于建立具有更多神经元、更多层级的深度神经网络。

### 历史进程
  
深度学习主要是使用到深度神经网络。

最早追溯到 1943 年。这一年，著名神经科学家沃伦·麦卡洛克（Warren S. McCulloch）以及瓦尔特·皮茨（Walter Pitts）试图在图灵机中 模拟神经元信号

1957 年心理学家弗兰克·罗斯布拉特（Frank Rosenblatt）发明了感知机算法，他尝试构造一种前馈人工神经网络结构，来完成二元线性分类任务，这也是第一次将神经网络用于分类任务

1969 年，马文·明斯基（M. Minsky）和西摩·帕尔特（S. Papert）的一项研究表明，感知机无法处理异或问题，由于算力不足，神经网络的研究进展停滞不前。

1975 年出现了反向传播算法，该算法有效地解决了异或的问题，更重要的是如何训练多层神经网络的问题。

1986 年，D. Rumelhart 等对该算法进行了全面的论述。反向传播算法的出现是神经网络发展的重要节点，以至于目前的深度神经网络都在使用。

接着：支持向量机等其他更简单的方法出现，转移了大家的注意力。

2000 年后，深度神经网络的出现再一次激发了大家对人工神经网络的热情。


其他：

1979 年出现的 Neocognitron 可谓是卷积神经网络的鼻祖。

1986 年出现的 LeNet 成为了第一个完整的卷积神经网络。

AlexNet 在 2012 年度 ImageNet 大规模图片分类比赛上夺冠惊动了世界

后面陆续出现了 VGG，以及目前深度够深、规模够大的 ResNet

  

## 感知机和人工神经网络

特点：模仿人类神经元工作


感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。


![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240530190622.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240530190633.png)


感知器公式推导：线性回归 + `sigmoid`
> 线性回归计算数值
> sigmoid 进行 -1、1 映射

### 感知机的损失函数

二维平面，点到直线的距离：![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240530190947.png)

推广到 n 维

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240530190845.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240530191022.png)


损失函数就是：所有误分类点到分类线的距离总和，即总错误值？

同时由于损失函数值 > 0，所以当损失函数值为 0 时则得到目标值，即使损失函数最小

### 随机梯度下降法（Stochastic Gradient Descent，简称：SGD)

计算损失函数最小值，可以使用梯度下降法（Gradient Descent），但是由于深度学习一般会涉及大量数据，所以使用了随机梯度下降

每次迭代都随机选取一个点进行梯度下降计算，n 个位置达到局部优？
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240530191528.png)

### 人工神经网络

如果只有感知机，那只能计算线性问题，即使多层的感知机叠加，因为每一层数据都是线性变化，所以仍无法计算非线性问题


激活函数：对感知机计算结果进行非线性变化
> 计算的目的是找到参数，使得在对应激活函数下，可以预测数据集甚至数据集外的情景
> 所以某种程度上激活函数主要的不同在于影响效率？


常见的激活函数：
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240604014300.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240604014310.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240604014318.png)


多重感知机 + 激活函数 = 人工神经网络

### 反向传播


随机梯度下降，面对单层感知机还可以，但是当多层感知机一起时就变得很复杂


前向传播：沿着人工神经网络，计算每一层节点的结果

反向传播：在前向传播计算完成一次后，将结果与期望值进行对比得到误差值，将误差值反向根据权重返回给上一层的节点，对应节点就得到了它计算的误差，一直反向传播到第一层，对第一层的 w 进行更新


![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240604015203.png)
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240604015233.png)

一次性计算一层的反向误差

## 深度学习框架

### TensorFlow

是由谷歌在 2015 年 11 月发布的深度学习开源工具，我们可以用它来快速构建深度神经网络，并训练深度学习模型

特点：
- 高度灵活性：采用数据流图结构，只要计算可以表示为一个数据流图，就可以使用 TensorFlow。

- 可移植性：在 CPU，GPU，服务器，移动端，云端，Docker 容器上都可以运行。

- 自动求微分：在 TensorFlow 中，梯度的运算都将基于你输入的模型结构和目标函数自动完成。

- 多语言支持：提供 Python，C++，Java，Go 接口。

- 优化计算资源：TensorFlow 允许用户将数据流图上的不同计算元素分配到不同设备，最大化利用硬件资源来进行深度学习运算。



2019 年，TensorFlow 推出了 2.0 版本

- 使用 Keras 和 Eager Execution 轻松构建模型。

- 在任意平台上实现稳健的生产环境模型部署。

- 为研究提供强大的实验工具。

- 通过清理废弃的 API 和减少重复来简化 API

#### 张量

粗暴理解：多维数组

概念：贯穿于物理学和数学

- 通常定义张量的物理学或传统数学方法，是把张量看成一个多维数组，当变换坐标或变换基底时，其分量会按照一定规则进行变换，这些规则有两种：即协变或逆变转换。

- 通常现代数学中的方法，是把张量定义成某个矢量空间或其对偶空间上的多重线性映射，这矢量空间在需要引入基底之前不固定任何坐标系统。例如协变矢量，可以描述为 1-形式，或者作为逆变矢量的对偶空间的元素

#### Eager Execution

TensorFlow 2 带来的最大改变之一是将 1.x 的 Graph Execution（图与会话机制）更改为 Eager Execution（动态图机制）


在 1.x 版本中，低级别 TensorFlow API 首先需要定义数据流图，然后再创建 TensorFlow 会话，这一点在 2.0 中被完全舍弃。TensorFlow 2 中的 Eager Execution 是一种命令式编程环境，可立即评估操作，无需构建图
> 简化了使用的复杂性

```python
# v2 加法
c + c  # 加法计算



# v1 加法
init_op = tf.global_variables_initializer()  # 初始化全局变量
with tf.Session() as sess:  # 启动会话
    sess.run(init_op)
    print(sess.run(c + c))  # 执行计算
```

#### 自动微分

简便的求导？

```python
w = tf.Variable([1.0])  # 新建张量

with tf.GradientTape() as tape:  # 追踪梯度
    loss = w * w  # 计算过程

tape.gradient(loss, w)  # 计算梯度
```


#### 交叉熵损失函数

深度神经网络构建过程中十分常用的一个损失函数：交叉熵损失函数
本质上是对数损失函数

主要用于度量两个概率分布间的差异性信息，交叉熵损失函数会随着正确类别的概率不断降低，返回的损失值越来越大
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240611012752.png)


Softmax 函数
将全连接层的输出通过该函数转换为概率，相对概率？相似度？
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240611012819.png)


####  Adam 优化器

一种数学优化方法，其最早由 Diederik P. Kingma 等于 2014 年提出 [🔗](https://arxiv.org/abs/1412.6980)。Adam 的全称为 Adaptive Moment Estimation，它是一种自适应学习率的算法，其针对每一个参数都计算自适应的学习率


#### Batch 和 Epoch

Epoch：数据集总共训练多少次
Batch：将数据集分割为 n 个 btach，迭代 n 此后完成一个 Epoch

所以总迭代次数 = Epoch * Batch
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240611013329.png)


#### Keras

用 Python 编写的独立高阶神经网络 API，它能够以 TensorFlow, CNTK，或者 Theano 作为后端运行


tensorflow 中的 tf.keras 模块

### PyTorch

由 Facebook 主导开发的深度学习框架,18 年 5 月，PyTorch 正式宣布集成 Caffe2 和 ONNX 的功能


核心功能 / 特色：

- 高效率的张量计算, 同时支持强大的 GPU 加速计算功能。
- 搭建深度神经网络, 构建在自动求导系统之上的网络结构。
- 兼容 NumPy 数组

## 深度学习应用：计算机视觉

### 卷积神经网络原理

类似于神经网络，卷积神经网络是模拟我们的视觉

1959 年，Hubel 和 Wiesel 的实验表明，生物的视觉处理是从简单的形状开始的，比如边缘、直线、曲线。

卷积神经网络实际是通过每层学习，从低级特征到高级特征，从而实现学习任务
> 比如识别人脸，第一层可能学到了简单的线条，第二层学到了轮廓，倒数第二层学到了双眼皮和耳垂，最后一层识别出了她是你的高中班主任


卷积神经网络一般是由卷积层、池化层和全连接层堆叠而成的前馈神经网络结构。卷积神经网络同样使用反向传播算法进行训练

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240614005500.png)


#### 卷积核 Kernel

卷积核（权值矩阵）： 用于从输入矩阵（图像）中提取一定的特征
> 通过不断平移卷积核，同时与输入矩阵进行卷积操作，就可以得到卷积后矩阵

理解：最简单的就是把卷积核看作是一片滤镜，原矩阵通过滤镜之后就得到了新的特征矩阵

#### 卷积步长 Stride

对于一些很大的图片，每次只移动一步运算效率会很低下，同时分解得到的特征也很容易冗余。于是，可以引入一个超参数能对每一次移动的步数进行调节，我们把它称之为卷积步长 Stride。

简单理解：卷积时横向和纵向走的时候是每次走一格还是 n 格
> 可以是元组，分别指定 横向和纵向

不同的步长，结果矩阵会不同，输出矩阵的计算公式
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240614005750.png)


#### 边距扩展 Padding

步长解决了卷积效率低的问题，但又造成了新的问题。你会发现，随着步长的增大，卷积输出的矩阵将持续变小

在构建网络的时候我们往往希望输出矩阵的大小为指定大小，而不完全由卷积步长的变换而左右。于是，就有了 Padding 的操作


在不改变卷积核大小和步长的前提下，卷积得到的矩阵大小变大，唯一的方法就是调整原矩阵的大小

简单理解：扩大原矩阵的边距，在输入图像周围填充 0

常见的 padding 形式：

- Arbitrary Padding：使输出矩阵尺寸大于输入尺寸
	- ![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240614010018.png)
- Half Padding 也叫 Same Padding：希望得到的输出矩阵尺寸与输入尺寸一致
- Full Padding：最普通的，不补 0？

#### 高维多卷积核过程

一般的图片是一个 𝑚×𝑛×𝑘 的三维矩阵（宽 x 高 x 通道）
此时可以让卷积核也是一个三维 𝑎×𝑏×𝑘 的矩阵(通常 𝑎=𝑏，且常取 1，3，5，7 这样的数)
- 重点：保证输入矩阵大小与卷积核大小在第三个维度上值相等

如果做一个 stride=𝑠，padding=𝑝 的卷积操作，那么将会得到一个 [(𝑚+2𝑝−𝑎)/𝑠+1]×[(𝑛+2𝑝−𝑎)/𝑠+1] 的二维矩阵


**让输出矩阵依然是一个三维矩阵**：每一层都由多个卷积核来实现操作，最后把得到的所有二维矩阵叠起来，就得到了一个 [(𝑚+2𝑝−𝑎)/𝑠+1]×[(𝑛+2𝑝−𝑎)/𝑠+1]×𝑐，𝑐 为卷积核的个数的输出矩阵


#### 池化

降采样操作过程：图片是一个非常大的训练数据，所以想达到很好的训练性能，只有卷积层是不够的。池化层通过降采样的方式，在不影响图像质量的情况下，压缩图片，达到减少训练参数的目的


##### 最大池化


界定一个过滤器和步长。最大值池化，取输入矩阵在过滤器大小上矩阵块的最大值作为输出矩阵对应位置输出

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240614010928.png)

每个格子块中取最大值？

##### 平均值池化
取输入矩阵在过滤器大小上矩阵块的平均值作为输出矩阵对应位置输出
![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240614010954.png)

每个格子块中取平均值？


## LeNet 神经网络

Yann LeCun 大牛（被称作卷积神经网络之父）在 1998 年搭建了 LeNet，并且在手写识别网络(MNIST)上广泛使用

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240616214156.png)
采用了平均值池化。前面各层都由 tanh 激活，最后一层由 RBF 激活


- 卷积层1：6 个 5x5 卷积核，步长为 1，relu 激活
- 平均池化1：平均池化，池化窗口默认为 2
- 卷积层2：16 个 5x5 卷积核，步长为 1，relu 激活
- 平均池化2：平均池化，池化窗口默认为 2
- 后续要进行全连接，展开数据
- 全连接层1：输出为 120，relu 激活
- 全连接层2：输出为 84，relu 激活
- 全连接层3：输出 10 labels，即输出为 10，relu 激活




## AlexNet 神经网络

将卷积层叠层后再叠上池化层的网络，运用上 Dropout（训练时，按照一定的概率将神经元暂时从网络中丢弃）和数据增强两种方法降低过拟合，以及 LRN(local responce normalization) 做为规划层来促使神经元学到更加广泛的特征

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240616214318.png)

5 个卷积层，5 个池化层，3 个全连接层，大约 5000 万个可调参数组成，最后的全连接层输出到 1000 维的 softmax 层，产生一个覆盖 1000 类标记的分布


## VGG 神经网络

5 个卷积组，2 层全图像连接特征和 1 层全连接分类特征，通常有 16 到 19 层

论文中给出了不同的卷积层配置方法，实验结果表明随着卷积层从 8 到 16 一步步加深，通过加深卷积层数准确率已经达到了瓶颈数值
> 单纯添加卷积层往往并不能起到更好的效果

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240616214522.png)


## Google Net 神经网络

[Inception](https://github.com/google/inception)，灵感来源于当时同时期的电影 Inception（盗梦空间）,增强卷积模块的功能


![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240616214723.png)
一个卷积网络里面的局部稀疏最优结构往往可以由简单可复用的密集组合来近似或者替代。就像上图里面 1×1，3×3，5×5 的卷积层，与 3×3 的池化层的组合一个 Inception。

1. 不同尺寸的卷积核可以提取不同尺度的信息。
2. 采用 1×1，3×3，5×5 可以方便对齐，Padding 分别为 0，1，2 就可以对齐。
3. 由于池化层在 CNN 网络里面的成功运用，也把池化层当做组合的一部分，在 GoogleNet 论文里也说明了它的有效性。

前一层经过 1×1 或者池化层降维后，在全连接层过滤器将 1×1，3×3，5×5 的卷积结果连接起来，使网络的深度和宽度均可以扩大。论文显示，Inception 模块使整个网络的训练过程有 2 到 3 倍的加速，且更容易捕捉到关键特征用以学习。同时，由于网络很深，为了避免梯度消失的问题，Google Net 还巧妙地在不同深度增加了两个损失函数来训练

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240616215447.png)


## ResNet 神经网络

训练深度达到了 152 层，将部分网络进行了一个跳层传递的操作。网络在深化的过程中，可能会出现一种叫网络退化的问题，ResNet 通过引入这个跳层传递，将前面某层的结果与这一层的卷积结果相加，去优化一个残差来规避这些问题

ResNet 将网络深度倍数化增加，从最多 20 层一下子拉到了 100 层以上，而且网络也不再是平常的层数堆叠，最终模型预测准确率已经达到了一个巨大的量变提高。

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240616215517.png)


## 卷积神经网络的发展史


![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240616215649.png)


## 为什么要使用卷积神经网络


卷积神经网络保留了图像的空间信息：19 世纪 60 年代，科学家通过对猫的视觉皮层细胞研究发现，每一个视觉神经元只会处理一小块区域的视觉图像，即感受野。在卷积神经网络中，一个卷积层可以有多个不同的卷积核（权值），而每个卷积核在输入图像上滑动且每次只处理一小块图像


- 输入端的卷积层可以提取到图像中最基础的特征，比如不同方向的直线或者拐角；
- 接着再组合成高阶特征，比如三角形、正方形等；
- 再继续抽象组合，得到眼睛、鼻子和嘴等五官；
- 最后再将五官组合成一张脸，完成匹配识别

过程中，每个卷积层提取的特征，在后面的层中都会抽象组合成更高阶的特征，最终习得足够多足够关键的特征，完成学习任务


卷积层具有两个关键优势：
- 局部连接：每个神经元只与上一层的一个局部区域连接，该连接的空间大小可视为神经网络神经元的感受野
- 权值共享：当前层在深度方向上每层神经元都使用同样的权重和偏差

## 图像分类原理与实践

- 图像分类 Image Classification: 图像检索
- 对象检测 Object Detection: 一般会使用边界框将检测对象框起来，并标记出置信度
- 目标跟踪 Object Tracking: 在特定场景跟踪某一个或多个特定感兴趣对象的过程，是无人驾驶的关键技术之一
- 语义分割 Semantic Segmentation: 可以看作是对象检测的延伸，不仅需要标记出对象的边界框，还需要精确识别出各部分的边界
- 实例分割 Instance Segmentation: 语义分割的拓展，例如用特定的颜色来标记同一类别对象的不同实例


### 数据扩增 Data Augmentation

深度学习的效果非常依赖于数据集的规模，我们当然期望规模足够庞大的数据。不过，很多时候因为各种条件限制，数据集可能不够大。那么，对于图片数据来讲，就可以采用随机裁剪、旋转、镜像、去色等操作，将一张图片变换成不同视角和角度的多张图片，起到数据扩增的效果。

通过 GAN 来生成新的图片也是数据扩增的一种方式

```python
import numpy as np
# 加载图片并转换为 PIL IMAGE
IMAGE = transforms.ToPILImage()(io.imread(data_path[0]))
# 尺寸变形
scale = transforms.Resize(256)
# 随机裁剪
crop = transforms.RandomCrop(128)
# 打包方法
composed = transforms.Compose([transforms.Resize(256), transforms.RandomCrop(224)])
# 将每个变换函数应用到一个样本上
fig = plt.figure()
for i, tsfrm in enumerate([scale, crop, composed]):
    transformed_sample = np.array(tsfrm(IMAGE))  # PIL.image 转换成 np.ndarray
    ax = plt.subplot(1, 3, i + 1)
    plt.tight_layout()
    ax.set_title(type(tsfrm).__name__)
    ax.imshow(transformed_sample)
```


### 网络设计/选择

一般情况下。工程师应该尽可能沿用我们在卷积神经网络原理实验中提到过的比较经典的神经网络结构，而不是自行设计。比如：AlexNet，VGG，Google Net 和 ResNet 

从头开始训练一个 AlexNet 所耗费的时间是非常长的。大概需要几个小时到十几个小时不等


### 迁移学习

从以前的任务当中去学习知识或经验，并应用于新的任务当中。换句话说，迁移学习目的是从一个或多个源任务中抽取知识、经验，然后应用于一个目标领域当中去。比如说一个通用的语音模型迁移到某个人的语音识别，一个已训练好的图片分类模型迁移到医疗疾病识别上


假设已经有了一个可以高精确度分辨猫和狗的深度神经网络，之后想训练一个能够分别不同品种的狗的图片模型，你需要做的不是从头训练那些用来分辨直线，锐角的神经网络的前几层。而是利用训练好的网络，提取初级特征，之后只训练最后几层神经元，让其可以分辨狗的品种即可 为什么不从头训练：很少有人从头开始训练整个卷积网络（随机初始化），因为拥有足够大小的数据集是相对罕见的


常用领域：
- 通过半监督学习减少对标注数据的依赖，应对标注数据的不对称性。
- 通过迁移学习来提高模型的稳定性和可泛化性，不至于因为一个像素的变化而改变分类结果。
- 使用迁移学习来做到持续学习，让神经网络得以保留在旧任务中所学到的技能

#### 学习策略


##### 预训练模型

ImageNet 有几千万张图片，当前的卷积神经网络又相当复杂，有非常多的训练参数，因此即使在很多 GPU 上训练也要花 2~3 周，所以为了发布模型经常保存训练的模型参数，给需要的人用这个模型微调。很多深度学习框架都提供了预训练模型，例如最早的 Caffe 在 Model Zoo 开放了大量预训练的模型。这些模型可以直接拿来使用。

特点：用了大型数据集做训练，已经具备了提取浅层基础特征和深层抽象特征的能力

##### 特征提取器

在 ImageNet 上预先训练一个卷积网络 ConvNet，删除最后一个全连接层（该层的输出是 ImageNet 的一个样本对应 1000 个类的概率），然后将其余的 ConvNet 视为新数据集的固定特征提取器。而重新为新的数据集训练线性分类器（例如线性 SVM 或 Softmax 分类器），例如在猫狗分类中有 2 类，分别是猫和狗，只需要训练最后一层用于实现二分类即可

##### 微调 Fine-tuning

不仅在新数据集上替换和重新训练 ConvNet 之上的分类器，而且还通过继续反向传播来微调预训练网络的权重，可以微调 ConvNet 的所有层； 为了避免过度拟合问题，也可以保留一些早期层，仅微调网络的某些更高级别部分

不做微调可能出现：
- 从头开始训练，需要大量的数据，计算时间和计算资源。
- 存在模型不收敛，参数不够优化，准确率低，模型泛化能力低，容易过拟合等风险。

一般情况下，当调参遇到下列情况时，就会考虑使用微调：

- 使用的数据集和预训练模型的数据集相似，如果不太相似，比如你用的预训练的参数是自然景物的图片，你却要做人脸的识别，效果可能就没有那么好了，因为人脸的特征和自然景物的特征提取是不同的，所以相应的参数训练后也是不同的。
- 自己搭建或者使用的卷积神经网络模型准确率太低，可以尝试微调是否提高自己的网络性能。
- 数据集相似，但数据集数量太少。
- 计算资源太少。

**微调操作**
- 通常的做法是截断预先训练好的网络的最后一层（分类器），并用新的分类器替换它。例如，ImageNet 上预先训练好的网络带有 1000 个类别的 Softmax 层。如果新任务是对 10 个类别的分类，则网络的新 Softmax 层将由 10 个类别组成，而不是 1000 个类别。然后在网络上运行预先训练的权重。确保执行交叉验证，以便网络能够很好地推广。
- 使用较小的学习率来训练网络。由于预先训练的权重相对于随机初始化的权重已经相当不错，所以一般不会过快地改变这些权重。所以，通常使用的初始学习率是从头训练（Training from scratch）的初始学习率的 1/10。
- 如果数据集数量过少，一般只训练最后一层。如果数据集数量中等，可以冻结预训练网络的前几层，训练后几层网络。因为前几个层捕捉了与新问题相关的通用特征，如曲线和边，我们希望保持这些权重不变。相反，一般会让网络专注于学习后续深层中特定于数据集的特征。


#### 过拟合与欠拟合

泛化能力强的模型才是好模型，对于训练好的模型
- 若在训练集表现差，不必说在测试集表现同样会很差，这可能是欠拟合导致
- 若模型在训练集表现非常好，却在测试集上差强人意，则这便是过拟合导致的。

在机器学习中，把模型在训练数据集上表现出的误差叫做训练误差，在任意一个测试数据样本上表现出的误差的期望值叫做泛化误差。 而欠拟合和过拟合在误差上的表现则分别是:
- 欠拟合：机器学习模型无法得到较低训练误差。
- 过拟合：机器学习模型的训练误差远小于其在测试数据集上的误差

当我们遇到过拟合时，主要有以下几种解决方法:
- 重新清洗数据。导致过拟合的一个可能原因是数据不纯，这就需要我们重新清洗数据。
- 训练数据量过少。此时可以增大训练数据占总数据的比例，或者进行数据扩增，例如反转、镜像、裁剪图片。
- 采用 Dropout 方法。Dropout 是一种神经网络的正则化方法，在训练的时候让神经元以一定的概率断开连接。
- Batch normalization 批量归一化。顾名思义，Batch normalization 就是对每批数据进行归一化处理，主要作用是加快网络的训练速度，但某种程度上也代替了 Dropout 的作用，Batch normalization 在训练阶段引入随机性，防止过度匹配。在测试阶段通过求期望等方式在全局去除掉这种随机性，从而获得确定而准确的结果。
- 添加 L1/L2 正则项。这种方法可以直接通过向训练参数添加惩罚项即可

欠拟合的解决方法会少很多。 欠拟合主要是网络学习不到位导致的，那么可以通过改善网络结构完善。通常情况下，可以通过增加网络层数提高提取到的特征数，例如说把 AlexNet 换成 ResNet

学习率选择得不好最终也会导致过拟合或欠拟合：
- Loss 抖动：学习率太大出现的超调现象，即在极值点两端不断发散，或是剧烈震荡，总之随着迭代次数增大 Loss 没有减小的趋势。
- Loss 下降速度很慢：学习率太小，导致卷积神经网络学习太慢的原因。
- Loss 爆炸，甚至超出了实数表达的范围：学习率超级大，优化器根本无法正常工作。

所以对于学习率的选择，通常会选择一些学习率衰减策略，可以避免固定学习率遇到的以上问题：
- 步长衰减：每训练一定 Epoch 降低学习率，通常给定大小，例如是上一个学习率的 0.1。
- 指数衰减：按照公式 𝛼=𝛼0𝑒−𝑘𝑡 衰减，其中 𝛼0,𝑘 是超参数，𝑡 是迭代次数。
- 1/t 衰减：按照公式 𝛼=𝛼0÷(1+𝑘𝑡) 衰减，其中 𝑎0,𝑘 是超参数，𝑡 是迭代次数。


### 卷积神经网络可视化

卷积神经网络目前还是黑盒，无法从数学原理上予以解释
目前其中一种解析方式就是可视化：
- Learning Deep … 这篇文章提出了一种确定卷积神经网络模型究竟在关注图片中的那一部分，或者说那一部分含有的信息最多，影响了卷积神经网络模型的判断，导致他输出当前类别

人类的大脑主要关注的图像的哪一部分。比如说我们看到一个人，如何判断她是个女人呢？当然是看脸、胸、衣服等等特征很明显的区域，根据这个原理，把它叫做深度学习中的「注意力机制」。 类似的卷积神经网络可视化有很多种，比如：
- 特征图（Feature map）可视化
- 权重可视化
- Saliency map

最初的可视化工作见于 Imagenet classiﬁcation … ，也就是提出实验所使用到的 AlexNet 网络的论文

### GAN

`r = E(x ~ Pdata(X))[logD(x)] + E(z ~ Pz(Z))[log(1 - D(G(z)))]`

- x ~ Pdata(X): 表示是从 x(真实数据) 抽样作为输入 
- D(x): 输入真实数据时，判别器输出的概率，我们期望这个概率能是 1，即判别器能判别真实数据 
- E: 期望值,表示要取所有概率可能性的平均值？ 
- logD(x): 对数似然函数
	- 似然函数：多个独立事件同事发生的概率（概率相乘）
	- 最大似然估计：根据给定的事件数据，有一组参数使得事件发生的概率最大，则为最优
	- 为什么要转为 log: 为了在数学上方便计算，转化为 log 形式 
	- `E(x ~ Pdata(X))[logD(x)]`: 表示是从 x 抽样作为输入，然后期望概率的似然函数的数学期望

`E(z ~ Pz(Z))[log(1 - D(G(z)))] `
- D(G(z)): 是输入生成器数据，的结果概率 
- 1 - D(G(z)): 即取反，从目标来说就是非 1 的概率（即输入是生成器数据的概率，判别器判别的是输入数据是生成数据还是真实数据，目标是让判别器识别不出来是生成数据） 
- log(1 - D(G(z))): 非 1 的最大似然估计 

`E(z ~ Pz(Z))[log(1 - D(G(z)))]`:

#### 应用

卷积神经网络擅长用于计算机视觉，循环神经网络擅长用于自然语言处理，GAN 尚且没有一个特别适合的应用场景

- 不收敛问题：GAN 是两个神经网络之间的博弈。试想，如果判别器提前学到了非常强的，那么生成器很容易出现梯度消失而无法继续学习。所有 GAN 的收敛性一直是个问题，这样也导致 GAN 在实际搭建过程中对各种超参数都非常敏感，需要精心设计才能完成一次训练任务。
    
- 崩溃问题：GAN 模型被定义为一个极小极大问题，可以说，GAN 没有一个清晰的目标函数。这样会非常容易导致生成器在学习的过程中开始退化，总是生成相同的样本点，而这也进一步导致判别器总是被喂给相同的样本点而无法继续学习，整个模型崩溃。
    
- 模型过于自由： 理论上，我们希望 GAN 能够模拟出任意的真实数据分布，但事实上，由于我们没有对模型进行事先建模，再加上「真实分布与生成分布的样本空间并不完全重合」是一个极大概率事件。那么，对于较大的图片，如果像素一旦过多，GAN 就会变得越来越不可控，训练难度非常大。

### 自动编码器

一种用于非监督学习过程的人工神经网络。 自动编码器通常又两部分构成：编码器和解码器

- 编码器：将输入压缩成潜在空间表征
- 解码器：重构潜在空间表征对其解码
作用：数据去噪和数据可视化降维

#### 原理
encoder -> code -> decoder

目标是训练模型，然后实现 decoder 出来的图片跟原始图片一致

如果实现了，那表明中间的 code 潜空间包含了图片需要有的所有信息，然后就可以拿去进行其他计算，因为降维了，训练效率会大大的提高

模型结构：经典的自动编码器结构中，一般编码器输出层会使用 Relu 激活，而解码器输出层会使用 Sigmoid 激活


### 去噪自动编码器

通过训练使得解码器有去噪能力

对上 MNIST 数据集添加随机的高斯噪声。 方式：先使用 np.random.normal 生成同尺寸的随机值，并与原数组进行求和。再使用 np.clip 将数组规约到[0,1]之间即可


### 目标检测

除了图像分类、图像生成、图像去噪，目标检测也是 计算机视觉领域非常常见的一类问题

应用：人脸检测，行人检测，图像检索和视频监控


目标检测（Object Detection）：识别图像中指定存在的所有对象及其位置，并标示出来

#### 方法

- 分割：将图像分割，然后对每个分割进行检测判断分割内是否存在某个物体
    - 更细的物体通过更小的分割实现，但是这样其实不太靠谱
- 结构化的划分：分割的发展，例如，我们可以将图像划分为 10 x 10 的网格
    - 对于每一个网格，都以其为中心尝试取出包含不同数量临近网格的切片，最终提交给分类器
    - 然后由分类器给出准确率高且面积最小的切片，框定出目标物体
    - 更进一步：加大网格，取更多的切片，或者不同比例的去切，但是这样计算的复杂度太高了


### 深度学习检测目标


#### R-CNN 家族

2013 年，Ross Girshick（简称 RBG）的 R-CNN 论文 Rich feature … 横空出世，这也是利用卷积 神经网络来做目标检测的开山之作。

R-CNN 是 Regions with CNN features 的简称，意思是基于区域的卷积 神经网络。其主要思路大致包含 3 个步骤：

- 利用启发式算法 Selective search 论文 生成约 2000 个 Region proposal 待检测区域。
    
- 将每个 Region proposal 输入卷积 神经网络提取特征，再使用支持向量机完成分类。
    
- 训练一个线性回归模型收缩边界框。
    

传统的区域选择采用滑动窗口的思路：滑动窗口即指定窗口后以等间距在图像上依次移动，这样每滑一个窗口检测一次，相邻窗口信息重叠高，检测速度慢

R-CNN 使用了 Selective search 选择性搜索算法，先一次性生成候选区域后再检测。

- Selective search 虽然对于每张图片都生成了约 2000 个 Region proposal，但相比于穷举滑动窗口搜索的数量已经大大降低。
    
- 关于 Selective search 方法，你可以阅读论文 [What makes …](https://arxiv.org/abs/1502.05082)，其中也对比了除 Selective search 之外的其他相关候边界框搜索算法。
    
- 将每个 Region proposal 输入卷积 神经网络提取特征，再使用支持向量机完成分类
    
    - 支持向量机完成的是二分类问题，即判断是否属于该类别 yes 或者 no
        

发展顺序：R-CNN → SPP Net → Fast R-CNN → Faster R-CNN → Mask R-CNN

- SPP Net：何恺明团队提出，改进了候选区域生成和卷积顺序，且打破了固定尺寸输入的束缚
    
- Fast R-CNN：优化了工作流程，提升效率
    
- Faster R-CNN：利用神经网络生成候选区域，而非之前的选择性搜索，从而进一步提升了边界框的精度
    
- Mask R-CNN：不仅可以做「目标检测」，还可以同时做「语义分割」
    

总体思路：先生成 Region Proposal 或者先使用 CNN 提取特征，然后再使用分类器分类并修正边界框位置

#### YOLO 和 SSD

全称是 You Only Look Once，采用回归方法一步确定边界框

1.输入图片被划分为 m x m 的网格。接下来，检测每个网格并输出边界框的 (x,y,w,h) 和置信度,置信度包含该网格内物体的类别及预测准确度 2.将每个网格处理完使用 非极大值抑制 NMS 算法 去除重叠框并得到结果

特点：改善了检测速度，但是对小物体的效果并不好。

SSD：在 YOLO 上添加了 Faster R-CNN 中 Anchor 的概念，并融合不同卷积层的特征做出预测。

- 结合了不同尺寸大小 Feature Maps 所提取的特征，然后进行预测，提升了对小物体的检测准确度。除此之外，SSD 省去了全连接层，减少了参数，提升了速度
    

#### Mask R-CNN

不仅可以完成「目标检测」，还可以同时用于计算机视觉中的「语义分割」

## 数据处理手段

### 差分运算

一种从序列中提取确定性信息的方法，也是一种非常基础的数学分析手段

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240527192120.png)


p 阶 k 步差分，看不同次数下看数据稳定性改善效果

一般在差分时阶数不宜过大。原因在于差分其实是对信息提取加工的过程，每次差分都会带来信息损失，过度差分会导致有效信息损失而降低精度。一般情况下，线性变化通过 1 次差分即可平稳，非线性趋势 2，3 次差分也能变得平稳，一般差分次数不超过 2 次


### 正则化方法

- 增加总数作为分母


### min-max 归一化

![image.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/20240523001412.png)


```python
moons = (moons - np.min(moons)) / (np.max(moons) - np.min(moons))
# 不管维度，拍平数据，找出最大、最小值
```

### 零值化处理

去除均值对变换的影响，减去均值后数据的信息量没有变化，即数据的区分度（方差）是不变的

### PCA 主成分分析

降低数据的维数，通过保留数据集中的主要成分来简化数据集
数学原理：通过对协方差矩阵进行特征分解，从而得出主成分（特征向量）与对应的权值（特征值）。然后剔除那些较小特征值（较小权值）对应的特征，从而达到降低数据维数的目的。

作用
1.方便将数据用于低维空间可视化。聚类过程中的可视化是很有必要的。
2.高维度数据集往往就意味着计算资源的大量消耗。通过对数据进行降维，我们就能在不较大影响结果的同时，减少模型学习时间。

一般情况下，我们不会拿到数据就进行 PCA 处理，只有当算法不尽如人意、训练时间太长、需要可视化等情形才考虑使用 PCA。其主要原因是，PCA 被看作是对数据的有损压缩，会造成数据集原始特征丢失。

#### 计算

**向量基**
基（basis）也称为基底，其是用于描述、刻画向量空间的基本工具。向量空间的基是它的一个特殊的子集，基的元素称为基向量。向量空间中任意一个元素，都可以唯一地表示成基向量的线性组合，使用基底可以便利地描述向量空间
可认为是坐标系的单位向量(并且模长为 1？)


**投影降维**

通过向新基进行映射（投影） ==> （新基矩阵） * （向量） = （向量映射）
通过增加、减少基，即可实现降维、升维


**基的寻找的理论**

要紧降维、升维，首先要选取合适的基

方差：是用来度量单个随机变量的离散程度。也就是说，最大方差给出了数据最重要的信息。
一个维度中的方差，可以看作是该维度中每个元素与其均值的差平方和的均值，公式如下：

降维：尽量留得更多，最重要的信息

总结：找到一个基，通过这个基映射原来的二维数据点，得到一个方差最大的结果，从而完成降维的目的

**寻找基的方法**

如果按方差最大一个基一个基的寻找，每次都寻找最合适的基，最终合并进行降维，那么可能会出现局部最优（其中有些基是重合的）

特点：基之间应该是没有相关性的
在二维空间中两个向量垂直，则线性无关，那么在高维空间中，向量之间相互正交，则线性无关

协方差：一般用来刻画两个随机变量的相似程度
基于方差公式，改为两个变量
方差 == 特殊情况的协方差：从原来衡量两个相同变量的误差，到现在变成了衡量两个不同变量的误差
两个变量之间的协方差等于 0 即可以选择到无关的基

协方差矩阵
为了解决协方差只能处理两个变量的问题，引入协方差矩阵
由向量自身的方差与向量之间的协方差构成。这个定义延伸到更多向量时同样受用。
选择一组基，使得原始向量通过该基映射到新的维度，同时需要满足使得该向量的方差最大，向量之间的协方差为 0 ，也就是矩阵对角化。


通过计算所有的特征的协方差矩阵，看哪些特征的特征值值比较大，即为权重比较大的特征



### 独热编码

One-Hot编码，又称为一位有效编码，采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效
  - 性别特征：["男","女"] 男 => 10  女 => 01
  - 祖国特征：["中国"，"美国，"法国"] 中国 => 100  美国 => 010  法国 => 001

### 规范化

目的：将特征数据的分布调整成标准正太分布，也叫高斯分布，即使得数据的均值维0，方差为1

具体步骤
![17138004047041713800404465.png](https://fastly.jsdelivr.net/gh/rquanx/my-statics@master/images/17138004047041713800404465.png)

- 统一尺度
- 避免数值不稳定和提高精度
- 改善算法收敛速度：更便于计算

# 疑问

## 多项式回归和深度学习神经网络

- 在多次多元回归中，我们通过将输入变量扩展为多项式特征来捕捉非线性关系
	- 需要手动选择多项式的次数（degree）以及特征的组合，这可能会导致维度灾难（特征数量急剧增加）
- 神经网络通过多层结构和非线性激活函数自动学习特征表示，而不需要手动构建多项式特征
	- 神经网络可以通过增加层数和神经元数量来增加模型的容量，从而捕捉更复杂的非线性关系
	- 神经网络的每一层都可以视为对输入进行非线性变换，最终输出是多层变换的结果，形式上更为复杂